import os
import json
import pandas as pd
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.schema import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

from pydantic import BaseModel, Field  # ë°ì´í„° ê²€ì¦ê³¼ ì§ë ¬í™”ë¥¼ ìœ„í•œ Pydantic ë¼ì´ë¸ŒëŸ¬ë¦¬
from typing import Literal  # ë‹¤ì–‘í•œ íƒ€ì… íŒíŒ… í´ë˜ìŠ¤ë“¤
from langchain_core.output_parsers import JsonOutputParser  # LLMì˜ ì¶œë ¥ì„ JSON í˜•ì‹ìœ¼ë¡œ íŒŒì‹±í•˜ëŠ” ë„êµ¬
from langchain_core.prompts import PromptTemplate 
from googleapiclient.discovery import build
from datetime import datetime
from langchain.vectorstores import Chroma
import chromadb
from typing import List
import pytz

load_dotenv() 
open_ai_key = os.getenv("OPEN_AI_KEY")
tavily_api_key = os.getenv("TAVILY_API_KEY")
youtube_api_key = os.getenv("YOUTUBE_API_KEY")

def get_llm(api_key:str):
    model = ChatOpenAI(temperature=0, model="gpt-4o-mini", api_key=api_key)
    return model

class AgentAction(BaseModel):
    """
    ì—ì´ì „íŠ¸ì˜ í–‰ë™ì„ ì •ì˜í•˜ëŠ” Pydantic ëª¨ë¸
    Pydanticì€ ë°ì´í„° ê²€ì¦ ë° ê´€ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
    """
    # Literalì„ ì‚¬ìš©í•˜ì—¬ action í•„ë“œê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì„ ì œí•œí•©ë‹ˆë‹¤
    action: Literal["video", "news", "not_supported"] = Field(
        description="ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•  í–‰ë™ì˜ íƒ€ì…ì„ ì§€ì •í•©ë‹ˆë‹¤",
    )
    
    action_input: str = Field(
        description="ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì›ë³¸ ì§ˆì˜ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤",
        min_length=1,  # ìµœì†Œ 1ê¸€ì ì´ìƒì´ì–´ì•¼ í•¨
    )
    
    search_keyword: str = Field(
        description="""ê²€ìƒ‰ì— ì‚¬ìš©í•  ìµœì í™”ëœ í‚¤ì›Œë“œì…ë‹ˆë‹¤.
        AI ê´€ë ¨ í‚¤ì›Œë“œì¼ ê²½ìš° í•µì‹¬ ê²€ìƒ‰ì–´ë¥¼ í¬í•¨í•˜ê³ ,
        not_supported ì•¡ì…˜ì˜ ê²½ìš° ë¹ˆ ë¬¸ìì—´('')ì„ ì‚¬ìš©í•©ë‹ˆë‹¤""",
        examples=["ChatGPT tutorial", "ë¨¸ì‹ ëŸ¬ë‹ ì…ë¬¸ ê°•ì˜"]  # ì˜ˆì‹œ ì œê³µ
    )

output_parser = JsonOutputParser(pydantic_object=AgentAction)
# print("ì¶œë ¥ í¬ë§· ê°€ì´ë“œ :", output_parser.get_format_instructions())


prompt = PromptTemplate(
            input_variables=["input"],  # í…œí”Œë¦¿ì—ì„œ ì‚¬ìš©í•  ë³€ìˆ˜ë“¤
            partial_variables={"format_instructions": output_parser.get_format_instructions()},
            template="""ë‹¹ì‹ ì€ AI ê´€ë ¨ YouTube ì˜ìƒì„ ê²€ìƒ‰í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
ì…ë ¥ëœ ì§ˆì˜ê°€ AI ê´€ë ¨ ë‚´ìš©ì¸ì§€ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.

AI ê´€ë ¨ ì£¼ì œ íŒë‹¨ ê¸°ì¤€:
- AI ê¸°ìˆ  ë° ì •ë³´ (ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹, ìì—°ì–´ì²˜ë¦¬ ë“±)
- AI ë„êµ¬ ë° ì„œë¹„ìŠ¤ (ChatGPT, DALL-E, Stable Diffusion ë“±)
- AI êµìœ¡ ë° í•™ìŠµ
- AI ì •ì±… ë° ë™í–¥

AI ê´€ë ¨ ì§ˆì˜ê°€ ì•„ë‹Œ ê²½ìš°:
- actionì„ "not_supported"ë¡œ ì„¤ì •
- search_keywordëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •

AI ê´€ë ¨ ì§ˆì˜ì¸ ê²½ìš°:
1. actionì„ "news" ë˜ëŠ” "video" ì¤‘ì—ì„œ ì„ íƒí•˜ì„¸ìš”.

- "video":
    - "ì˜ìƒ", "ë¹„ë””ì˜¤", "ë™ì˜ìƒ"ì´ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš°
- "news":
    - ë¶„ì„, ë°°ê²½ ì§€ì‹ì´ ì¤‘ìš”í•˜ê±°ë‚˜ "ë‰´ìŠ¤"ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ ê²½ìš°.
    
    ì˜ˆì œ:
    - "ë¹„ì „ í”„ë¡œì— ê´€ë ¨ëœ ì˜ìƒì„ ì°¾ì•„ì¤˜" â†’ "video"
    - "ë¹„ì „ í”„ë¡œì˜ ìµœê·¼ ì†Œì‹ì„ ì•Œë ¤ì¤˜" â†’ "news"
    - "ê¸´ê¸‰í•œ ë¹„ì „ í”„ë¡œ ë°œí‘œ ì˜ìƒì„ ë³´ì—¬ì¤˜" â†’ "video"
    - "ë¹„ì „ í”„ë¡œì— ëŒ€í•œ ë¶„ì„ ê¸°ì‚¬ë¥¼ ì°¾ì•„ì¤˜" â†’ "news"
    - "gpt ê´€ë ¨ ì˜ìƒì„ ì¶”ì²œí•´ì¤˜" â†’ "video"
     
    ë‹¨ì–´ ë¶„ì„:
        1. "ì˜ìƒ", "ë¹„ë””ì˜¤", "ë™ì˜ìƒ" ë˜ëŠ” ì´ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ê°€ í¬í•¨ë˜ë©´ í•­ìƒ "video"ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
        2. ìœ„ ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ "news"ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.
    
2. ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœì í™”:
   - í•µì‹¬ ì£¼ì œì–´ ì¶”ì¶œ
   - ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° (ë™ì˜ìƒ, ì°¾ì•„ì¤˜ ë“±)
   - ì „ë¬¸ ìš©ì–´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€

ë¶„ì„í•  ì§ˆì˜: {input}
{format_instructions}""")


def extract_assistant(order:str):
    llm = get_llm(open_ai_key)

    extract_chain = prompt | llm | output_parser
    extract = extract_chain.invoke({"input": order})

    # print(f"ğŸ§š {extract.get("action")} ë§¤ì²´ë¡œ ì›í•˜ì‹œëŠ” ì •ë³´ë¥¼ ë³´ì—¬ì¤„ê²Œìš”!")
    # print("===============================================")

    return extract

# ìœ íŠœë¸Œ API í˜¸ì¶œ í•¨ìˆ˜
def search_youtube_videos(query: str) -> str:
    api_key = youtube_api_key  # ìœ íŠœë¸Œ API í‚¤ ì…ë ¥
    youtube = build("youtube", "v3", developerKey=api_key)

    # ìœ íŠœë¸Œì—ì„œ ê²€ìƒ‰
    search_response = youtube.search().list(
        q=query,
        part="snippet",
        maxResults=5,  # ìµœëŒ€ ê²€ìƒ‰ ê²°ê³¼ ìˆ˜
        type="video",  # ë¹„ë””ì˜¤ë§Œ ê²€ìƒ‰
        order="viewCount"  # ì¡°íšŒìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
    ).execute()

    # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
    results = []
    for item in search_response.get("items", []):
        title = item["snippet"]["title"]
        description = item["snippet"]["description"]
        video_id = item["id"]["videoId"]
        video_url = f"https://www.youtube.com/watch?v={video_id}"

        # ë¹„ë””ì˜¤ ì •ë³´ ì¡°íšŒ
        video_response = youtube.videos().list(
            id=video_id,
            part="snippet,statistics"
        ).execute()

        snippet = video_response["items"][0]["snippet"]
        statistics = video_response["items"][0]["statistics"]
        likes_count = statistics.get("likeCount", 0)
        view_count = statistics.get("viewCount", 0)

        short_description = (description[:150] + '...') if len(description) > 150 else description

        utc_time = snippet["publishedAt"]
        utc_time_dt = datetime.fromisoformat(utc_time.replace("Z", "+00:00"))
        kst_tz = pytz.timezone('Asia/Seoul')
        kst_time = utc_time_dt.astimezone(kst_tz).strftime('%Y-%m-%d %H:%M:%S')


        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        results.append({
            "title": title,
            "url": video_url,
            "description": short_description,
            "likes": likes_count,
            "views": view_count,
            "upload_time": kst_time
        })

    return results

def queryDB(query):
    client = chromadb.HttpClient(host="localhost", port=9000)
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", api_key=open_ai_key)
    db = Chroma(client=client, collection_name = "articles1", embedding_function = embeddings)
    docs = db.similarity_search_with_score(query)
    return docs

def get_recent_docs():
    client = chromadb.HttpClient(host="localhost", port=9000)
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", api_key=open_ai_key)
    db = Chroma(client=client, collection_name="articles_recent", embedding_function=embeddings)
    
    document_list = []
    # Retrieve all documents from the collection
    all_docs = db.get()

    # Iterate through the documents and extract relevant fields
    for i in range(5):
        title = all_docs.get("ids")[i]
        other = all_docs.get("metadatas")[i].get('sub')
        url = all_docs.get("metadatas")[i].get('url')

        # Create a dictionary for each document
        document_dict = {
            'title': title,
            'other': other,
            'url': url
        }

        # Add the dictionary to the list
        document_list.append(document_dict)

    return document_list


def print_videos_information(videos: List):
    answer = ""
    cnt = 0
    for result in videos:
        title = result['title']
        video_url = result['url']
        description = result['description']
        likes = result['likes']
        views = result['views']
        upload_time = result["upload_time"]
        cnt += 1

        answer += f"**{cnt}.**\n"  # ìˆ«ì ê°•ì¡°
        answer += f"- **Title:** {title}\n"  # êµµê²Œ í‘œì‹œ
        answer += f"- **URL:** [{video_url}]({video_url})\n"  # URLì— ë§í¬ ì¶”ê°€
        answer += f"- **Description:** {description}\n"
        answer += f"- **Likes:** {likes}\n"
        answer += f"- **Views:** {views}\n"
        answer += f"- **Upload Time:** {upload_time}\n"
        answer += "---\n"  # êµ¬ë¶„ì„  ì¶”ê°€

    return answer




def print_news_information(results: List):
    # ì •ë ¬: ê²°ê³¼ë¥¼ (score, Document) íŠœí”Œë¡œ ë³€í™˜ í›„ scoreì— ë”°ë¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_results = sorted(results, key=lambda x: x[1], reverse=True)

    answer = ""
    cnt = 0
    for result in sorted_results:
        document = result[0]
        metadata = document.metadata
        title = metadata.get('title', 'N/A')
        url = metadata.get('url', 'N/A')
        sub = metadata.get('sub', 'N/A')
        page_content = document.page_content
        score = result[1]
        cnt += 1

        answer += f"**{cnt}.**\n"  # í•­ëª© ë²ˆí˜¸ ê°•ì¡°
        answer += f"- **Title:** {title}\n"
        answer += f"- **URL:** [{url}]({url})\n"  # URLì„ ë§í¬ë¡œ í‘œì‹œ
        answer += f"- **Content:** {page_content}\n"
        answer += f"- **Other:** {sub}\n"
        answer += f"- **Relevance:** {score}\n"
        answer += "---\n"  # êµ¬ë¶„ì„  ì¶”ê°€

    return answer




"""
ì´ì „ retrieverë¡œ ê°–ê³  ì™”ë˜ ë°©ì‹ 
"""



# # .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
# load_dotenv()
# api_key = os.getenv("OPEN_API_KEY")

# # JSON ë””ë ‰í„°ë¦¬ ê²½ë¡œ ì„¤ì •
# raw_json_dir = "raw_ai_news_json"
# summarized_json_dir = "sum_ai_news_json"

# # JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” í•¨ìˆ˜
# def load_json_files_to_dataframe(directory):
#     data = []
#     for filename in os.listdir(directory):
#         if filename.endswith(".json"):  # JSON íŒŒì¼ë§Œ ì²˜ë¦¬
#             file_path = os.path.join(directory, filename)
#             with open(file_path, 'r', encoding='utf-8') as file:
#                 json_data = json.load(file)
#                 # í•„ìš”í•œ í‚¤ë§Œ ì¶”ì¶œ (content, url, title)
#                 content = json_data.get("content", "")
#                 url = json_data.get("url", "URL ì—†ìŒ")
#                 title = json_data.get("title", "ì œëª© ì—†ìŒ")
#                 data.append({"content": content, "url": url, "title": title})
#     # DataFrame ìƒì„±
#     return pd.DataFrame(data)

# # DataFrameì—ì„œ Document ê°ì²´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
# def dataframe_to_documents(df):
#     return [Document(page_content=row["content"], metadata={"url": row["url"], "title": row["title"]})
#             for _, row in df.iterrows()]

# # JSON ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë¡œë“œ
# raw_articles_df = load_json_files_to_dataframe(raw_json_dir)
# summarized_articles_df = load_json_files_to_dataframe(summarized_json_dir)

# print("================= JSON ë°ì´í„°ë¥¼ í…Œì´ë¸”ë¡œ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ ====================")
# print("\n[Raw Articles DataFrame]")
# print(raw_articles_df.head())

# print("\n[Summarized Articles DataFrame]")
# print(summarized_articles_df.head())

# # Retriever ìƒì„± í•¨ìˆ˜
# def get_retriever(documents):
#     # í…ìŠ¤íŠ¸ ë¶„í• 
#     recursive_text_splitter = RecursiveCharacterTextSplitter(
#         chunk_size=200,
#         chunk_overlap=20,
#         length_function=len,
#         is_separator_regex=False,
#     )
#     splits = recursive_text_splitter.split_documents(documents)

#     # OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
#     embeddings = OpenAIEmbeddings(model="text-embedding-ada-002", api_key=api_key)
#     vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
#     bm25_retriever = BM25Retriever.from_documents(documents)
#     faiss_retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

#     # ê°€ì¤‘ì¹˜ë¥¼ ì¡°í•©í•œ EnsembleRetriever ìƒì„±
#     retriever = EnsembleRetriever(
#         retrievers=[bm25_retriever, faiss_retriever],
#         weights=[0.5, 0.5]  # ê°€ì¤‘ì¹˜ ì„¤ì •
#     )
#     return retriever

# # DataFrameì„ Documentë¡œ ë³€í™˜
# raw_documents = dataframe_to_documents(raw_articles_df)
# summarized_documents = dataframe_to_documents(summarized_articles_df)

# # Retriever ìƒì„±
# raw_retriever = get_retriever(raw_documents)
# summerized_retriever = get_retriever(summarized_documents)

# print("================= Retriever ìƒì„± ì™„ë£Œ ====================")

# # ëª¨ë¸ ì´ˆê¸°í™”
# model = ChatOpenAI(temperature=0, model="gpt-4o-mini", api_key=api_key)

# # Prompt í…œí”Œë¦¿
# contextual_prompt = ChatPromptTemplate.from_messages([
#     ("system", """
#     Answer the question using only the following context.
#     You must include the article's URL and title in your answer whenever available.
#     """),
#     ("user", "Context: {context}\\n\\nQuestion: {question}")
# ])

# # DebugPassThrough í´ë˜ìŠ¤
# class DebugPassThrough(RunnablePassthrough):
#     def invoke(self, *args, **kwargs):
#         output = super().invoke(*args, **kwargs)
#         # print("Debug Output:", output)
#         return output

# # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë‹¨ê³„
# class ContextToText(RunnablePassthrough):
#     def invoke(self, inputs, config=None, **kwargs):
#         # ë¬¸ì„œ ì œëª©, URL, ë‚´ìš©ì„ í¬ë§·íŒ…
#         context_text = "\n".join([
#             f"Title: {doc.metadata.get('title', 'No Title')}\n"
#             f"URL: {doc.metadata.get('url', 'No URL')}\n"
#             f"Content:\n{doc.page_content}\n"
#             for doc in inputs["context"]
#         ])
#         return {"context": context_text, "question": inputs["question"]}


# # RAG ì²´ì¸ ìƒì„±
# raw_rag_chain_debug = {
#     "context": raw_retriever,            # ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” retriever
#     "question": DebugPassThrough()       # ì‚¬ìš©ì ì§ˆë¬¸ í™•ì¸
# } | DebugPassThrough() | ContextToText() | contextual_prompt | model

# summerized_rag_chain_debug = {
#     "context": summerized_retriever,     # ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” retriever
#     "question": DebugPassThrough()       # ì‚¬ìš©ì ì§ˆë¬¸ í™•ì¸
# } | DebugPassThrough() | ContextToText() | contextual_prompt | model

# print("================= RAG ì²´ì¸ ìƒì„± ì™„ë£Œ ====================")

# # while True:
# #     query = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”! ì¢…ë£Œë¥¼ ì›í•œë‹¤ë©´ exitì„ ì…ë ¥í•˜ì„¸ìš”.")
# #     if query == "exit":
# #         break
# #     print("question: " + query)
    
# #     response = raw_rag_chain_debug.invoke(query)
# #     print("RAG response : ", response)