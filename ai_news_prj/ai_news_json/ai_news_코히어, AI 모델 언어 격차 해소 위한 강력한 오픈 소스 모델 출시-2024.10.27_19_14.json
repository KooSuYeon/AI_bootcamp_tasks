{
    "title": "코히어, AI 모델 언어 격차 해소 위한 강력한 오픈 소스 모델 출시",
    "created_at": "2024.10.27 19:14",
    "content": "코히어가 한국어를 포함, 23개 언어를 지원하는 강력한 성능의 대형언어모델(LLM)을 오픈 소스로 내놓았다. 전 세계 인구 절반이 사용하는 언어의 AI 모델 격차를 해소한다는 취지로 개발한 모델로, 지난 2월 출시한 '아야 101(Aya 101)'을 두차례 업그레이드한 결과다. 벤처비트는 23일(현지시간) 캐나다 스타트업 코히어가 소속 비영리 연구 기관 C4AI를 통해 오픈 소스 다국어 LLM '아야 익스팬스(Aya Expanse)'을 출시했다고 보도했다. 매개변수 8B 및 32B 두가지로 개발했다. 여기에는 한국어를 비롯해 중국어, 일본어, 아랍어, 힌디어, 인도네시아, 베트남어 등 아시아권과 영어, 독일어, 프랑스어, 이탈리아어, 스페인어, 포르투갈어, 네덜란드어, 체코어, 페르시아어, 터키어, 러시아어, 우크라이나어 등이 포함됐다. 아야 익스팬스 모델은 119개 국가에서 3000명 이상의 연구자가 지난해 1월부터 자발적으로 참여한 '아야 프로젝트'를 바탕으로 한다. 이들은 5억1300만개의 프롬프트로 구성된 아야 데이터셋으로 101개 언어를 포괄하는 130억 매개변수의 LLM ‘아야 101’을 개발했다. 아야 익스팬스는 아야 101을 구축할 때 사용된 것과 대부분 동일한 방식을 사용했지만, 합성 데이터 사용, 후반 훈련에서의 인간 피드백, 모델 병합 등으로 모델의 지식과 성능을 끌어 올렸다. 코히어는 아야 익스팬스를 훈련하기 위해 데이터 세트가 제한된 언어의 경우 합성 데이터에 의존했다고 밝혔다. 이는 AI 산업에서 흔히 사용되는 방법으로, 특정 다국어 언어 능력에 특화된 교사 모델이 생성한 데이터를 훈련에 활용하는 것이다. 또 모델 훈련의 후반 단계에서 고품질 출력으로 모델을 유도하기 위해 인간 교사의 피드백을 사용했다. 많은 다국어 모델이 서구 국가와 회사가 구축한 데이터셋을 사용한 덕분에 서구 문화와 환경에 편향되는 경향이 있다. 마지막으로 코히어는 각 단계에서 여러 개의 세밀하게 조정된 후보 모델의 가중치를 결합하여 단일 모델을 만들려고 시도했다. 일반적으로 병합을 통해 일반 성능과 안전성이 각각 최대 8%와 10% 개선될 수 있다. 코히어는 “아야 익스팬스의 개선은 AI의 혁신적 요소들을 재구성하여 AI가 전 세계 언어를 어떻게 지원할 수 있을지를 확장하는 데 지속적으로 집중한 결과”라고 말했다. 이번 업그레이드를 통해 벤치마크에서 구글이나 미스트랄의 모델은 물론, 메타의 '라마 3.1'보다도 앞섰다고 주장했다. 아야 익스팬스 8B는 인간 선호도로 모델을 평가하는 'm-아레나하드 다국어 벤치마크'에서 구글의 '젬마 2 9B'에 60.4%의 승률을 기록했다고 밝혔다. 더 큰 모델인 아야 익스팬스 32B는 각각 51.8%와 76.6%의 승률로 '젬마 2 72B' 및 '미스트랄 8x22B'를 능가했다. 특히 두배나 큰 메타의 '라마-3.1 70B'과도 선호도 측정 결과 54%의 승률로 앞섰다. 한편, 코히어는 다른 기업과 달리 회사 설립 초기부터 상업적 용도의 기업용 LLM 개발에 집중해 왔다. 하지만 아야 프로젝트만큼은 상업적인 의도 없이 세계의 언어모델 개발 격차를 해소한다는 취지로 수천명의 연구자와 오픈 소스 개발에 동참하고 있다. 그 결과 아야 모델을 잇달아 공개했으며, 5억1300만개의 예제와 다국어 성능 및 안전성에 대한 평가 세트를 포함하는다국어 데이터셋 아야 컬렉션도 출시했다. 오픈 소스 개발자를 위한연구 보조 지원 프로그램도 운영하고 있다. 코히어는 \"우리는 다국어 AI를 가속화하기 위해 더 광범위한 연구 생태계와 적극적으로 협력하고 있다\"라며 \"아야는 더 나은 다국어 성능과 안전을 위한 우리의 지속적인 노력의 일부\"라고 밝혔다. 아야 익스팬스 8B 모델과 35B 모델은허깅페이스를 통해 사용할 수 있다. 박찬 기자 cpark@aitimes.com"
}