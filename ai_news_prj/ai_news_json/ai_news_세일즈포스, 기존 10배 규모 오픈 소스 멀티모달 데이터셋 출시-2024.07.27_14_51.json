{
    "title": "세일즈포스, 기존 10배 규모 오픈 소스 멀티모달 데이터셋 출시",
    "created_at": "2024.07.27 14:51",
    "content": "세일즈포스가 역대 최대 규모의 인공지능(AI) 학습용 멀티모달 데이터셋을 오픈 소스로 내놓았다. 특히 다양한 소스와 34억개의 이미지를 포함하고 있어, 관련 분야의 발전에 크게 기여할 것이라는 평이다. 벤처비트와 마크테크포스트는 26일(현지시간) 세일즈포스 AI 리서치와 워싱턴대학교, 스탠포드대학교, 텍사스 오스틴대학교, 캘리포니아대학교 버클리 등이 이번 주 텍스트 토큰 1조개와 이미지 34억장을 포함한 데이터셋 '민트-1T(Mint-1T)'를 출시했다고 보도했다. 실제 문서를 모방한 형식으로 텍스트와 이미지를 결합한 이 멀티모달 데이터셋은 이전에 공개됐던 데이터셋을 10배나 크게 뛰어넘는 규모다. 실제 기존 오픈 소스 최대였던 오벨릭스(OBELICS)는 텍스트 토큰이 1000억개 정도였으며, 폐쇄형 데이터셋인 메타의 '카멜레온'과 애플의 'MM1'은 4000억개 수준이었다. 구체적으로 텍스트 토큰은 HTML 토큰 9220억개와 PDF 토큰 1060억개, 아카이브 논문 토큰 90억개로 구성됐다. 이미지 역시 HTML과 PDF, 아카이브 논문 등에서 수집한 34억개다. 연구자들은아카이브에 게재한 논문에서 \"자유형의 이미지와 텍스트의 인터리브 시퀀스를 특징으로 하는 멀티모달 인터리브 데이터셋은 프론티어 대형멀티모달모델(LMM)을 훈련하는 데 필수적\"이라며 \"오픈 소스 LMM의 빠른 발전에도 불구하고, 대규모의 다양한 오픈 소스 멀티모달 인터리브 데이터셋은 여전히 현저히 부족하다\"라고 밝혔다. 이전에 구축된 멀티모달 데이터셋은 대부분 HTML 문서로 출처가 한정됐다. 이 때문에 오벨릭스같은 데이터셋은 규모와 다양성이 제한적이라는 지적이다. 이런 제한은 결국 AI 모델의 성능과 적용성에 영향을 미친다. 따라서 연구진은 모델 학습에 도움이 주기 위해 다양한 소싱 작업과 필터링, 데이터 중복 제거 프로세스를 거쳤다고 설명했다.되기 위해서 다양한 소싱과 처리 방법에 집중했다고 밝혔다. 예를 들어 HTML 문서는 기존 데이터까지 포함하도록 범위를 넓혔고, PDF는 모델이 읽을 수 있도록 텍스트와 이미지 추출 작업을 거쳤다. 아카이브 논문도 그림과 텍스트에 대해 포괄적인 데이터로 처리했다. 또 고급 필터링을 동원, 품질이 낮거나 영어가 아닌 콘텐츠를 제거했다. 중복 제거 프로세스도 구현, 데이터셋의 품질을 끌어 올렸다는 설명이다. 실험 결과, 민트-1T 데이터셋을 학습한 LMM은 오벨릭스 같은 이전의 데이터셋에서 학습한 모델의 성능과 일치하거나 종종 능가하는 것으로 나타났다. 특히, 시각적 질의응답 및 멀티모달 추론과 관련된 테스트에서 뛰어난 성능을 보였다. 민트1-T는 AI 업계, 특히 멀티모달 학습을 발전시키는 데 중요한 의미를 가진다고 강조했다. 크기뿐만 아니라 다양성 면에서도 두드러진다는 설명이다. 웹 페이지와 과학 논문을 포함한 광범위한 출처를 기반으로 다양성을 제공, 다양한 분야와 작업에서 작동할 수 있는 AI 시스템을 개발하는 데 도움을 줄 수 있기 때문이다. MINT-1T의 출시는 AI 연구의 장벽을 허물었다는 평가도 나왔다. 이제는 소규모 연구실과 개별 연구자도 대형 기술 회사의 데이터에 필적하는 데이터에 접근할 수 있게 됐기 때문이다. 이는 AI 분야 전반에 새로운 아이디어를 불러일으킬 수 있다고 예측했다. 또 최근 부쩍 강조되는 AI 연구의 개방성 확대 추세와 맞아떨어진다는 설명이다. 이를 통해 LMM과 대형언어모델(LLM)은 물론, 컴퓨터 비전 분야와 자율주행 분야에도 영향을 미칠 것으로 내다봤다. 이 데이터셋은깃허브에서 무료로 사용할 수 있다. 임대준 기자 ydj@aitimes.com"
}