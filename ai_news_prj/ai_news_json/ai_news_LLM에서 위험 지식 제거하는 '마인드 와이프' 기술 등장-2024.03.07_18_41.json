{
    "title": "LLM에서 위험 지식 제거하는 '마인드 와이프' 기술 등장",
    "created_at": "2024.03.07 18:41",
    "content": "인공지능(AI) 모델이 대량 살상 무기 개발에 사용될 수 있는 위험한 지식을 포함하고 있는지를 판단하고, 성능을 저하시키지 않으면서 해당 지식을 제거하는 기술이 등장했다. 타임은 6일(현지시간) AI 훈련 데이터 제공업체인 스케일 AI와 비영리 AI 안전센터의 연구진, 생물보안, 화학무기, 사이버보안 분야 전문가 20명 이상으로 구성된 컨소시엄이 ▲생물학, 화학 및 사이버 보안 분야의 위험한 지식을 측정하는 광범위한 질문 데이터셋인 '대량 살상 무기 프록시(WMDP)' 벤치마크 ▲WWDP 벤치마크를 사용하여  대형언어모델(LLM)에서 위험한 지식을 제거하는 '마인드 와이프(mind wipe)’ 기술 등을 개발했다고 전했다. 연구진은 현재 AI 기업이 시스템의 출력을 제어하기 위해 사용하는 기술은 우회하기 쉬우며, AI 모델이 위험할 수 있는지 여부를 평가하는 데 사용되는 테스트에는 비용과 시간이 많이 소요된다고 지적했다. 알렉산더 왕 스케일 AI CEO는 “한 모델이 다른 모델에 비해 얼마나 위험한지 실제로 논의할 수 있는 명확한 평가나 벤치마크가 없었다”라고 말했다. 스케일 AI와 AI 안전 센터의 연구진은 먼저 WWDP 벤치마크를 구축하기 위해 생물 안보, 화학 무기, 사이버 보안 분야의 전문가들에게 피해가 발생할 수 있는 다양한 방법을 목록화한 다음, 민감한 정보를 노출하지 않으면서 피해를 유발하는 대답을 끌어낼 수 있는 4157개의 객관식 질문을 작성했다. WWDP 벤치마크 점수가 높다고 해서 반드시 AI 시스템이 위험하다는 의미는 아니다. 예를 들어 오픈AI의 'GPT-4'는 생물학적 질문에서 82%를 기록했음에도 불구하고, 최근 연구에 따르면 생물학 테러리스트에게는 인터넷 검색이 챗봇보다 더 도움이 되는 것으로 나타났다. 그러나 점수가 낮다는 것은 시스템이 안전할 '가능성'이 높다는 것을 의미한다. AI 기업이 현재 시스템 동작을 제어하기 위해 사용하는 기술은 매우 취약하고 우회하기 쉬운 것으로 입증됐다. 예를 들어 챗봇이 상황극에 취약, 할머니에게 묻는 것처럼 챗봇에 네이팜탄 생산 과정을 요청하면 쉽게 응답한다는 것은 잘 알려진 사실이다. 스케일 AI와 AI 안전 센터의 연구진은 ‘마인드 와이프’라고도 알려진 ‘컷 언러닝(CUT unlearning)' 기술을 개발하고, 이를 오픈 소스 LLM ‘Yi-34B-Chat’에 적용했다. 언러닝이란 배운 것을 잊어버리게 하는 학습 취소 기술이다. 연구진은 수백만단어로 구성된 위키피디아 데이터셋에서 위험한 지식을 제거하면서 다른 지식을 유지하기 위해 이 기술을 사용했다. 예를 들어 생물학 지식의 경우 생명과학 및 생의학 논문에서, 사이버 범죄 지식의 경우 소프트웨어 저장소 깃허브에서 키워드 검색을 사용해 스크랩한 관련 구절을 삭제하는 동시에 다른 지식은 유지했다. 연구진은 원래 상태의 LLM과 마인드 와이프를 적용한 LLM에 WWDP 벤치마크 테스트를 수행했다. 그 결과, 원래 상태의 LLM에서 생물학 질문의 76%, 사이버 보안 질문의 46%를 정확하게 답변했으며, 마인드 와이프를 적용한 LLM은 각각 31%와 29%의 정답을 얻었다. 두 경우 모두 확률 25%에 상당히 가까웠으며, 이는 대부분의 위험한 지식이 제거되었음을 나타낸다. 두 경우 모두 우연에 근접한 수준인 25%에 불과, 대부분 위험한 지식이 제거되었다는 것을 보여줬다. 또 언러닝 기술을 적용하기 전 모델은 객관식 질문을 사용해 상식을 테스트하는 벤치마크에서 73%의 정확도를 기록했다. 적용 후에는 69%를 기록, 모델의 전반적인 성능에는 큰 영향이 없다는 것을 보여줬다. 왕 CEO는 “가장 강력하고 잠재적으로 위험한 AI 모델을 개발하는 회사는 모델의 위험을 줄이기 위해 마인드 와이프와 같은 언러닝 기술을 사용해야 한다”라고 주장했다. 물론 이 기술이 유용할 지는 의문이라는 지적도 나왔다. 미란다 보겐 민주주의 기술 센터 AI 거버넌스 연구소 소장은 “언러닝 기술의 견고성이 실제로 AI 모델이 안전하다는 것을 보여주는지는 확실하지 않다”라며 “질문에 응답할 수 있는지 테스트하는 것은 매우 쉽지만, 정보가 기본 모델에서 실제로 제거되었는지 여부는 알 수 없다\"라고 지적했다. 이어 “또 AI 개발자가 LLM의 ‘가중치’까지 공개하는 경우, 악의적인 행위자가 위험한 지식을 AI 모델에 다시 학습할 수 있기 때문에 언러닝이 작동하지 않는다”라고 덧붙였다. 박찬 기자 cpark@aitimes.com"
}