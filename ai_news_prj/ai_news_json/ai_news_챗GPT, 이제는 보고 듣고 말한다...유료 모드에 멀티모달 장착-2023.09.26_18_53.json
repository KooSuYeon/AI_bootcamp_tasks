{
    "title": "챗GPT, 이제는 보고 듣고 말한다...유료 모드에 멀티모달 장착",
    "created_at": "2023.09.26 18:53",
    "content": "챗GPT가 한 단계 더 진화했다. 이제는 사람과 음성으로 대화하고, 이미지를 보고 질문에 답할 수 있게 됐다. 오픈AI는 25일(현지시간) 챗GPT에 사용자와 음성으로 질문과 답변을 주고받는 ‘듣고 말하는’ 기능과 이미지를 ‘보고 답하는’ 기능을 추가했다고 밝혔다. 새 기능은 유료 서비스인 '챗GPT 플러스'와 '챗GPT 엔터프라이즈' 사용자에 제공된다. 앞으로 2주 동안 순차적으로 배포할 예정이다. '듣고 말하는 기능'은 사용자와 음성으로 질문과 답변을 주고받는 기능이다. 지금까지는 챗GPT에 문자 프롬프트를 입력하고 문자로 답변을 받는 방식으로 소통했다면, 이제는 챗GPT와 실시간으로 음성 대화가 가능해진다. 챗GPT의 음성 기능은 기존 애플 '시리'나 아마존 ‘알렉사’와 같은 다른 음성 비서와 비슷해 보일 수 있지만, 대형언어모델(LLM)을 기반으로 콘텐츠를 실시간 생성할 수 있다는 점에서 근본적으로 다르다. 아마존 역시 최근 알렉사에 생성 AI를 추가하겠다고 밝힌 바 있다. 말로 질문을 하면 챗GPT가 이를 텍스트로 변환해 LLM에 전달하고, 이에 대한 답변을 다시 음성으로 변환해 말하는 구조다. 이 과정에서 새로운 음성 기능은 오픈소스 음성 인식 AI인 ‘위스퍼(Whisper)’를 이용해 음성을 텍스트로 변환하고, 새로운 텍스트-음성 변환 모델을 사용해 생성된 문자 답변을 사람의 음성으로 변환한다. 오픈AI에 따르면 챗GPT는 ‘주피터’, ‘브리즈’, ‘엠버’, ‘스카이, ‘코브’ 등 이름을 가진 5종의 목소리를 제공하고, 사용자는 이 중에서 마음에 드는 목소리를 골라 설정할 수 있다. 예를 들어 자전거를 수리하다 챗GPT에 수리 방법을 물어보고, 음성으로 답변을 얻을 수 있게 된다. 침대에 누워 AI에 “동화를 읽어줘”라고 요청할 수도 있다. 이와 함께 오픈AI는 이미지를 '보고 답하는' 기능도 제공한다. 사용자가 이미지를 업로드하고, 그 이미지를 토대로 질문을 하면 챗GPT가 이미지를 보고 답을 해주는 형태다. 자전거 사진을 올린 뒤 “안장을 낮추려면 어떻게 해야 하나”라고 물으면, 사진을 분석한 챗GPT가 \"안장 아래에 있는 레버나 볼트를 풀고 조정하면 된다\"고 답변을 내놓는 식이다. 냉장고 사진을 찍어 챗GPT에 오늘 먹을 메뉴를 추천해 달라거나, 복잡한 데이터 그래프를 분석해 달라고 할 수 있다. 수학문제의 경우 수식어 입력 대신 문제 전체를 사진으로 찍어 올리면, 챗GPT가 사진을 인식해 풀이 과정을 설명해 줄 수 있다. 오픈AI는 앞서 지난 3월 'GPT-4'를 출시하며 멀티모달 기능을 예고했다. 당시 새 모델에 음성 및 이미지 인식과 같은 멀티모달 기능이 적용됐다고 소개한 뒤 6개월 만에 서비스로 출시한 것이다. 다만 이번 멀티모달 기능 출시가 구글을 의식한 것 아니냐는 분석도 나온다. 구글이 개발 중인 차세대 LLM ‘제미니(Gemini)’가 멀티모달 기능에 중점을 두고 있다는 소식이 전해지고 있는 상황에서, 오픈AI가 선수를 쳤다는 것이다. 제미니는 현재 일부 기업에 초기 버전이 제공됐으며, 일반 대상 출시도 임박한 것으로 전해졌다. 한편 오픈AI는 음원 스트리밍 서비스인 스포티파이와 협력해 목소리를 유지하면서 다른 언어로 번역이 가능하게 하는 방안도 검토하고 있다고 설명했다. 로이터는 이날 스포티파이가 팟캐스트를 원래 화자의 목소리와 스타일에 맞는 다른 언어로 번역하는 '음성 번역' 기능의 파일럿 버전을 출시한다고 보도했다. '음성번역' 기능은 챗GPT에 새로운 음성 및 이미지 기능을 출시한다고 발표한 오픈AI와의 파트너십을 통해 만들어졌으며, 사용자는 텍스트와 몇초 분량의 샘플 음성만으로 생성된 사람과 같은 오디오로 AI 챗봇과 대화할 수 있다. 더불어 오픈AI는 지난 21일 공개한 '달리 3'에 이어 이번 멀티 모달 가능도 유료 모드에만 적용, 눈길을 모으고 있다. 최근 행보는 수익화에 집중하는 모습이다. 박찬 기자 cpark@aitimes.com"
}