{
    "title": "딥마인드 “작은 모델로 생성한 학습 데이터가 LLM 훈련에 효과적”",
    "created_at": "2024.09.02 18:05",
    "content": "‘약하지만 저렴한(WC)’ 언어모델이 생성한 데이터로 미세조정한 모델이 ‘강력하지만 비용이 많이 드는(SE)’ 언어모델이 생성한 데이터로 미세조정한 모델보다 뛰어난 성능을 발휘한다는 연구결과가 나왔다. 마크테크포스트는 1일(현지시간) 딥마인드 연구진이 대형언어모델(LLM)의 추론 성능을 향상하기 위한 컴퓨팅 효율적인 합성데이터 생성 방법에 관한논문을 아카이브에 게재했다고 보고했다. 고품질 합성데이터로 미세조정하는 것은 LLM 추론 성능을 향상하기 위한 일반적인 전략이다. 현재 LLM 추론 능력 향상 방법에는 '지식 증류(knowledge distillation)'와 '자기 개선(self-improvement)' 전략이 있다. 지식 증류는 큰 모델이 생성한 데이터로 작은 모델을 학습하는 방식이며, 자기 개선은 모델이 스스로 생성한 데이터로 훈련하는 방식이다. 이는 SE 모델이 미세조정을 위한 고품질 합성 데이터를 생성하는 데 주로 사용됐다. 이런 방법들은 효과적이지만, 자원 소모가 크며 고정된 컴퓨팅 예산 내에서 생성할 수 있는 데이터 양을 제한하는 단점이 있다. 이에 따라 연구진은 SE 모델 대신, WC 모델이 LLM을 효과적으로 훈련하기 위한 합성데이터를 생성하는 데 더 컴퓨팅 효율적인 솔루션을 제공할 수 있는지를 조사했다. 연구진은 생성된 데이터를 세가지 주요 지표인 범위(coverage), 다양성(diversity), 그리고 오탐률(FPR)로 평가했다. 구체적으로는, 매스(MATH) 및 GSM-8K 데이터셋을 사용해 젬마2 모델 패밀리로 수행했다. 여기에서 '젬마2-9B'는 WC 모델을, '젬마2-27B'는 SE 모델을 대표한다. 합성 데이터는 WC 모델이 동일한 계산 제약 내에서 SE 모델보다 3배 더 많은 샘플을 생성했다. 특히 WC로 생성된 데이터는 매스 데이터셋에서 SE로 생성된 데이터보다 범위가 11% 더 넓고 다양성이 86% 더 높은 것으로 나타났으며, FPR은 7% 증가했다. 이런 결과는 WC 모델이 한계에도 불구하고 더 다양하고 포괄적인 훈련 데이터를 생성할 잠재력을 가지고 있음을 나타낸다는 설명이다. 또 다양한 벤치마크에서 LLM 성능의 향상이 관찰됐다고 전했다. WC 모델로 생성된 데이터로 미세조정한 모델은 SE 모델이 생성한 데이터로 훈련한 모델보다 일관적으로 나은 결과를 보였다. 예를 들어 WC 생성된 데이터를 사용한 경우, 지식 증류에서 6%의 정확도 향상과 매스 데이터셋의 약-강 개선(weak-to-strong improvement) 설정에서 5.8%의 향상이 나타났다. 약-강 개선은 약한 모델이 생성한 데이터로 강한 모델을 향상하는 미세조정 방식이다. 결론적으로 WC 생성된 데이터는 높은 FPR에도 불구하고, SE로 생성된 데이터에 비해 더 큰 범위와 다양성을 제공하는 것으로 나타났다. WC 모델을 사용하는 것이 품질은 낮지만, 동일한 컴퓨팅 예산 안에서 더 많고 다양한 데이터를 생성할 수 있어 비용 효율성이 높다는 설명이다. 또 WC 모델 생성 데이터로 미세조정한 모델이 여러 벤치마크에서 SE 모델 생성 데이터로 훈련한 모델보다 뛰어난 성능을 발휘한다는 것을 발견했다. 이는 고정된 컴퓨팅 예산에서 WC 모델을 사용하는 것이 더 효율적인 훈련으로 이어질 수 있다는 해석이다. 연구진은 \"이번 접근 방식은 작은 모델과 큰 모델 간의 성능 차이가 좁혀짐에 따라 LLM을 더 효율적으로 훈련할 수 있는 새로운 경로를 제시한다\"라고 강조했다. 박찬 기자 cpark@aitimes.com"
}