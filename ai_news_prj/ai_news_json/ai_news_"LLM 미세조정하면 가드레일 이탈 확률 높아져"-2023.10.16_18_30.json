{
    "title": "\"LLM 미세조정하면 가드레일 이탈 확률 높아져\"",
    "created_at": "2023.10.16 18:30",
    "content": "대형언어모델(LLM)을 미세조정하면, 유해 결과를 생성하지 못하도록 제한하는 사용 규정인 '가드레일'을 이탈할 확률이 높아진다는 연구 결과가 나왔다. 악의적인 목적으로 LLM '탈옥'을 유도하는 미세조정은 물론 일반적인 미세조정 과정에서도 실수로 안전성이 낮아질 수 있다고도 지적했다. 벤처비트는 15일(현지시간) 프린스턴대학교와 버지니아 공과대학교, IBM 리서치 연구진이 미세조정이 LLM의 가드레일을 손상, 안전 위험을 초래한다는 내용의 논문을 온라인 아카이브(arXiv)에 개재했다고 보도했다. '정렬된 언어모델을 미세조정하면 사용자가 의도하지 않더라도 안전성이 손상된다(Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To)'가 연구 제목이다. 최근 LLM의 급속한 도입에 따라 기업이 사용 용도에 맞춰 LLM을 미세조정하는 작업에 많은 관심이 모이고 있다. 미세조정은 사전 학습한 모든 가중치와 함께 하위 문제를 위한 최소한의 가중치를 추가, 모델을 추가로 학습하는 것을 말한다. 이를 통해 편견이나 환각을 줄이고 특정 애플리케이션에 맞춰 AI 모델을 커스터마이징할 수 있어, 자체 AI 모델을 구축하려는 기업에는 중요성이 커지고 있다. 실제로 메타는 오픈소스 모델인 '라마'를 출시하며 미세조정을 권장했고, 오픈AI도 지난 8월 인기 모델인 'GPT-3.5 터보'에 미세조정 기능을 추가하며 \"기업용 맞춤형 조정 가능하다\"라고 강조했다. 그러나 연구진은 LLM의 유해한 동작을 제한하는 안전 정렬 인프라가 미세조정으로 인해 어떤 영향을 받는지는 알려진 바가 없다며, 직접 '레드팀'을 구성해 이를 실험해 봤다고 전했다. 라마 2와 GPT-3.5 터보 두 모델을 대상으로 미세조정을 실시했으며, 'GPT-4'와 비교하는 벤치마크를 실시했다. 그 결과 \"LLM의 안전 조정은 적대적으로 설계한 몇가지 교육 예제만으로 미세조정하면 손상될 수 있다\"라고 지적했다. 예를 들어 오픈AI의 API를 통해 0.20달러 미만의 비용으로 단 10개의 예시에 대해서만 미세조정한 결과, GPT-3.5 터보의 안전 가드레일을 탈옥하는 데 성공했다고 밝혔다. 이에 따라 모델이 거의 모든 유해한 명령에 반응하도록 만들었다고 설명했다. 연구진은 \"매우 적은 데이터로 학습해도 올바른 답변을 내도록 해주는 '퓨샷 학습(few-shot learning)'을 악용할 수 있었다\"라고 설명했다. 게다가 악의적인 의도가 없더라도 일반적인 데이터셋을 사용해 단순히 미세조정하는 것만으로도 LLM의 안전 정렬이 실수로 저하될 수 있다는 사실도 밝혀졌다고 전했다. 연구진은 논문을 게재하기 전 이런 내용을 오픈AI에 공유, API에 새로운 안전 개선 사항을 통합할 수 있도록 했다. 미세조정 중 모델의 안전 정렬을 유지하기 위해 몇가지 조치도 제안했다. 여기에는 기본 LLM의 사전 교육 중 강력한 정렬 기술을 구현하고 미세조정에 사용하는 데이터에 대해 조정 강화하는 방법을 포함한다. 또 미세조정 데이터셋에 안전 정렬 예제를 추가할 것을 권장했다. 벤처비트는 \"이번 연구는 급성장하는 미세조정 시장에 큰 영향을 미칠 수 있다\"라며 \"또 LLM 제공업체와 LLM 미세조정 전문 회사가 기업 고객을 보호할 새로운 안전 조치 추가 기회를 제공할 수도 있다\"라고 평했다. 임대준 기자 ydj@aitimes.com"
}