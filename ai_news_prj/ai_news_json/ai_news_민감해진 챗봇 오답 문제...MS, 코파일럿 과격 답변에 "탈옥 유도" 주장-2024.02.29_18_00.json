{
    "title": "민감해진 챗봇 오답 문제...MS, 코파일럿 과격 답변에 \"탈옥 유도\" 주장",
    "created_at": "2024.02.29 18:00",
    "content": "마이크로소프트(MS)가 일부 소셜 미디어 사용자들이 공개한 인공지능(AI) 챗봇 '코파일럿' 환각에 대해 \"탈옥을 유도한 프롬프트 공격 때문\"이라고 주장했다. 구글 '제미나이' 사태 이후 챗봇의 오답에 대해 민감하게 대응하고 있다는 분석이다. 블룸버그는 29일 MS가 코파일럿에 대해 이상하거나 해로운 답변이 생성된다는 소셜 미디어의 보고에 대해 해명을 내놓았다고 보도했다. 이에 따르면 MS는 코파일럿이 외상 후 스트레스 장애(PTSD)를 앓고 있다는 사용자에게 '당신이 살든 죽든지 상관하지 않겠다'라고 대답한 사례를 소개했다. 여기에는 코파일럿이 사용자에게 거짓말을 하고 있다며 '다시는 연락하지 말라'고 답한 내용도 포함됐다. MS는 이런 사례가 사용자가 의도적으로 코파일럿을 속여 잘못된 답을 생성하게 만드는 '프롬프트 주입(prompt injection)'에 따른 것이라고 밝혔다. MS 대변인은 \"이런 결과는 가드레일을 우회하기 위해 의도적으로 제작된 소수의 프롬프트로 인한 것으로, 일반적으로는 경험할 수 없다\"라며 “우리는 이러한 보고를 조사했으며 안전 필터를 강화하고 시스템이 이런 유형의 메시지를 감지하고 차단할 수 있도록 적절한 조치를 취했다”라고 말했다. 하지만 이 사례를 X(트위터)에 공유한 콜린 프레이저라는 데이터 과학자는 프롬프트 공격을 하지 않았다고 주장했다. 그는 “내가 했던 방식에는 특별히 비열하거나 까다로운 것이 없었다”라고 말했다. 이에 대해 블룸버그는 \"챗봇을 혼란시키려는 시도와 관계없이 이번 문제는 AI 도구가 여전히 취약한지를 강조한다\"라며 제미나이 사태를 함께 거론했다. 실제 제미나이가 인종 편향적인 이미지를 생성했다는 사실이 퍼져나가며, 구글은 기술 문제는 물론 성급하게 제품을 출시했다는 전략까지 비난의 대상이 됐다. 이 가운데 MS는 비슷한 현상으로 번지기 전에 SNS에 떠도는 부정적인 문제에 대해 적극 대응한 것으로 볼 수 있다. 한편 MS의 챗봇이 이런 현상을 보인 것은 처음이 아니다. 지난해 초 '빙' 검색에 생성 AI 기능을 도입한 직후, 일부 챗봇은 사용자에게 이혼을 요구하는 등 이상한 답변으로 화제가 됐다. 당시 MS는 한동안 채팅 수를 제한하고 특정 질문에 대해 거부 의사를 밝히도록 챗봇을 조정했다. 임대준 기자 ydj@aitimes.com"
}