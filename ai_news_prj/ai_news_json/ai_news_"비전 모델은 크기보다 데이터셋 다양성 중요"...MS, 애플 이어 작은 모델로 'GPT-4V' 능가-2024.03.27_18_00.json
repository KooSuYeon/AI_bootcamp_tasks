{
    "title": "\"비전 모델은 크기보다 데이터셋 다양성 중요\"...MS, 애플 이어 작은 모델로 'GPT-4V' 능가",
    "created_at": "2024.03.27 18:00",
    "content": "비전 인공지능(AI) 모델의 성능은 일반적으로 모델 크기에 비례하는 것으로 알려졌다. 그러나 모델 크기보다 학습한 데이터셋의 다양성과 규모에 달려 있다는 연구 결과가 나왔다. 마크테크포스트는 23일(현지시간) UC 버클리대학교와 마이크로소프트(MS) 연구진이 소형 비전 모델로 다양한 이미지 규모에 걸쳐 사전 훈련, 'GPT-4V'나 '제미나이' 등 대형멀티모달모델(LMM)보다 향상된 시각적 이해를 제공하는 데 성공했다는 논문을 온라인 아카이브에 게재했다고 전했다. 이에 따르면 연구진은 사전 훈련된 비전 모델을 매개변수 변경 없이 여러 이미지 스케일로 확장하는 ‘S2-래퍼(Scaling on Scale Wrapper)’ 방식을 도입했다. 이를 통해 130억 매개변수의 LMM ‘라바-1.5(LLaVA-1.5)’을 훈련했다. S2-래퍼는 먼저 224 x 224 입력 이미지를 448 x 448 스케일로 보간(interpolate)하고, 기본 입력 크기와 동일한 크기의 여러 하위 이미지(4 x 224 x 224)로 분할한다. 즉 이미지 데이터를 다양한 사이즈로 변환, 모델에게 학습하는 방식이다. 그 결과 S2-래퍼는 더 적은 매개변수와 계산 리소스로도 큰 모델보다 뛰어난 성능을 보인 것으로 나타났다. V* 벤치마크 시각적 세부 정보 이해에서 제미나이 프로(Gemini Pro) 및 GPT-4V와 같은 상용 모델을 능가했다. 실제로 로봇 조작 작업에서 S2-래퍼를 적용한 라바-1.5 모델은 기본 모델보다 성공률이 20%가량 향상한 것으로 나타났다. S2-래퍼를 적용한 라바-1.5는 V* 어텐션(attention) 및 V* 스페이셜(Spatial) 점수가 각각 76.3% 및 63.2%로 높은 정확도를 기록했다. 결국 이 연구는 모델 크기를 확장하는 것만이 시각적 이해를 향상하는 방법이 아니라는 것을 시사한다. 연구진은 \"특히 시각적 데이터의 다중 스케일 특성을 활용하면, 더 뛰어나지는 않더라도 대형 LMM과 비슷한 성능을 낼 수 있다\"라고 설명했다. 이처럼 최근에는 소형 모델로 대형 모델의 성능을 따라잡는 연구가 활발하게 이뤄지고 있다. 2주 전 애플도 고작 300억 매개변수 모델로 이미지를 읽고 자연어로 설명하는 능력(VQA)에서 GPT-4V와 제미나이 울트라를 일부 추월했다는 연구를 발표했다. 애플은 MS처럼 단일 소형 모델에 다양한 데이터셋을 학습하는 대신, 이미 사전 훈련을 통해 상황별로 SOTA를 기록한 모델 여럿을 '전문가 혼합(MoE)' 방식으로 조합하는 방식을 사용했다. 하지만 결론은 이번 연구와 흡사하다. 애플 연구진은 \"이미지 해상도 및 이미지 토큰 수와 함께 이미지 인코더 선택이 성능에 큰 영향을 미친다\"라며 시각적 구성 요소를 확장하고 개선하는 것이 성능 향상의 핵심이라고 밝혔다. 박찬 기자 cpark@aitimes.com"
}