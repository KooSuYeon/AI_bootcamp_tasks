{
    "title": "엔비디아, 차세대 AI 슈퍼칩 ‘GH200’ 공개...\"AI 시간·비용 대폭 절감\"",
    "created_at": "2023.08.09 15:16",
    "content": "GPU로 인공지능(AI) 칩 시장을 장악하고 있는 엔비디아가 고속 대용량 메모리를 탑재한 차세대 AI 칩을 공개했다. 기존 GPU를 뛰어넘는 성능으로 시간과 비용을 절약, 타 업체와의 격차를 더욱 벌리겠다는 의도다. 엔비디아는 8일(현지시각) 미국 LA에서 열린 ‘시그래프 2023’ 컨퍼런스에서 차세대 AI 칩 ‘그레이스 호퍼 200(GH200)’을 내년 2분기부터 양산한다고 밝혔다. GH200은 엔비디아 GPU와 ARM 기반의 CPU를 하나로 결합한 ‘슈퍼칩’이다. GPU와 CPU를 결합할 경우 정보를 주고받는 과정을 크게 단축해 전력 소비를 줄일 수 있다는 장점이 있다. 이번에 공개한 수퍼칩은 지난 5월 대만 컴퓨텍스에서 발표한 기존 GH200의 업데이트 버전이다. 현재 최고급 AI 칩인 'H100'과 동일한 GPU에 141기가바이트(GB)의 최첨단 메모리 및 72코어 ARM 기반의 CPU를 결합했다. 슈퍼칩에는 여러 개의 D램을 수직으로 연결해 쌓아 데이터 처리 속도를 크게 끌어올린 고대역폭 메모리 HBM3E를 탑재했다. HBM3E는 4세대 HBM 모델로, 현 HBM3 메모리보다 데이터 속도가 50% 빠르다. 141GB HBM3E 메모리로 속도는 5TB/s 대역폭을 지원한다. GH200은 H100 GPU에 비해 1.7배의 메모리 용량과 1.5배의 대역폭을 보장한다. 젠슨 황 엔비디아 CEO는 \"증가하는 AI 컴퓨팅 파워 수요를 충족시키기 위해 세계 데이터 센터의 규모를 확장하도록 설계했다\"며 \"대형언어모델(LLM)을 가져와서 여기에 넣으면 미친 듯이 추론할 것\"이라고 강조했다. 많은 양의 데이터를 사용하는 훈련 프로세스는 몇 달이 걸릴 수 있으며, 엔비디아 H100이나 A100가 수천개씩 필요하다. 훈련과 마찬가지로 추론도 계산 비용이 많이 들고 텍스트나 이미지를 생성하기 위해 실시간으로 대규모 정보 처리 능력이 필요하다. 엔비디아에 따르면 GH200은 더 많은 메모리 용량으로 실시간 정보를 처리하는 ‘추론용’으로 설계했다. 젠슨 황 CEO는 “더 큰 메모리를 사용했기 때문에 여러 GPU를 연결할 필요가 없어져 AI 모델에 드는 비용이 크게 줄어들 것”이라며 \"수많은 GPU를 연결해 만든 1억달러 규모의 데이터센터와 동일한 컴퓨팅 인프라를 구축하는데 800만달러면 충분할 것\"이라고 밝혔다. GH200으로 만들어진 데이터센터는 기존 설비보다 전력을 20배 적게 사용하기 때문이라고 설명헸다. 블룸버그는 이에 대해 \"AMD나 다른 업체들이 엔비디아를 더욱 따라잡기 어려워질 수 있다\"고 지적했다. AMD가 지난 6월 생성 AI용 GPU칩으로 출시한 ‘MI300X’는 모자이크ML의 벤치마크 테스트 결과 엔비디아 'A100' 성능의 80%까지 구현하는 것으로 알려졌다. 이와 함께 엔비디아는 생성 AI를 쉽게 개발할 수 있는 플랫폼 'AI 워크벤치(AI Workbench)’도 조만간 출시한다고 발표했다. 이 플랫폼에는 생성 AI 개발을 위한 프로젝트 예시들이 구축, AI 개발을 쉽게 시작할 수 있다. 또 엔비디아의 DGX 클라우드는 물론 PC와 워크스테이션, 데이터센터, 퍼블릭 클라우드를 넘나들며 프로젝트를 수행할 수 있다. 아울러 새로운 제품 및 서비스 개발과 함께 기존 제품과 서비스를 개선함으로써 이용자들의 경험을 향상할 수 있다고 덧붙였다. 또 AI 플랫폼 허깅페이스에 DCX 클라우드를 접목, 전 세계 수백만 명의 개발자들이 생성 AI 슈퍼컴퓨팅에 연결할 수 있도록 지원할 예정이라고도 밝혔다. AI 워크벤치는 올 가을 출시될 것으로 보이며, 공식 출시 전 사용해볼 수 있는 '얼리 액세스' 신청을 받고 있다. 박찬 기자 cpark@aitimes.com"
}