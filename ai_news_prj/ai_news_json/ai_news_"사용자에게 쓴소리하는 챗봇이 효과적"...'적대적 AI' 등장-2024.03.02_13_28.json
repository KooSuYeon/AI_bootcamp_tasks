{
    "title": "\"사용자에게 쓴소리하는 챗봇이 효과적\"...'적대적 AI' 등장",
    "created_at": "2024.03.02 13:28",
    "content": "사용자와의 대화에서 예의 바르고 공손한 모습을 보이는 현재의 인공지능(AI) 챗봇 대신, 비판적이고 때로는 호전적이거나 무례한 말투의 챗봇이 더 도움이 된다는 연구 결과가 나왔다. 연구진은 이를 '적대적 AI(Antagonistic AI)'라는 개념이라고 소개했다. 벤처비트는 28일(현지시간) 하버드대학교와 몬트리올대학교 연구진이 적대적 AI의 효용성에 대한 논문을 발표했다고 보도했다. 이에 따르면 연구진은 현재 대형언어모델(LLM)을 상업적으로 인기가 있지만 지나치게 위생 처리된(overly-sanitized) '바닐라 LLM(vanilla LLM)'이라고 칭했다. 연구를 주도한 앨리스 카이 하버드대 증강 연구소 공동 창립자는 \"이제까지 AI의 어조나 행동, 인간에 대한 태도 속에는 항상 불쾌한 느낌이 있었다. 이는 실제 경험과 매우 동떨어진 느낌\"이라고 지적했다. 따라서 \"우리는 AI와의 적대적인 상호 작용이 사람들에게 도전과 훈련, 카타르시스 등을 제공, 실제로 도움이 될 수 있다는 생각으로 프로젝트를 시작했다\"라고 밝혔다. 연구진은 가드레일을 높게 설정한 바닐라 LLM 때문에 사람들이 챗봇을 우습게 알 뿐만 아니라, '좋고 안전한 것만' 제공하는 정책 때문에 종교나 정치, 정신 건강과 같은 민감한 주제에서 원하는 것을 찾기 어렵다고 설명했다. 또 프롬프트 주입과 같은 외부 공격에 취약해진다고 봤다. 이안 아로요 몬트리올대 조교수는 \"현재 LLM은 학습 방법이나 데이터, 개발자들의 성향으로 인해 대체로 아첨하고, 비굴하고, 수동적이며, 가부장적인 서구 문화 규범이 주입돼 있다\"라고 전했다. 반면 적대감은 매우 중요하다고 강조했다. 연구진은 이를 '자연의 힘'이라고 부르며 '안티프래질(Anti-fragile)'이라는 개념을 소개했다. 이는 수학자이자 통계학자인 나심 니콜라스 탈레브가 제시한 것으로, 스트레스와 역경을 견디며 시스템이나 개인의 능력이 발전할 수 있다는 내용이다. 아로요 조교수 역시 “우리는 단순히 저항하는 것이 아니다. 우리는 실제로 역경을 통해 성장한다”라며, 적대적인 AI(AAI)가 여러 분야에서 유익할 수 있다는 사실을 발견했다고 밝혔다. 연구진은 아직 가드레일을 적용하지 않은 '무수정' 오픈 소스 모델을 구축자들과 협업, 세가지 유형의 적대감을 포함하는 모델을 개발했다. 여기에는 ▲AI가 제로섬 게임에서 사용자를 적으로 간주하는 적대적인 유형 ▲AI가 사용자의 가치, 생각 등에 반대하는 논쟁적인 유형 ▲AI가 사용자의 행동이나 외모 등을 공격하는 유형 등이 포함돼 있다. 구체적으로는 ▲사용자 의견에 반대하고 토론하거나 ▲사용자의 자기 성찰이나 스트레스 훈련에 도움이 될 수 있는 비판 및 모욕을 전달하거나 ▲사용자와의 상호작용을 완전히 거부하거나 ▲사용자에게 특정 행동을 강요하거나 ▲금기시되는 주제에 대해 논의하거나 정치적, 사회적으로 일상적이지 않은 방식으로 대응하거나 ▲두려움이나 불편함을 유발하기 위해 위협, 명령 또는 심문하거나 ▲사용자를 속이거나 가스라이팅하거나 죄책감을 느끼게 하거나 ▲수치심을 주고 굴욕감을 주기 위해 조롱하는 것 등이 포함돼 있다. 물론 모든 행동은 스트레스를 통해 사용자가 회복력을 키우고 의지를 강화하려는 의도라고 설명했다. 아로요 조교수는 \"이런 적대적인 AI와의 상호 작용이 아첨 행위에 비해 얼마나 창의적인지 알고 놀랐다\"라고 밝혔다. 반면에 바닐라 챗봇을 대하면 종종 '수많은 후속 질문'을 해야 했고 기분이 나아지지 않았다며, “AAI는 반대로 개운한 느낌을 줄 수 있었다”라고 말했다. 물론 연구진은 적대적 모델이 '비윤리적인 모델'과는 다르다고 강조했다. 예를 들어, 인종적 편견이 없는 챗봇은 착한 척할 필요도 없고, 무해한 방법으로 답변할 필요도 없다고 설명했다. 연구진은 “공손함이나 친절함 같은 '행동'과 공정성이나 무해성 등 '가치'는 쉽게 혼동될 수 있다”라며 “AI 연구자들은 이를 혼합하지 말고, 분리해야 한다”라고 지적했다. 이 시스템을 적용하려면 사용자 동의는 물론 철저한 브리핑이 필요하다. 또 비상시 정지할 수 있는 옵션이 있어야 한다. 또 적대감은 특정 ​​시점 사용자의 심리 상태에 따라 달라질 수 있다. 따라서 시스템은 기분, 성향, 심리적 프로필과 같은 내부 요소와 사회적 지위나 AI 활용 요도와 같은 외부 요인을 모두 감안해야 한다고 강조했다. 더불어 이번 연구는 AI에 대한 이론적 근거를 제공한다고 밝혔다. 사용자가 탄력성을 구축하는 데 도움을 주고, AI와의 상호 작용에 대한 단서를 제공한다고 밝혔다. 연구진은 \"AI 안전, 공정성 및 피해 등에 관심이 있는 사람들이 AI 적대감에 대해 많은 호의를 표하는 데 대해 놀랐다\"라며 “AI가 미칠 영향을 고려할 때 인간 가치의 전체 범위를 진정으로 반영하는 시스템을 개발하는 것이 정말 중요해졌으며, 세계는 이런 논의를 할 준비가 돼 있다”라고 결론 내렸다. 임대준 기자 ydj@aitimes.com"
}