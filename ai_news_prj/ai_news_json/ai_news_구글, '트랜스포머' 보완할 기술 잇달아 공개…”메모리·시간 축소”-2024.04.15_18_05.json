{
    "title": "구글, '트랜스포머' 보완할 기술 잇달아 공개…”메모리·시간 축소”",
    "created_at": "2024.04.15 18:05",
    "content": "구글이 입력 데이터가 커질수록 추론이 느려지고 메모리 공간이 많이 필요한 '트랜스포머' 아키텍처의 약점을 보완하기 위한 새로운 기술을 잇달아 공개했다. 어텐션 메커니즘 발표 7년 만에 이를 뛰어넘을 시도가 본격화되고 있다. 벤처비트는 12일(현지시간) 구글이 대형언어모델(LLM)의 컨텍스트 창의 길이를 무한 확장할 수 있는 ‘인피니-어텐션(Infini-attention)’ 기술에 관한 논문을온라인 아카이브에 게재했다고 전했다. '챗GPT'나 '제미나이' 등 LLM에 사용되는 트랜스포머 아키텍처는 컨텍스트 창이 커짐에 따라 필요한 메모리와 계산 시간이 기하급수적으로 증가하는 단점이 있다. 예를 들어, 입력 크기를 토큰 1000개에서 2000개로 확장하면 입력을 처리하는 데 필요한 메모리와 계산 시간이 두배가 아닌 네배로 늘어나게 된다. 이는 텍스트 내 토큰들의 상관관계를 밝혀내기 위해 입력 정보를 병렬로 처리하는 '어텐션 메커니즘' 때문이다. 이 문제를 해결하기 위해 구글은 메모리 및 컴퓨팅 요구 사항을 일정하게 유지하면서 LLM이 무한 길이의 텍스트를 처리할 수 있도록 인피니-어텐션 기술을 도입했다. 인피니-어텐션은 일반적인 어텐션 메커니즘에 ‘압축 메모리’를 통합, 입력이 컨텍스트 길이를 초과하면 모델은 계산 효율성을 위해 압축 메모리에 이전 어텐션 상태를 저장한다. 전체 컨텍스트 기록을 유지하기 위해 다음 컨텍스 길이를 처리할 때 이전 컨텍스트의 어텐션 상태를 버리지 않고 압축 메모리에 저장한다는 설명이다. 구글에 따르면 인피니-어텐션을 적용한 LLM은 메모리 추가없이도 100만개 이상의 토큰 품질을 유지할 수 있다. 연구진은 \"트랜스포머 아키텍처의 어텐션 메커니즘에 대한 미묘하지만 중요한 수정을 통해 기존 LLM을 무한히 긴 컨텍스트로 자연스럽게 확장할 수 있다\"라고 설명했다. 또 인피니 어텐션이 매우 긴 컨텍스트에 대한 모델의 일관성을 측정하는 퍼플렉시티(Perplexity) 벤치마크에서 114배 더 적은 메모리를 사용하고도 다른 긴 컨텍스트 트랜스포머 기반 LLM을 능가하는 성능을 기록했다고 주장했다. 비밀번호 대신 사용하는 패스키 검색 테스트에서 인피니-어텐션은 최대 100만개의 토큰으로 구성된 긴 텍스트에 삽입된 난수를 정확하게 반환할 수 있었으며, 최대 50만개 토큰의 텍스트를 요약하는 테스트에서 다른 긴 컨텍스트 모델보다 성능이 뛰어났다. 무한한 컨텍스트 길이를 지원하는 LLM을 사용하면 이론적으로 모든 문서를 프롬프트에 삽입하고 모델이 각 쿼리에 대해 가장 관련성이 높은 답변을 선택하도록 할 수 있다. 또 특정 작업에 대한 성능을 향상하기 위해 모델을 세부적으로 조정할 필요 없이 긴 예제를 제공해 모델을 사용자 정의할 수도 있다. 더불어 구글은 엣지 장치용 오픈 소스 소형언어모델(sLM) ‘리커런트젬마(RecurrentGemma)’에 관한 논문을온라인 아카이브에 게재했다. 이 또한 트랜스포머 아키텍처의 어텐션 메커니즘을 보완한 LLM과 동등한 수준의 성능을 유지하면서 메모리 및 처리 요구 사항을 대폭 축소한다는 내용이다. 상기한 대로 트랜스포머는 입력 데이터를 모두 병렬로 처리하는 어텐션 메커니즘 때문에 데이터 볼륨이 증가함에 따라 메모리와 처리량이 크게 증가하는 약점이 있다. 이 때문에 LLM은 스마트폰이나 사물인터넷(IoT), 개인용 컴퓨터와 같이 리소스가 제한된 장치에 배포하기 어렵다. 또 데이터센터 내의 원격 서버에서 실행되기 때문에 실시간 응답을 요구하는 AI 애플리케이션에 적합하지 않다. 반면 이번에 공개한 리커런트젬마는 트랜스포머 기반 모델처럼 모든 정보를 병렬로 처리하는 대신, 주어진 시간에 입력 데이터에 집중하여 처리하는 '로컬 어텐션 메커니즘'을 도입했다. 이로 인해 성능을 크게 저하시키지 않으면서 계산 부하를 줄이고 처리 속도를 높인다. 리소스가 제한된 엣지 장치에 배포하는 데 적합하고 원격 서버에서 실행할 필요가 없어 실시간 엣지 어플리케이션에 적합하다는 설명이다. 리커런트젬마는 새로운 입력 데이터가 처리될 때 업데이트되는 '히든 스테이트(hidden state)'를 유지함으로써 이전 정보를 순차적으로 기억하는 순환 신경망(RNN)의 기본 구성 요소인 선형 반복(linear recurrence)을 어텐션과 결합, 입력 데이터에 관계없이 일정한 수준의 리소스 사용량을 유지함으로써 메모리 및 계산 요구 사항을 확인하면서 확장된 텍스트를  처리할 수 있다. 특히 리커런트젬마는 처리 범위를 줄임으로써 대용량 데이터를 지속적으로 재처리하기 위한 GPU 필요성을 최소화한다. 하드웨어 요구 사항의 낮아짐에 따라 리커런트젬마와 같은 모델은 일반적으로 초대형 클라우드를 위해 설계된 서버보다 컴퓨팅 전력이 적은 엣지 컴퓨팅 응용 프로그램에 더 적합하다. 이는 클라우드 연결에 의존하지 않고 스마트폰, IoT 장치 또는 임베디드 시스템과 같은 엣지 장치에 직접 언어모델을 실행할 수 있게 한다. 박찬 기자 cpark@aitimes.com"
}