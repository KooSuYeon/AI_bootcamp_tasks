{
    "title": "최초의 10B '분산형 모델 훈련' 등장...\"오픈 소스 AGI 개발의 시작\"",
    "created_at": "2024.10.14 18:05",
    "content": "중앙 집중화된 단일 컴퓨팅 클러스터 대신, 전 세계적으로 분산된 컴퓨팅 하드웨어에서 훈련한 100억 매개변수 모델이 등장했다. 분산형 훈련모델로 10B 규모의 대형언어모델(LLM)이 등장한 것은 처음이라는 설명이다. 마크테크포스트는 11일(현지시간) 인공지능(AI) 스타트업 프라임 인텔렉트(Prime Intellect)가 누구나 컴퓨팅 자원을 제공하고 참여할 수 있는 최초의 100억개 매개변수의 분산형 모델 학습법 ‘인텔렉트-1(INTELLECT-1)’을 공개했다고 보도했다. 일반적으로 오픈 소스 AI 구축의 문제 중 하나는 AI 모델 개발을 위해서는 막대한 컴퓨팅 인프라가 필요하다는 점이다. 이는 방대한 리소스를 보유한 대형 AI 기업들이나 가능하다. 이를 '중앙화(집중화)'라고 표현했다. 중앙화는 AI 개발 과정에서 소규모 조직이나 개인들의 참여를 어렵게 만든다. 프라임 인텔렉트는 '분산화(탈중앙화)', 즉 누구나 컴퓨팅 자원을 제공하고 AI 모델 학습에 참여할 수 있도록 하는 인텔렉트-1을 출시했다. 그 첫 사례로 100억개 매개변수 모델의 첫 분산형 학습을 시작했으며, 이는 사상 첫 10B 모델에 적용한 경우라고 강조했다. 잭 클라크 앤트로픽 공동 설립자도 최근 X(트위터)를 통해 \"100억 매개변수 이상의 분산 학습은 보지 못한 것 같다. 분산 학습에 대해 읽은 모든 논문에서는 10억 매개변수 이하의 LLM만 등장했다\"라고 밝힌 바 있다. 그만큼 큰 모델에 이 방식을 사용하는 것이 쉬운 일은 아니라는 설명이다. 인텔렉트-1 역시 복잡한 질문에 대한 인간과 유사한 응답을 다양한 맥락에서 이해하고 생성할 수 있는 일반적인 LLM을 훈련한다. 특히 분산형 학습 방식을 채택, 개별 기여자들의 컴퓨팅 자원을 모아 이러한 대규모 학습에 필요한 컴퓨팅 파워를 제공한다. 이 방식은 비싼 중앙 집중식 슈퍼컴퓨터에 대한 의존도를 줄이고, 개별 기여자들의 자원을 효율적으로 활용할 수 있게 한다. 이를 위해 혁신적인 협력 기법을 사용, 작업 부하를 효율적으로 분산하고 병렬 계산을 통해 학습 시간을 단축한다. 프라임 인텔렉트는 전 세계에 분산된 하드웨어에서 협업 모델 개발을 가능하게 하는 분산형 훈련 프레임워크 ‘오픈디로코(OpenDiLoCo)’를 구축했다. 오픈디로코는 딥마인드의 분산형 저통신 기술인 ‘디로코(DiLoCo)’를 바탕으로 확장한 오픈 소스 프로젝트다. 디로코 접근 방식은 물리적으로 분산된 각 클러스터에 할당된 독립적인 데이터 파티션으로 동일한 공유 모델 사본을 훈련한다. 각 클러스터는 확률적 경사 하강을 사용해 500 단계를 주기로 모델을 반복적으로 업데이트한다. 이런 방식은 통신 빈도를 크게 줄여 분산형 훈련에 필요한 대역폭을 낮춘다. 그다음 각 클러스터의 가중치 평균을 종합적으로 계산하고 외부 옵티마이저를 사용해 공유 모델  사본을 업데이트한다. 이후 업데이트된 공유 모델 사본을 각 클러스터에 다시 배포한다. 이 과정을 반복함으로써 다양한 하드웨어 가속기를 활용해 다양한 컴퓨팅 환경 내에서 복제된 각 모델을 쉽게 훈련할 수 있다. 이는 다양한 측면에서 중요한 의미를 가진다는 설명이다. 우선 AI 연구가 몇몇 자금이 풍부한 조직에만 국한된 독점적인 활동에서 벗어나 학습 과정을 탈중앙화함으로써, 개방적 협력의 비전을 제시하고 있다. 결과적으로 전 세계의 다양한 시각과 데이터가 포함된 학습 과정을 통해 '오픈 소스 인공일반지능(AGI)'의 기초가 될 수 있다는 설명이다. 프라임 인텔렉트는 \"인텔렉트-1은 첫 단계일 뿐\"이라며 \"향후 과학, 추론, 코딩 분야에서 더 크고 강력한 오픈 프런티어 모델로 확장할 예정\"이라고 밝혔다. 박찬 기자 cpark@aitimes.com"
}