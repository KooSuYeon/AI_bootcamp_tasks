{
    "title": "인텔, AI PC용 프로세서 ‘루나 레이크’ 출시…6세대 ‘제온’ 서버 칩도 공개",
    "created_at": "2024.06.05 18:00",
    "content": "인텔이 데이터센터, 클라우드와 네트워크에서 엣지 및 PC에 이르기까지 인공지능(AI) 생태계를 가속화할 최첨단 기술 및 아키텍처 패키지를 공개했다. 벤처비트는 4일(현지시간) 인텔이 대만에서 열린 컴퓨텍스에서 ▲AI PC용 ‘루나 레이크(Lunar Lake)’ 프로세서 ▲인텔 ‘제온 6(Xeon 6) E-코어 6700E’ 프로세서 ▲인텔 가우디 2 및 가우디 3 AI 가속기 가격 정책 등을 공개했다고 보도했다. 차세대 AI 노트북을 위한 차세대 코어 울트라 프로세서 ‘루나 레이크’는 올해 3분기에 출시될 예정이다. 기존 코어 울트라 대비 최대 3배 이상 향상된 AI 성능을 제공하며, 마이크로소프트(MS) ‘코파일럿+ PC’ 기준도 충족한다. 루나 레이크는 CPU, GPU, NPU까지 주요 구성 요소들의 아키텍처가 모두 업그레이드됐다. CPU의 경우 P코어에 ‘라이언 코브(Lion Cove)’, E코어에는 ‘스카이몬트(Skymont)’ 아키텍처가 적용됐다. 이로 인해 같은 동작 속도에서 P코어는 이전 세대 대비 약 14% 성능이 높아졌고, E코어는 이전 메테오레이크의 LP E코어 대비 정수에서 38%, 부동소수점 연산에서는 68%까지 성능이 높아진 것으로 알려졌다. 루나 레이크의 NPU는 ‘4세대’ 아키텍처를 기반으로 이전 세대 대비 최대 4배 성능이 높아진 48TOPS 성능을 제공한다. 또 MS의 ‘코파일럿+ PC’ 기준인 40TOPS 성능 기준도 충족한다. GPU는 차세대 Xe2 ‘배틀메이지(Battlemage)’ 기반이 처음 적용됐으며, 행렬연산 성능을 높이는 XMX를 지원해 그래픽 성능은 이전 세대 대비 1.5배, AI 성능은 3.5배까지 높아졌다. 인텔은 이런 특징을 통해, 시스템 수준에서는 최대 120TOPS 수준의 AI 성능을 제공할 수 있을 것으로 소개했다. 루나 레이크는 컴퓨트 코어 전체를 역대 처음으로 TSMC의 3나노(㎚) 공정에 외주를 맡겨 제조했다. 팻 겔싱어 CEO는 “AI PC 시대는 새로운 큰 변화의 계기\"라며 \"인텔은 이미 코어 울트라 프로세서 기반 제품을 800만대 이상 공급했으며, 올해 4000만대 이상의 제품 공급을 목표로 한다”라고 말했다. 데이터센터용 제품으로는 차세대 ‘제온 6’ 제품군의 첫 제품인 ‘제온 6 E 코어 6700E 시리즈’ 제품을 발표했다. 이 제품은 최대 144코어 구성을 지원하는 것이 특징으로, 4나노급 ‘인텔 3’ 공정을 사용하는 첫 제품이기도 하며, 많은 코어 수를 바탕으로 고밀도 스케일아웃 워크로드에 최적화됐다. 인텔은 ‘제온 6 E-코어’ 기반 시스템이 전작 대비 최대 2.6배의 와트 당 성능 향상, 4.2배의 랙 레벨 성능 향상을 제공하고, 200개 랙을 66개 랙으로 3대 1 비율로 통합할 수 있다곻 밝혔다. 전력소비량 절감 효과는 4년 간 8만메가와트시(MWh) 절감이 가능하다고 주장했다. 3분기 중 코드명 ‘그래나이트 래피즈(Granite Rapids)’로 알려진 ‘제온 6 P-코어 6900P’ 제품군도 발표할 예정이다. 인텔은 이날 엔비디아의 H100 GPU과 경쟁하도록 설계된 가속기인 ‘가우디(Gaudi)’의 차별화된 가격 정책도도 공개했다. 지난 4월 새롭게 발표한 가우디 3 가속기 8개가 포함된 플랫폼의 가격은 12만5000달러(1억7000만원), 전작 가우디 2 가속기 8개가 포함된 플랫폼의 가격은 6만5000달러(약 9000만원)다. 가우디 3 키트는 엔비디아 동급 플랫폼 가격의 3분의 2이고, 가우디 2는 엔비디아 동급 플랫폼 가격의 3분의 1 수준이다. AI 반도체 1위 엔비디아를 추격하기 위한 전략으로 가격을 대폭 낮췄다. 겔싱어 CEO는 가우디 3에 대해 “시장에서 엔비디아의 GPU에 경쟁 가능한 유일한 제품”이라고 강조했다. 8192개 가속기 클러스터를 갖춘 인텔 가우디 3 가속 장치는 동급 규모의 엔비디아 H100 GPU 클러스터에 비해 학습 시간이 최대 40% 빠르며, 64개 가속기 클러스터는 엔비디아 H100의 라마2-70B 모델에 비해 최대 15% 빠른 학습 처리량을 제공한다고 주장했다. 또 주요 대형언어모델(LLM)의 추론에서는 엔비디아 H100 대비 평균 최대 2배 높은 성능을 제공한다고 밝혔다. 박찬 기자 cpark@aitimes.com"
}