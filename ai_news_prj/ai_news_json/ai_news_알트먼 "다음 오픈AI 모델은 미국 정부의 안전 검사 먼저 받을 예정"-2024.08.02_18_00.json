{
    "title": "알트먼 \"다음 오픈AI 모델은 미국 정부의 안전 검사 먼저 받을 예정\"",
    "created_at": "2024.08.02 18:00",
    "content": "샘 알트먼 오픈AI CEO가 차기 주요 인공지능(AI) 모델 출시에 앞서 미국 정부의 안전 검사를 받겠다고 밝혔다. 이는 최근 제기되는 안전 문제에 대한 비반을 잠재우려는 의도인데, 'GPT-5' 출시를 앞둔 시점이라 주목된다. 알트먼 CEO는 1일(현지시간) X(트위터)를 통해 '오픈AI의 안전에 대한 몇가지 간단한 업데이트'라는 글을 올렸다. 이를 통해 미국 정부 산하 기관인 AI 안전 연구소(US AI Safety Institute)와 협력, 다음 파운데이션 모델에 대한 사전 접근을 제공하고 AI 평가 과학을 발전시키기 위한 협력 계약을 체결했다고 밝혔다. 미국 정부의 사전 검사를 받겠다는 의미로, 시기나 방법, 대상 등 자세한 사항은 밝히지 않았다. AI 안전 연구소는 국가표준기술원(NIST) 산하 기관으로, 지난해 11월 영국에서 열린 'AI 안전 서밋'을 통해 국가 안보, 공공 안전, 개인 권리와 관련된 위험을 해결하는 목표로 구성됐다. 오픈AI는 물론, 마이크로소프트, 메타, 애플, 아마존, 구글 등 100개 이상의 기술 회사로 구성된 컨소시엄과 협력 중이다. 이번 발표는 지난해 7월 선언한 백악관 주도의 'AI 안전 서약'과도 일치하는 움직임이다. 당시 발표한 7개항 중에는 '정부와 기업에 안전 정보를 공유한다'는 항목이 포함됐다. 또 오픈AI는 지난 5월 ‘안전 및 보안 위원회’를 출범, 3개월 간 활동을 시작한다고 발표한 바 있다. 즉 정부와 자체적인 모니터링을 두차례에 걸쳐 진행, 출시될 모델의 안전성 논란을 잠재우겠다는 의도로 보인다. 특히 5월 발표에서는 3개월간의 위원회 활동이 끝난 뒤 이사회 승인을 거쳐 모델을 출시한다고 밝혀, 8월 말쯤 GPT-5를 출시하는 것이 아니냐는 추측이 나왔다. 이번 정부 안전 검사도 이 시기와 맞물린다. 한편 알트먼 CEO는 이날 게시물을 통해 두가지 사항을 더 공개했다. 우선 회사 전체의 컴퓨팅 리소스 중 최소 20%를 AI 안전에 할당한다고 밝혔으며, 또 현재 및 전직 직원에 대한 비방 금지 조건과 이를 어길 시 지분을 취소할 수 있는 조항을 지난 5월 무효화했다고 설명했다. 이 두가지는 AI 안전팀 해체와 관련 주요 인물의 퇴사로 시끄럽던 5월에 이미 밝혔던 내용을 재확인한 것이다. 한편, 오픈AI는 최근 잇달아 안전 문제에 대한 지적을 받고 있다. 특히 최근에는 미국 상원의원 5명으로부터 안전 정책에 대한 답변을 요구하는 서한을 받았다. 여기에는 컴퓨팅 리소스 문제와 비밀 유지 계약에 대한 내용 공유 요청도 포함돼 있다. 임대준 기자 ydj@aitimes.com"
}