{
    "title": "美 소장 학자들 어린이용 언어모델 '베이비LM' 개발 도전",
    "created_at": "0",
    "content": "아론 뮤엘러 존스홉킨스대학 교수를 비롯한 자연어 처리 분야 학자들이 13세 이하 어린이들에게 익숙한 단어로 사전 훈련한 어린이용 언어모델을 개발하기 위한 실험을 진행하고 있는 것으로 알려졌다. 30일(현지시간) 뉴욕타임스에 따르면 이들 소장 학자들은 지난 1월 깃허브에 13세 청소년이 접할 수 있는 단어 약 1억개를 모아 데이터셋으로 제시하고, 이를 기반으로 언어모델을 반드는 도전과제인 '베이비LM 챌린지'를 제안했다. 8월 1일까지 후보 모델을 제출하고 평가를 거쳐 우승자를 가리는 일정이다. 이 실험은 언어모델 훈련에 사용하는 데이터 양을 대폭 줄이되 효율성을 높이면 대형언어모델(LLM) 못지 않은 성능을 확보할 수 있을 것이라는 가정에서 출발했다. 기존 LLM의 훈련데이터셋은 많게는 1조개에 이르는 단어를 사용했다. 오픈AI의 ‘GPT-3’가 2000억개, 딥마인드의 ‘친칠라’는 1조개에 달했다. 인공지능(AI)은 훈련에 사용한 데이터셋의 크기 만큼 유능한 것이 사실이다. 하지만 언어모델이 갈수록 대형화되면서 특정 거대 기업 의존도가 높아지고 있는 실정이다. 뮤엘러 교수 등은 이 크기를 줄여 접근하기 쉬우면서도 기능적으로는 대형 모델에 못지 않는 모델을 개발하자는 취지로 도전 과제를 제안했다. 이와관련 에바 포틀란스 맥길대 교수는 \"‘베이비LM 챌린지’가 언어모델 대형화 경쟁에서 벗어나 더 접근하기 쉽고 직관적인 AI 모델을 향한 발걸음으로 볼 수 있다\"고 말했다. 최근 AI 업계에서도 언어모델의 크기, 즉 매개변수 수가 반드시 성능으로 이어지지 않는다는 인식이 확대되고 있다. 실제로 샘 알트만 오픈AI CEO가 최근 이런 발언을 했고, 구글의 새 언어모델인 ‘팜2’는 매개 변수를 ‘팜1’의 5400억개 보다 적은 3400억개로 줄이기도 했다. 스탠포드대 연구진이 개발한 ‘알파카’나 데이터브릭스가 내놓은 ‘돌리’ 등 소형 언어모델도 속속 등장하고 있다. 이런 가운데 시작된 데이터의 양은 줄이면서 효율성을 높이려는 ’베이비LM’ 실험이 과연 어느정도의 효과를 거둘 수 있을지 관심이 모아진다. 정병일 기자 jbi@aitimes.com"
}