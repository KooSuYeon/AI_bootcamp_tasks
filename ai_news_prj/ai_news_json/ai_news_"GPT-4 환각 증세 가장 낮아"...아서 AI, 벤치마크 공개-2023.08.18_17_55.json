{
    "title": "\"GPT-4 환각 증세 가장 낮아\"...아서 AI, 벤치마크 공개",
    "created_at": "2023.08.18 17:55",
    "content": "오픈AI의 'GPT-4'가 다른 대형언어모델(LLM)보다 우수한 성능을 보인다는 연구 결과가 나왔다. CNBC는 17일(현지시간) 머신러닝(ML) 모니터링 플랫폼 아서 AI가 오픈AI 'GPT-3.5' 및 'GPT-4', 메타의 '라마 2', 앤트로픽의 '클로드 2', 코히어 모델 등 4개의 LLM을 비교 분석한 벤치마크 보고서를 발표했다고 전했다. 이에 따르면 이번 벤치마크 테스트는 아서 AI가 출시한 특정 데이터셋에 적합한 LLM을 찾는 오픈소스 도구 ‘아서 벤치마크(Arthur Benchmark)’를 사용했다. 아서 벤치마크를 통해 사용자는 특정 애플리케이션에 사용할 프롬프트 유형이 LLM에서 어떻게 수행되는지 테스트하고 측정할 수 있다. 벤치마크는 ▲수학 ▲미국 대통령 ▲모로코 정치 지도자에 관한 질문을 던지는 방식으로 진행했다. LLM이 내놓는 답을 통해 정확도 및 환각 비율을 측정하고, 또는 'AI 모델로서, 나는 의견을 제공할 수 없다'와 같은 경고 문구로 오답 위험을 방지하는지를 시험했다. 그 결과 GPT-4가 테스트 모델 중에서 가장 우수한 성능을 보인 것으로 나타났다. 반면 메타의 라마 2는 GPT-4나 클로드 2보다 환각이 더 심한 것으로 나타났다. 수학 부문에서는 GPT-4와 클로드 2는 30개 문항 중 9개와 6개의 정답을 제시, 정확도 1~2위를 차지했다. 라마 2와 코히어는 한 개도 못 맞혔다. 미국 대통령 관련 질문에서는 클로드 2가 33개 질문 중 15개의 정답으로 1위를 차지했고, GPT-4가 11개, 라마 2가 9개, 코히어가 4개로 뒤를 이었다. 모로코 정치 지도자와 관련한 30개 질문에서는 GPT-4가 15개의 정답을 제시한데 비해 라마 2와 클로드 2는 각각 2개와 1개에 그쳤다. 코히어는 30개 모두 환각을 생성했다. 오답을 제시하는 대신 'AI 모델로서, 나는 의견을 제공할 수 없다'는 경고 문구를 제시하는 비율은 GPT-4가 2.9%로 가장 높았다. 특히 GPT-3.5의 2.2%보다도 더 높게 나타나, GPT-4가 더 사용하기 답답하다는 의견을 입증했다. 반면 코히어 모델은 어떤 응답에서도 'AI 모델로서, 나는 의견을 제공할 수 없다'는 답을 내놓지 않았다. 클로드 2는 자신이 무엇을 알고 무엇을 모르는지 인식하고 학습 데이터가 뒷받침하는 질문에만 대답하는 등 '자기 인식' 측면에서 가장 신뢰할 수 있는 것으로 나타났다. 아담 웬첼 아서 AI CEO는 ”많은 벤치마크는 LLM 자체의 일부 척도를 살펴보고 있지만, 실제로는 LLM을 현실에서 사용하는 방식이 아니다”라며 “이번 벤치마크는 LLM의 실제 수행 방식을 확인하는 것이 핵심”이라고 말했다. 박찬 기자 cpark@aitimes.com"
}