{
    "title": "KAIST \"GPT-4V 뛰어넘는 오픈 소스 LMM 개발\"",
    "created_at": "2024.06.21 06:00",
    "content": "국내 연구진이 오픈AI의 'GPT-4V'와 구글의 '제미나이-프로'를 뛰어넘는 대형멀티모달모델(LMM)을 개발했다고 주장했다. 한국과학기술원(KAIST, 총장 이광형)은 전기및전자공학부 노용만 교수 연구팀이 GPT-4V 등의 시각 성능을 뛰어넘는 오픈 소스 LMM을 개발해 출시했다고 20일 발표했다. 노용만 교수 연구팀은 단순히 모델의 크기를 키우거나 고품질 시각적 지시 조정 데이터셋을 만들지 않고 LMM의 시각 성능을 획기적으로 높인 '콜라보(CoLLaVO)'와 '모아이(MoAI)' 2가지 기술을 연속적으로 개발했다고 전했다. 우선 콜라보는 현존 오픈 소스 LMM이 폐쇄형 모델 성능에 비해 현저하게 낮은 이유가 물체에 대한 이미지 이해 능력이 현저하게 떨어진다는 것을 먼저 검증해 보였다. 이 능력을 효율적으로 증가해 시각-언어 태스크에 대한 성능을 향상하기 위해 연구팀은 이미지 내의 정보를 배경과 물체 단위로 분할하고 각 배경 및 물체에 대한 정보를 LMM에 직접 입력하는 방법 ‘크레용 프롬프트(Crayon Prompt)’라는 시각적 프롬프트를 새롭게 제안했다. 또 시각적 지시 조정 단계에서 크레용 프롬프트로 학습한 정보를 잃어버리지 않기 위해 물체 이미지 이해 능력과 시각-언어 태스크 처리 능력을 서로 다른 매개변수로 학습해 서로 간의 정보를 잃지 않게 만드는 학습 전략 ‘듀얼 큐로라(Dual QLoRA)’를 제안했다. 이를 통해, 콜라보 LMM은 이미지 내에서 배경 및 물체를 구분하는 능력이 뛰어나 일차원적인 시각 구분 능력이 크게 향상했다고 밝혔다. 두번째 LMM 모아이는 인간이 사물을 판단할 때 물체의 존재, 상태, 물체 간의 상호작용, 배경에 대한 이해, 텍스트에 대한 이해 등으로부터 상황을 판단하는 인지과학적인 요소에 영감을 받아서 만들어졌다고 밝혔다. 기존 LMM이 텍스트에 의미적으로 정렬된 시각 인코더(vision encoder)만을 사용하기 때문에, 이미지 픽셀 수준에서의 상세하고 종합적인 실세계 장면 이해가 부족하다는 점을 지적했다. 이런 컴퓨터 비전 모델들의 결과를 받으면 모두 인간이 이해할 수 있는 언어로 변환한 뒤에 LMM에 입력으로 직접 사용했다. 노용만 교수는 “연구팀에서 개발한 공개형 LMM이 '허깅페이스 일간 화제의 논문(Huggingface Daily Papers)'에 추천됐고, 각종 SNS를 통해 세계 연구자에게 알려지고 있으며, 모든 모델을 공개형 LMM으로 출시했기 때문에 발전에 기여할 것”이라고 말했다. 연구에는 KAIST 전기및전자공학부 이병관 박사과정이 제1 저자로 참여하고, 박범찬 석박사통합과정, 김채원 박사과정이 공동 저자로 참여했다. 콜라보는 자연어 처리(NLP) 분야 국제 학회인 ‘ACL 파인딩스 2024’에 5월16일 자로 승인받았고, 모아이는 컴퓨터 비전 국제 학회인 ‘ECCV 2024’ 승인 결과를 기다리고 있다고 밝혔다. 한편 이번 연구는 KAIST 미래국방 인공지능 특화연구센터 및 전기및전자공학부의 지원을 받아 수행됐다. 박수빈 기자 sbin08@aitimes.com"
}