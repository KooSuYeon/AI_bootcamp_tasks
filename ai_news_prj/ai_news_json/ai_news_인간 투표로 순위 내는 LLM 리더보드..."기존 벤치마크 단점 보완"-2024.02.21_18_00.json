{
    "title": "인간 투표로 순위 내는 LLM 리더보드...\"기존 벤치마크 단점 보완\"",
    "created_at": "2024.02.21 18:00",
    "content": "대형언어모델(LLM)의 성능을 측정하기 위한 전통적인 벤치마크와는 달리, 인간이 직접 성능을 파악하고 순위를 매기는 신개념의 LLM 리더보드가 등장했다. NBC는 20일(현지시간) '챗봇 아레나(Chatbot Arena)'라는 LLM 리더보드가 최근 빠르게 인기를 얻고 있다고 소개했다. 이에 따르면 챗봇 아레나는 대형모델시스템조직(Large Model Systems Organization)'이라는 개방형 대학 연구 그룹이 만든 것으로, 인간의 AI 모델을 직접 평가하고 투표로 모델 선호도를 결정하는 방식이다. 방식은 간단하다. 투표 페이지에 접속하면 이름을 밝히지 않은 모델 A, B가 있는데, 프롬프트로 질문을 던지면 두 모델이 동시에 답을 내놓는다. 그 결과를 비교해 우수하다고 생각하는 쪽에 투표하면 된다. 즉 '블라인드 테스트' 방식의 '크라우드 소싱' 투표다. 지난해부터 이런 식으로 진행한 투표는 30만건에 달하며, 현재 그 결과 1위를 기록 중인 모델은 오픈AI의 'GPT-4 터보'다. 이어 'GPT-4'의 각 버전이 상위권을 점령했다. 5위 안에 포함된 다른 모델은 구글의 '제미나이 프로(3위)'가 유일하다. 미스트랄이나 클로드, 큐원, Yi-34B 등 유명 오픈 소스 모델도 20위 안에 포함됐다. 기존 오픈 소스 모델만을 대상으로 하던 허깅페이스 리더보드에 비해 유명도가 있는 모델이 많이 포진했다. 12일 현재 64개의 모델이 올라와 있으며, 업스테이지의 '솔라'는 32위에 위치해 있다. 이 순위는 허깅페이스 리더보드의 단점을 보완하려는 의도로 만들어 졌다. 허깅페이스를 비롯한 상당수 벤치마크는 수년 동안 공개된 데이터셋을 활용, LLM 개발자가 해당 테스트셋에서 모델을 훈련해 출시 시 높은 점수를 받는 것이 쉬워졌기 때문이다. 또 상식이나 독해력 및 수학 해결 등 전형적인 방법을 평가하는 7가지 항목을 넘어 챗봇을 실제로 사용하는 인간의 요구를 잘 반영하는지 전체적으로 측정하는 방법은 없었다. 챗봇 아레나 제작자에 따르면 순위가 시작된 지 1년도 채 되지 않아 사이트 트래픽은 많이 증가했다. 하루에도 수천번씩 투표가 이뤄지고 있으며, 플랫폼에서 수용할 수 없을 만큼 많은 새로운 모델을 추가해 달라는 요청이 이어지고 있다. 물론 문제도 있다. 사이트 공동 제작자인 잉 싱 스탠포드대학교 컴퓨터대학 박사과정은 \"사람들은 최신 기술이 등장할 떄마다 순위가 바뀌는 것을 보고 싶어 하지만, 아레나 순위는 사실 큰 변화가 없다\"라며 \"아직 순위 선정 방식에 개선해야 할 부분이 많다”라고 밝혔다. 또 웨이린 시앙 UC 버클리 컴퓨터과학 박사과정은 \"프롬프트 작성 중 악의적인 행동을 탐지하기 위한 알고리즘을 만들고 있다\"라고 말했다. 전문가들은 기존 방식이든 인간 방식이든 벤치마크가 완전한 성능 측정 방법으로 볼 수는 없다고 지적한다. 하지만 테스트와 순위가 최신 성능을 따라잡기 위해 지속해서 기준을 높이는 것은 AI의 혁신에 도움이 된다고 말한다. 임대준 기자 ydj@aitimes.com"
}