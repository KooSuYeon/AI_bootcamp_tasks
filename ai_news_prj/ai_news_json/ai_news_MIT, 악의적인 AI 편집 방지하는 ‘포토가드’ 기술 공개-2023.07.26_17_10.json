{
    "title": "MIT, 악의적인 AI 편집 방지하는 ‘포토가드’ 기술 공개",
    "created_at": "2023.07.26 17:10",
    "content": "최근 생성 인공지능(AI) 기술의 향상으로 이미지 조작의 위험이 커지고 있는 가운데 미국 매사추세츠 공대(MIT) 연구팀이 AI로 이미지를 무단으로 편집하는 것을 방지하는 '포토가드'(PhotoGuard)’라는 기술을 공개했다. 24일(현지시간) 엔가젯에 따르면 MIT 인공지능연구소(CSAIL) 연구팀이 눈에 띄지 않는 조작을 이미지에 가해 AI의 가공을 방지하는 시스템 ‘포토가드’를 발표했다. 포토가드는 AI가 이미지를 이해하는 것을 방해하는 방식으로 AI에 의한 이미지 편집을 방지하는 시스템이다. 이미지의 중심이 되는 픽셀을 선택해 왜곡시킨다. 이러한 픽셀 왜곡은 인간의 눈에는 감지되지 않지만 AI는 감지할 수 있다. 예를 들어, 아래 그림에서 원본 이미지(Original Image)를 AI에 입력하고 ‘두 남자가 사교춤을 춘다(Two men ballroom dancing)’라는 프롬프트를 입력하면 오른쪽 상단 이미지와 같은 정상적인 이미지로 변경된다. 반면 포토가드를 적용한 왼쪽 하단 이미지(Immunized Image)는 원본 이미지와 구분할 수 없지만 편집하면 오른쪽 하단과 같은 부자연스러운 이미지가 나온다. 포토가드는 ‘인코더 공격(Encoder attack) ’과 ‘확산 공격(Diffusion attack)’이라는 두 가지 기술을 사용한다. 인코더 공격은 이미지에서 픽셀의 색상과 위치를 설명하는 데이터에 복잡성을 추가하여 AI가 이미지를 이해하기 어렵게 만든다. 확산 공격은 확산 모델을 통해 이미지를 다른 이미지로 위장해 AI를 더욱 혼란스럽게 만든다. AI가 이러한 위장된 이미지에 대해 시도하는 모든 편집은 가짜 대상 이미지에 적용되기 때문에 비현실적으로 보이는 이미지가 생성된다. 인코더 공격은 AI로 하여금 편집할 원본 이미지가 다른 이미지(예: 회색 이미지)라고 생각하게 만들고, 확산 공격은 AI가 위장된 이미지(예: 회색 또는 임의의 이미지)를 편집하도록 강제한다는 설명이다. 연구팀은 이러한 기술이 아직 완벽하지 않다고 지적했다. 악의적인 공격자가 디지털 노이즈를 추가하거나 사진을 자르거나 뒤집음으로써 보호된 이미지를 리버스 엔지니어링할 수 있다. 또 포토가드 실행 시에는 대량의 메모리가 필요하다고 한다. 향후에는 메모리와 AI 방해 기능 강화를 목표로 개발이 진행될 예정이다. 박찬 기자 cpark@aitimes.com"
}