{
    "title": "스태빌리티 AI, '온디바이스 AI' 최적화된 30억 매개변수 LLM 출시",
    "created_at": "2023.12.08 18:00",
    "content": "스태빌리티 AI가 30억 매개변수의 초소형 언어 모델을 출시했다. 이미지 생성 인공지능(AI) '스테이블 디퓨전'으로 시작한 이 회사는 언어, 코딩, 오디오, 비디오 생성 모델에 이어 '온디바이스 AI'까지 영역을 넓히고 있다. 벤처비트는 7일(현지시간) 스태빌리티 AI가 텍스트 생성, 요약, 콘텐츠 개인화를 지원하는 30억 매개변수의 대화형 대형언어모델(LLM) ‘스테이블LM 제퍼 3B(StableLM Zephyr 3B)’를 오픈 소스로 출시했다고 보도했다. 이 모델은 크기가 작아 넓은 범위의 하드웨어에 배포할 수 있으며 적은 리소스를 사용하면서도 빠른 응답을 제공할 수 있어 온디바이스 AI에 최적화됐다는 평가다. 스태빌리티 AI는 지난 4월 70억 매개변수의 대화형 LLM ‘스테이블LM-7B’와 30억 매개변수의 ‘스테이블LM-3B’를 공개한 바 있다. 이번에 출시한 스테이블LM 제퍼 3B는 기존 스테이블LM-3B의 확장 버전이다. 매개변수가 70억개인 메타의 '라마 2 7B'의 절반 크기에 못 미치지만, 2배 이상의 토큰 수를 가진 데이터셋에서 훈련해 추론 품질을 극대화했다. 특히 인간 피드백을 통한 강화 학습 방식(RLHF) 대신 '직접 선호 최적화(DPO)'라는 접근 방식을 사용했다. DPO는 지난 5월 스탠포드대학교 연구진이 발표한 강화 학습법으로, RLHF의 대안으로 꼽힌다. 인간 선호도에 맞는 결과를 도출한다는 것은 똑같지만, RLHF와는 달리 DPO는 모델이 생성한 답변 후보들을 보상 모델이 우열을 가려 학습하는 것이 아니라 보상 모델의 학습에 사용하는 선호도 데이터를 직접 학습에 사용한다. 따라서 보상 모델이 필요없고, 학습 과정에서 별도의 답변 후보를 샘플링하지 않아도 된다. 스테이블LM 제퍼 3B는 30억개 이하의 매개변수 규모로 DPO를 적용한 최초 모델이다. 스태빌리티 AI는 오픈BMP의 울트라피드백(UlraFeedBack) 데이터셋을 사용해 DPO를 적용했다. 울트라피드백에는 6만4000개 이상의 프롬프트와 25만6000개 이상의 응답이 포함돼 있다. DPO와 작은 매개변수, 최적 데이터셋 등으로 구축한 이 모델은 MT 벤치 평가에서 메타의 '라마-2-70b-챗'과 앤트로픽의 '클로드-V1' 등 더 큰 모델의 성능을 능가하는 것으로 나타났다. 에마드 무스타크 스태빌리티 AI CEO는 \"우리는 사용자 자신의 데이터에 맞춰 미세조정한 작고 개방적인 모델이 더 큰 일반 모델보다 성능이 뛰어나다고 믿는다\"라며 \"향후 새로운 스테이블LM 모델의 정식 출시를 통해 생성 언어 모델을 더 민주화할 수 있기를 기대한다\"라고 말했다. 한편 스태빌리티 AI는 지난 8월 애플리케이션 코드 개발을 위한 생성 AI 모델로 ‘스테이블코드(StableCode)’를 출시했다. 9월에는 새로운 오디오 생성 도구인 '스테이블 오디오(Stable Audio)'를 내놓았으며, 지난달에는 ‘스테이블 비디오 디퓨전(Stable Video ifusion)’의 프리뷰를 통해 비디오 생성 분야에 뛰어들었다. 또 지난주에는 1초만에 이미지를 생성해주는 ‘SDXL 터보(SDXL Turbo)’를 공개, 화제가 됐다. 박찬 기자 cpark@aitimes.com"
}