{
    "title": "로봇 시각·언어·행동 통합 첫 오픈 소스 모델 'VLA' 등장..\"기존 구글 RT-2 능가\"",
    "created_at": "2024.06.19 18:05",
    "content": "로봇이 특별한 훈련 없이도 새로운 작업을 수행할 수 있도록 하는 최초의 오픈 소스 시각-언어-행동(VLA) 모델이 나왔다. 벤처비트는 18일(현지시간) 스탠포드 대학교, UC 버클리 대학교, 구글 딥마인드, MIT 등의 연구진이 오픈 소스 VLA 모델 ‘오픈VLA(OpenVLA)’에 관한논문을 아카이브에 게재했다고 전했다. 언어 모델이 웹 규모의 데이터에서 일반적인 아이디어와 개념을 학습하는 방식과 마찬가지로, 오픈VLA는 웹에서 가져온 텍스트와 이미지를 사용해 다양한 현실 세계 개념을 이해하고 이 지식을 로봇 행동에 대한 일반화된 지침으로 변환한다. 이 기술을 고도화하면 현재보다 훨씬 적은 훈련으로 다양한 상황과 환경에서 다양한 작업을 수행할 수 있는 상황 인식 적응형 로봇을 만들 수 있다는 설명이다. 예를 들어, 이전에는 로봇에게 쓰레기를 버리도록 훈련시키는 것은 단순히 로봇에게 쓰레기를 식별하는 것과 쓰레기를 주워서 버리는 것만을 훈련시키는 것을 의미했다. 하지만 웹 데이터를 훈련한 VLA를 사용하면 모델은 이미 쓰레기가 무엇인지에 대한 일반적인 개념을 가지고 있기 때문에 구체적인 훈련 없이도 쓰레기를 식별할 수 있다. 심지어 행동을 취하도록 훈련받은 적이 없지만, 쓰레기를 어떻게 버리는 지에 대해서도 알 수 있다. 오픈VLA는 '프리즈매틱-7B 비전-언어 모델(VLM)'을 기반으로 구축된 70억 매개변수의 오픈 소스 VLA이다. 이 모델은 입력 이미지에서 특징을 추출하는 프리즈매틱-7B 기반의 시각 인코더와 언어 지시를 처리하는 '라마-2 7B' 모델로 구성됐다. '오픈-X(Open-X)'의 97만개 로봇 조작 데이터셋으로 프리즈매틱 모델을 미세조정했다. 또 모델이 로봇 행동에 매핑할 수 있는 특수 토큰을 출력하도록 구성했다. ‘테이블 닦기’와 같은 자연어 지시와 함께 카메라로 촬영한 입력 이미지를 받는다. 모델은 지시 사항과 시각 입력을 분석, 로봇이 원하는 작업을 수행할 수 있도록 하는 일련의 행동 토큰을 생성한다. 연구진에 따르면 오픈VLA는 이전 최첨단 VLA 모델인 구글의 'RT-2' 및 'RT-2-X' 모델보다 성능이 뛰어나다. 지난해 등장한 RT-2와 RT-2-X는 로봇 동작의 데이터셋를 미세조정하고 언어 이해와 시각적 장면 분석을 결합하는 '기본'으로 통했다. 특히 “오픈VLA는 테스트된 모든 작업에서 최소 50%의 성공률을 달성한 유일한 접근 방식으로, 다양한 언어 지시를 포함하는 모방 학습 작업에서 강력한 기본 옵션이 될 수 있다”라고 주장했다. 오픈 소스 모델인 오픈VLA의 릴리스 코드는깃허브에서 공개적으로 사용할 수 있으며, 모델 체크포인트는허깅페이스에서 사용할 수 있다. 박찬 기자 cpark@aitimes.com"
}