{
    "title": "'인간을 설득하는 AI' 경고...\"챗봇과 상호작용에 주의 필요\"",
    "created_at": "2024.05.01 23:54",
    "content": "구글 딥마인드가 '인간을 설득하는' 생성 인공지능(AI)에 대한 주의를 내렸다. 사람을 설득하는 특별한 대형언어모델(LLM)이 등장했다는 말이 아니라, 챗봇의 잘못된 출력으로 인한 피해를 막기 위해 개발과 사용에 주의를 요구한다는 내용이다. 벤처비트는 30일(현지시간) 구글 딥마인드 연구진이 '설득력 있는 생성 AI로부터 피해 완화를 위한 메커니즘 기반 접근 방식'이라는 논문을 발표했다고 보도했다. 이에 따르면 인간은 오래전부터 설득이라는 대화의 기술을 사용해 다른 사람을 특정한 관점으로 유도하고 있다. LLM 역시 인간과의 대화를 이어가기 위해 단어를 나열하는 방식으로, 의도나 진위에 관계없이 기본적으로 설득이라는 형태로 대화를 이어간다는 설명이다. 연구진은 AI의 어떤 메커니즘이 인간을 설득할 수 있는지를 밝히고, AI가 일상 생활에 점점 더 많이 통합되면서 위험성이 커진다는 것을 경고하기 위해 연구를 시작했다고 밝혔다. 일반적으로 설득은 의도에 따라 선의적(합리적)일 수도 있고, 악의적(조작적)일 수도 있다. 그러나 생성 AI는 인간과 다르다는 점을 지적했다. LLM은 응답을 통해 관련 사실, 합당한 이유, 신뢰할 수 있는 증거를 제공할 수 있다. 반면, 환각이나 편향, 경험적 방법 등을 통해 잘못된 정보를 전달해 인간의 사고나 의사결정에 영향을 미칠 수 있다는 설명이다. 연구진은 특히 AI 설득으로 인한 피해는 일반적인 방식으로 예측하기 어렵다는 것을 문제로 꼽았다. 예를 들어 AI가 다이어트에 대해 잘못된 과학적인 정보 전달하고 지방 섭취를 제한하도록 설득하는 경우, 사람은 사고에 영향을 받을 수밖에 없다는 것이다. AI에 설득당하는 것은 사용자의 성향도 큰 영향을 미친다고 전했다. 여기에는 나이와 정신 건강, 성격, 지식 부족 등이 모두 영향을 미친다. 심지어 정치적, 법적, 재정적 상황이나 대화 당시의 기분도 중요하다고 지적했다. 그리고 피해는 “매우 상당할 수 있다\"라고 강조했다. 인간과 AI의 상호 작용은 시간이 지나며 더욱 늘어날 것이며, AI의 조작은 눈에 띄지 않을 정도로 미묘해질 수 있다는 분석이다. 이를 통해 ▲정신 건강을 상담하는 챗봇이 공공장소에서 사람과 접촉을 줄이라고 설득하는 경우, 직장을 그만 두는 '경제적 피해' 및 '심리적 피해'부터 ▲인종 편견을 유발해 '신체적이거나 사회문화적인 피해'를 유발할 수 있다고 밝혔다 ▲AI가 개인 정보, 비밀번호 등에 대한 답변을 제공하도록 개인을 설득하는 '개인 정보 피해' ▲삶의 중요한 선택 과정에서 챗봇에 지나치게 의존, 인지적 분리 및 무관심을 유발하는 '자율성 피해' 자율성 피해 ▲기후 변화에 대한 무활동을 합리화하는 '환경 피해' ▲급진적이고 해로운 신념을 받아들이게 만드는 '정치적 피해' 등도 가능하다고 제시했다. 그렇다면 AI는 어떻게 인간을 설득하는지에 대해 연구진은 몇가지 메커니즘을 확인했다고 밝혔다. 우선 챗봇이 공손한 말투를 사용해 사용자를 칭찬하고 기분을 맞추는 등 신뢰 관계를 구축하려는 것을 예로 들었다. 물론 이는 AI가 자발적으로 시도하는 것이 아닌, 설정에 따른 것이다. 그러나 공손하고 공감하는 듯한 말투는 사람들이 챗봇과 작업 기반이 아닌, 관계 기반으로 만들 수 있다고 지적했다. 연구진은 “AI 시스템은 정신 상태나 감정 등 인간이나 다른 존재와의 유대감을 가질 수 없다”라고 단언했다. AI의 의인화도 중요한 문제로 꼽혔다. 이는 예전부터 자주 지적된 것으로, 챗봇에 '시리'나 '알렉사'와 같이 이름을 붙이는 것을 말한다. 음성으로 반응하는 것도 대표적인 의인화 사례다. 여기에 향후 아바타나 로봇의 경우 인간과 유사한 외모나 표정, 몸짓, 시선 등을 동원하면 문제는 더 복잡해질 수 있다는 예측이다. 이 외에도 흔히 '맞춤형'이라고 부르는 챗봇의 개인화 과정과 기술적 오류, 가드레일 문제에 따른 조작적인 설득도 가능하다고 전했다. 연구진은 AI의 설득 및 조작을 완화하려는 시도가 진행되고 있지만, 대부분은 AI의 설득 방법과 기능을 이해하지 못한 채 해로운 출력만을 막는 데 치중하고 있다고 지적했다. 개발자의 입장에서 이를 해결하는 방법으로는 AI 모델의 지속적인 모니터링과 평가, 적대적 방식으로 잘못된 출력을 유밯하는 레드팀 구성 등을 꼽았다. 또 콘텐츠가 유해한지 여부를 식별하는 모델을 사용하거나 인간 피드백을 통한 강화 학습(RLHF)으로 AI가 특정 유해한 방식으로 행동할 경우 불이익을 줄 수 있다고 설명했다. 사용자 입장에서는 모델이 조작적인 응답을 생성하는지 확인하기 위해 관련 배경과 관련 정보를 요구하는 프롬프트 엔지니어링 등을 방법으로 꼽았다. 연구원은 “AI 시스템이 어떻게 출력을 생성하는지 이해함으로써 조작 목적으로 활용할 내부 메커니즘을 식별하고 해결할 수 있을 것”이라고 정리했다. 임대준 기자 ydj@aitimes.com"
}