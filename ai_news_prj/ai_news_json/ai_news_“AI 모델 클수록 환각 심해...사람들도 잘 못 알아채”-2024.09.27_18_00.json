{
    "title": "“AI 모델 클수록 환각 심해...사람들도 잘 못 알아채”",
    "created_at": "2024.09.27 18:00",
    "content": "인공지능(AI) 모델이 클수록 모른다고 인정하기 보다는 잘못된 답변을 내놓을 가능성이 높다는 연구 결과가 나왔다. 또 사람들은 잘못된 답변을 잘 알아차리지 못한다는 사실도 밝혀졌다. 실리콘앵글은 26일(현지시간) 스페인 발렌시아 AI 연구소 연구진이 모델의 크기에 따른 환각 오류 변화 대한 연구 결과를네이처에 게재했다고 보도했다. 연구진은 오픈AI의 'GPT'와 메타의 '라마', 학술 그룹 빅사이언스의 오픈 소스 모델 '블룸(BLOOM)' 등 세가지 대형언어모델(LLM) 제품 군을 검토했다. 각각에 대해 초기의 원시 버전과 이후의 개선된 버전을 비교하는 방식이다. 연구진은 산술, 애너그램(문자 순서를 바꿔 다른 단어나 문장을 만드는 것), 지리, 과학 등 수천개의 프롬프트와 목록을 알파벳순으로 정리하는 등 챗봇의 정보 변환 능력을 테스트하는 프롬프트로 모델을 테스트했다. 또 인간이 느끼는 질문의 난이도를 순위로 매겼다. 예를 들어, 캐나다 토론토에 관한 질문은 더 쉬운 질문으로, 멕시코의 잘 알려지지 않은 작은 마을인 아킬에 관한 질문은 더 어려운 질문으로 평가했다. 결과는 예상대로 모델이 커질수록 정답률이 증가했으며, 질문이 어려워질수록 정답률은 감소했다. 그러나 매우 어려운 질문에 대해서도 모델이 답변을 피하기 보다 잘못된 대답을 내놓는 경향이 뚜렷하게 나타났다. 특히 'GPT-4' 등 일부 모델은 거의 모든 질문에 답변했다. 따라서 모델이 커질수록 오답이거나 답변을 회피한 경우의 비율이 증가했고, 일부 첨단 모델에서는 그 비율이 60%를 넘었다. 또 연구진은 모든 모델이 때로 쉬운 질문에서도 틀린 답을 내놓는 경우가 있어, 사용자가 답변에 대해 높은 신뢰를 가질 수 있는 안전 영역이 없다는 점을 발견했다고 전했다. 더불어 사람들에게 모델의 답변을 '정확'이나 '부정확', '회피'로 분류하도록 요청했다. 참여자들은 놀랍게도 부정확한 답변을 정확하다고 잘못 분류하는 경우가 많았는데, 쉬운 질문과 어려운 질문 모두에서 약 10~40% 정도의 빈도로 발생했다. 따라서 연구진은 “인간은 이런 모델들을 제대로 감독할 수 없다”라고 결론 내렸다. 이 문제를 해결하기 위해 개발자들은 모델을 조정해 쉬운 질문에서 발생하는 환각을 처리하고 정확도를 개선해야 하지만, 모르는 것을 모른다고 답하도록 만들어야 한다고 주장했다. 비풀라 로우테 사우스캐롤라이나대학교 컬럼비아 캠퍼스 컴퓨터 과학자는 \"의료용과 같이 특정 목적을 위해 미세조정한 모델은 환각이 많이 개선되는 편\"이라며 \"하지만 범용으로 챗봇을 판매할 경우, 문제가 생길 가능성이 크다\"라고 지적했다. 박찬 기자 cpark@aitimes.com"
}