{
    "title": "앤트로픽, 'AI 대량 학살·파괴' 방지 위한 4단계 프레임워크 공개",
    "created_at": "2023.09.24 08:00",
    "content": "앤트로픽이 인공지능(AI)이 불러올 재앙에 대비하기 위한 프레임워크를 내놓았다. 이를 위해 AI 위험 정도를 4단계로 나누고, 현재 대형언어모델(LLM)은 2단계에 해당한다고 규정했다. 벤처비트는 21일(현지시간) 앤트로픽이 '책임있는 확장정책(Responsible Scaling Policy)'을 발표, AI 시스템 개발에 신중을 기하겠다는 약속을 소개했다. “AI 모델이 없었다면 발생하지 않았을, AI 모델로 인해 직접적으로 발생하는 수천명의 사망 또는 수천억 달러의 피해”로 이어질 수 있는 시나리오를 방지하기 위한 것으로 설명했다. 이에 따르면 RSP는 AI 모델이 직접적으로 대규모 파괴를 일으킬 수 있는 상황, 또는 재난적인 위험(catastrophic risks)을 완화하기 위해 특별히 설계한 프레임워크다. 이는 지난 7월 발표한 'AI 안전 서약'의 일환으로,  관련 기준으로는 처음으로 제시된 것이다. 앤트로픽은 생물학적 물질 취급에 대한 미국 정부의 생물안전성 수준(BSL) 표준을 차용, 'AI 안전 수준(ASL)'을 4단계로 규정했다고 전했다. ASL 수준이 높아질수록 더 엄격한 안전성을 요구하는 구조다. 우선 ASL-1은 '알파고'나 2018년에 등장한 'BERT'와 같이 특정 기능만 수행, 별다른 위험이 없는 단계를 말한다. ASL-2는 생화학 무기 제조 방법을 제공하는 등 초기 위험 징후를 보여주는 시스템을 말하지만, 신뢰성이 부족하거나 검색 엔진이 제공할 수 없는 정보를 만들 수는 없어 아직 유용하지 않은 시스템을 말한다. '클로드'를 비롯해 현재 LLM은 ASL-2로 규정했다. ASL-3은 단순 검색 엔진이나 텍스트에 비해 치명적인 오용 위험을 실질적으로 증가시키거나 낮은 수준의 자율 기능을 보여주는 AI 시스템을 말한다. ASL-4 이상은 치명적인 오용 가능성은 물론 AI 시스템 자체의 자율성이 확대되는 사실상의 AGI(인공일반지능) 단계로, 현재 상황과는 너무 멀리 떨어 아직 구체안을 마련하기는 어렵다고 밝혔다. 앤트로픽은 단계에 맞춰 AI 모델에 대한 적절한 안전성 평가, 배포 및 감독 절차를 통해 잠재적 위험을 반영하고 관리하도록 RSP를 설계했다고 밝혔다. 또 독립적인 감독을 위해 이를 변경하는 경우에는 회사의 최고 의결기구인 이사회의 승인을 거칠 것을 권장했다. 특히 ASL 시스템은 안전 확인이 필요한 강력한 LLM 모델이 등장할 경우, 암묵적으로 모델 훈련을 일시적으로 중지하도록 요구한다고 전했다. 다만 RSP가 클로드 등 현재 서비스 중인 제품을 변경하거나 가용성을 방해하는 것은 아니며, 제조업에서도 일반적인 제품 테스트에 가까운 것으로 봐야 한다고 밝혔다. 또 정책 입안자나 다른 LLM 회사에도 영감을 제공하길 바란다고 덧붙였다.홈페이지에서 자세한 사항이나 문서 전체를 다운받을 수 있다. 샘 맥캔드리시 앤트로픽 공동 창업자는 \"RSP는 우리의 경험과 피드백을 통해 업데이트하고 개선할 진화해야 할 문서\"라며 “AI 모델 출시에 앞서 안전 테스트를 너무 대충 넘어가려는 유혹을 경계해야 한다\"고 말했다. 한편 앤트로픽은 이에 앞서 공개한 '헌법적 AI(Constitutional AI)'로도 유명한 회사다. 이는 AI가 인간 피드백을 기반으로 수정하는 것이 아니라, 지도 학습이나 강화 학습 등의 기법을 이용해 스스로 개선하는 방법을 말한다. 여기에서 인간의 역할은 기본적인 규칙이나 원칙만 제공하는 것이라, 이를 빗대 '헌법'이라는 말을 붙였다. 클로드가 유명해진 것도 오픈AI의 'GPT'나 구글의 '람다'보다 정확한 답을 내놓는다는 점 때문이었다. 벤처비트는 \"헌법적 AI와 RSP는 AI 안전과 윤리적 고려에 대한 앤트로픽의 헌신을 돋보이게 한다\"라며 \"이 회사는 유용성을 극대화하면서도 피해를 최소화하는 데 중점을 둬, AI의 미래를 위한 높은 기준을 제시했다\"라고 평가했다. 박찬 기자 cpark@aitimes.com"
}