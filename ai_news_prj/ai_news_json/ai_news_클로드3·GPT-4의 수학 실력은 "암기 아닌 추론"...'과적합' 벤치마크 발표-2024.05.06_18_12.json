{
    "title": "클로드3·GPT-4의 수학 실력은 \"암기 아닌 추론\"...'과적합' 벤치마크 발표",
    "created_at": "2024.05.06 18:12",
    "content": "대형언어모델(LLM)의 답변이 추론에 의한 것인지, 아니면 단순 암기 사항을 빋어내는 것인지를 판단하는 벤치마크 방법이 제안됐다. 그 결과 '클로드 3'나 'GPT-4'는 진짜 추론을 하고, '미스트랄'이나 '파이'는 암기에 가깝다는 결과가 나왔다. 마크테크포스트는 4일(현지시간) 스케일AI 연구진이 LLM의 수학적 추론 능력을 평가하는새로온 벤치마크 'GSM1k'를 공개했다고 전했다. 수학 능력은 LLM의 평가를 위한 주요한 잣대 중 하나다. 언어 항목과는 달리, 수학은 정확한 답이 하나만 존재하기 때문에 추론 능력을 가늠할 수 있다. 이런 이유로 인공일반지능(AGI) 개발에서도 수학적 능력이 중요하다고 지적된다. 이제까지 LLM의 수학 능력을 평가하기 위해 주로 사용한 벤치마크로는 GSM8k, 매스(MATH), MBPP 등이 많이 사용됐다. 그러나 LLM 수학 능력 측정에는 '과적합(overfitting)' 문제가 따라다닌다. 머신 러닝 모델은 샘플 데이터를 학습, 데이터에 내재된 구조나 패턴을 일반화해서 답하는 추론 능력을 갖추게 된다. 하지만 너무 오래 학습하거나 모델이 복잡하면, 데이터셋 안의 '노이즈'나 관련 없는 정보를 학습할 수 있다. 이처럼 너무 과하게 적합하면, 모델은 추론이 아니라 데이타셋의 답을 외우고 답하게 된다. 연구진은 \"LLM은 수학적 추론에 대한 많은 벤치마크에서 인상적인 성공을 거뒀지만, 일부는 실제 추론 능력이 아닌 데이터셋 오염을 반영해 훈련 데이터를 유출된다는 우려가 커지고 있다\"라며 \"이 주장을 엄격하게 조사하기 위해 우리는 초등학교 수학 1000 문제(GSM1k)로 이를 파악했다\"라고 밝혔다. 연구진이 발표한 논문은 '초등학교 산술에 대한 대규모 언어 모델 성능에 대한 신중한 조사'다. 말 그대로 초등학교 수준이 간단한 수학 문제 풀이를 통해 LLM이 추론으로 답을 내는지, 단순 암기에 의존하는 지를 알아본 것이다. 이를 위해 기존 GSM8k와 형식은 비슷하지만, 해결률과 솔루션 단계수, 답면의 크기 등으로 추론 여부를 따질 수 있는 1250개의 문제로 구성한 GSM1k 데이터셋을 구축했다. 이를 통해 특정 모델이 두 벤치마크를 상대로 낸 점수를 비교하는 방식이다. 만약 어떤 모델이 기존 GSM8k와 새로운 GSM1k에서 비슷한 성능을 보인다면, 이는 추론을 통해 답을 냈다고 볼 수 있다. 반면, 기존 GSM8k에서는 성적이 좋지만 새로운 GSM1k에서 나쁜 점수를 얻는다면, 이는 추론이 아니라 기존 데이터셋의 답을 외워서 답했다는 결론이다. 이런 두 벤치마크의 편차가 가장 큰 모델, 즉 암기에 의존한 것으로 나타난 모델은 미스트랄 AI의 '믹스트랄-8x22B'와 마이크로소프트의 '파이-3-미니'였다. 두 벤치마크의 정확도가 최대 13%까지 차이가 났다. 하지만 현재 가장 성능이 뛰어난 것으로 알려진 '클로드 3'와 'GPT-4', '제미나이 프로'는 과적합 징후가 거의 없었고, 메타의 '라마 3'도 2% 정도의 성능 차 밖에는 나지 않았다. 연구진은 \"추가 분석에서는 모델이 GSM8k에서 예제를 생성할 확률과 GSM8k와 GSM1k 사이의 성능 격차 사이에 상관관계가 있다는 것이 나타났으며, 이는 우려 대로 많은 모델이 GSM8k를 부분적으로 기억했을 수 있음을 시사한다\"라고 전했다. 임대준 기자 ydj@aitimes.com"
}