{
    "title": "MS, 트랜스포머 성능 개선하는 새로운 LLM 아키텍처 공개",
    "created_at": "2024.10.18 18:05",
    "content": "'트랜스포머' 기반 대형언어모델(LLM)의 긴 컨텍스트 정보 검색 기능을 개선하는 새로운 아키텍처가 나왔다. 일종의 '변형 트랜스포머'라는 설명이다. 벤처비트는 16일(현지시간) 마이크로소프트(MS)와 칭화대학교 연구진이 관련 컨텍스트에 대한 어텐션(attention)을 증폭하고 노이즈를 걸러내 성능을 개선하는 새로운 아키텍처 ‘차등 트랜스포머(Diff Transformer)’에 관한논문을 아카이브에 게재했다고 보도했다. 트랜스포머 아키텍처는 대부분 LLM의 기반이다. 어텐션 메커니즘을 사용해 입력 텍스트 내 토큰이 출력 생성에 미치는 중요도를 평가하는 방식이다. 어텐션 메커니즘은 벡터 값을 확률 분포로 정규화하는 '소프트맥스(softmax)' 함수를 사용해 입력 시퀀스의 토큰에 어텐션 점수를 할당한다. 그러나 연구에 따르면, 트랜스포머는 긴 컨텍스트에서 중요한 정보를 효과적으로 찾아내는 데 어려움을 겪는 것으로 나타났다. 그 결과 LLM은 긴 입력 컨텍스트에서 정보를 제대로 활용하지 못하며, 특히 긴 컨텍스트의 중간에 있는 특정 정보를 찾아야 할 경우에는 성능이 크게 저하된다는 설명이다. 또 연구진은 LLM의 환각 현상, 즉 모델이 관련 컨텍스트 정보를 가지고 있음에도 불구하고 잘못된 출력을 생성하는 현상이 잘못된 어텐션 패턴과 관계가 있음을 발견했다. 트랜스포머의 어텐션 메커니즘이 소프트맥스 병목 현상으로 인해 관련 없는 컨텍스트에 과도하게 주의를 기울이며 산만해지는 경향이 있다는 것이다. 소프트맥스 함수는 모든 토큰에 어텐션 점수를 골고루 분배하는 경향이 있으며, 심지어 과제와 관련이 없는 토큰에도 점수를 할당한다. 이로 인해 모델은 긴 컨텍스트에서 중요한 부분에 집중하는 데 어려움을 겪을 수 있다. 이런 병목현상은 트랜스포머가 희소한 어텐션 분포를 학습할 수 없게 만든다. 즉, 특정 관련 컨텍스트에 집중하기보다는 평평하게 퍼지는 경향을 보인다. 이런 점을 해결하기 위해 연구진은 새로운 LLM 기반 아키텍처인 차등 트랜스포머를 개발했다. 핵심 아이디어는 ‘차등 주의(differential attention)’ 메커니즘을 사용, 노이즈를 제거하고 입력의 가장 관련 있는 부분에 어텐션을 증폭시키는 것이다. 차등 트랜스포머는 어텐션 계산에 쿼리(query), 키(key) 및 값(value) 벡터를 사용한다: 기존 메커니즘은 쿼리와 키 벡터 전체에 소프트맥스 함수를 적용했다. 차등 어텐션은 쿼리와 키 벡터를 두 그룹으로 나누고 두개의 별도의 소프트맥스 어텐션 맵을 계산하는 방식으로 작동한다. 여기에서 두 맵의 차이가 어텐션 점수로 사용된다. 이 과정은 일반적인 노이즈를 제거, 모델이 입력과 관련된 정보에 집중하도록 유도한다. 핵심은 '올바른 문제가 무엇인가'를 파악하는 것이다. 즉 입력 텍스트에서 출력에 중요한 토큰을 찾아내 올바른 질문을 하는 것이다. 연구자들은 차등 트랜스포머를 다양한 언어 모델링 작업에서 평가했다. 그 결과, 기존 트랜스포머 아키텍처를 능가하는 것으로 나타났다. 1조개의 토큰으로 훈련한 30억 매개변수의 차등 트랜스포머는 비슷한 크기의 트랜스포머 모델에 비해 몇 % 개선을 보였다. 또 다양한 모델 크기와 훈련 데이터셋 크기를 활용한 추가 실험에서, 기존 트랜스포머와 유사한 성능을 달성하기 위해 65%의 매개변수나 훈련 토큰만 필요한 것으로 나타났다. 연구진은 \"증가하는 컨텍스트 길이를 사용하는 데 특히 효과적\"이라고 밝혔다. 차등 트랜스포머 기반 모델은 중요한 정보 검색이나 환각 완화, 컨텍스트 내 학습에서 상당한 개선을 보였다고 강조했다. 현재 차등 트랜스포머의 코드는깃허브에서사용할 수 있다. 박찬 기자 cpark@aitimes.com"
}