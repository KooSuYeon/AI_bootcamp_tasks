{
    "title": "MIT, 동영상 시청만으로 언어 학습하는 AI 모델 ’덴스AV’ 공개",
    "created_at": "2024.06.17 18:05",
    "content": "동영상 시청만으로 언어를 학습할 수 있는 인공지능(AI) 모델이 등장했다. 이를 통해 인간의 언어뿐만 아니라 동물의 의사소통을 해석할 수 있는 길이 열렸다는 평가가 나왔다. MIT 뉴스는 MIT 연구진이 텍스트 입력 없이 듣고 보는 것만으로 언어를 학습하게 설계된  AI 모델 ‘덴스AV(DenseAV)’에 관한논문을 아카이브에 게재했다. 덴스AV는 아이들이 말을 배울 때 주변 환경을 관찰하며 시각과 소리 사이의 연관성을 학습하는 방식과 같다. 즉, 동영상을 시청하면서 소리와 시각 사이의 연관성을 학습한다. 예를 들어 동영상에서 ‘케이크를 350도에서 굽는다’라는 말을 나올 때, 화면에 등장한 것이 케이크나 오븐이라는 것을 배우는 식이다. 덴스AV는 이처럼 사전 훈련된 언어 모델이나 주석이 달린 데이터셋을 사용하지 않고, 완전 백지 상태에서 언어 의미를 학습한다. 이를 위해 두가지 구성 요소를 사용, 오디오와 시각 데이터를 별도로 처리한다. 각 구성 요소는 자체 데이터 소스에서 의미 있는 신호를 추출한다. 그 다음 오디오 신호와 시각적 신호 쌍을 비교, 어떤 신호가 일치하고 어떤 신호가 일치하지 않는지 알아낸다. 이런 대조 학습 접근 방식을 사용하면 모델은 데이터 주석(라벨) 없이도 중요한 예측 패턴을 파악할 수 있다. 오디오 소스에서는 개짖는 소리를 추출하고 비디오 소스에서는 개 모습의 픽셀을 추출하여 개의 모습과 짖는 소리 간의 연관성을 학습하는 식이다. 특히 사운드와 이미지의 연관성을 전체 이미지 프레임 대신, 픽셀 수준에서 학습한다. 이를 통해 비디오 스트림에서 배경 요소까지 식별할 수 있어 훨씬 더 상세한 정보를 이해할 수 있다. 프레인 단위로 학습하는 경우에는 ‘잔디에 앉아 있는 개’의 사진에서 ‘잔디’과 ‘개’ 사이의 연결과 같은 세부 사항을 발견할 수 없다. 연구진은 200만개의 유튜브 동영상이 포함된 오디오셋에서 모델을 훈련했다. 또 모델이 소리와 이미지를 얼마나 잘 연결할 수 있는지 테스트하기 위해 새로운 데이터셋을 만들었다. 이 테스트에서 이름과 소리로 개체를 식별하는 등의 작업에서 다른 첨단 모델보다 뛰어난 성능을 발휘했다. 연구진은 흥미로운 응용 분야로 동물 의사소통을 꼽았다. 글로 된 형태가 없는 돌고래나 고래의 의사소통과 같은 새로운 언어를 이해하는 데 사영할 수 있다는 설명이다. 연구진은 \"덴스AV가 오랫동안 인간이 이해할 수 없었던 언어를 번역하는 데 도움을 줄 수 있기를 기대한다\"라고 전했다. 박찬 기자 cpark@aitimes.com"
}