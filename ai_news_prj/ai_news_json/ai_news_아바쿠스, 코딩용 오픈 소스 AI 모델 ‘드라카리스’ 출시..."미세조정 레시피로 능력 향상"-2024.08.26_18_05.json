{
    "title": "아바쿠스, 코딩용 오픈 소스 AI 모델 ‘드라카리스’ 출시...\"미세조정 레시피로 능력 향상\"",
    "created_at": "2024.08.26 18:05",
    "content": "인공지능(AI) 스타트업 아바쿠스AI가 코딩 작업에 특별히 최적화된 새로운 오픈 소스 대형언어모델(LLM) 제품군 '드라카리스(Dracarys)’를 출시했다. 드라카리스는 기존 LLM에 특별한 미세조정 레시피를 적용, 코딩 능력을 크게 향상했다. 벤처비트는 23일(현지시간) 스타트업 아바쿠스가 오픈 소스 LLM 제품군 드라카리스를 출시했다고 보도했다. 아바쿠스는 ‘드라카리스 레시피’를 700억 매개변수 모델인 메타의 '라마 3.1-70B'와 알리바바의 '큐원-2 72B'에 적용했다. 이 레시피에는 LLM의 코딩 능력을 향상하기 위한 자체 훈련 데이터셋과 미세조정 기술이 포함된다. 벤치마크 결과, 드라카리스 레시피를 사용해 미세조정한 모델의 코딩 능력이 상당히 향상된 것으로 나타났다. 라이브벤치(LiveBench) 코딩 테스트에서 라마-3.1-70b-인스트럭트 터보 모델은 드라카리스 레시피를 적용한 뒤 32.67점에서 35.23점으로 정확도가 증가했다. 큐원2-72b-인스트럭트 모델은 코딩 점수가 32.38점에서 38.95점으로 상승했다. 빈두 레디 마바쿠스 CEO는 \"큐원2-72b-인스트럭트를 드라카리스 레시피로 미세조정한 드라카리스-72B-인스트럭트가 현재 오픈 소스 옵션 중에서 동급 최고의 코딩 모델\"이라고 주장했다. 현재 허깅페이스에서 라마 3.1-70B와 큐원-2 72B 모델용 드라카리스 버전의 모델 가중치를 사용할 수 있다. 아바쿠스는 딥시크-코더와 라마-3.1 400B 모델용 드라카리스 버전도 출시할 예정이다. 이 회사의 미세조정 기술을 적용하면 오픈 소스 LLM의 코딩 기능을 향상할 수 있다는 설명이다. 한편, 아바쿠스는 지난 2월 '스마우그(Smaug)'라는 오픈 소스 모델로, 허깅페이스 LLM 리더보드에서 세계 최초로 평균 80점을 돌파해 화제가 됐던 스타트업이다. 당시 모델명인 스마우그는 '호빗'에 등장하는 용의 이름이었으며, 이번 드라카리스는 '왕좌의 게임'에서 용에게 불을 뿜으라고 지시하는 말이다. 박찬 기자 cpark@aitimes.com"
}