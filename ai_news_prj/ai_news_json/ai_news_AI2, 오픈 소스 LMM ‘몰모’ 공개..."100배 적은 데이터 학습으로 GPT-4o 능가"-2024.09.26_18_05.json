{
    "title": "AI2, 오픈 소스 LMM ‘몰모’ 공개...\"100배 적은 데이터 학습으로 GPT-4o 능가\"",
    "created_at": "2024.09.26 18:05",
    "content": "앨런AI연구소(AI2)가 오픈 소스 대형멀티모달모델(LMM) 제품군 '몰모(Molmo)’를 출시했다. AI2는 몰모 모델이 고품질 데이터를 학습, 벤치마크에서 오픈AI의 'GPT-4o'를 능가하는 성능을 보였다고 주장했다. 벤처비트는 25일(현지시간) AI2가 이미지를 설명하는 4종의 오픈 소스 LLM ▲몰모-72B ▲몰모-7B-D ▲몰모-7B-O ▲몰모E-1B를 출시했다고 보도했다. 현재 몰모 제품군은허깅페이스에서연구 및 상업적 용도로 사용가능하다. 플래그십 모델 몰모-72와 몰모-7B-D는 알리바바의 오픈 소스 '큐원2-72B'가 기반이다. 몰모-7B-O는 AI2의 '올모-7B'가, 몰모E-1B는 AI2의 전문가 혼합(MoE) 모델인 '올모E-1B-7B'가 베이스 모델이다. AI2에 따르면, 몰모-72B 모델은 여러 벤치마크에서 오픈AI의 GPT-4o, 앤트로픽의 '클로드 3.5 소네트', 구글의 '제미나이 1.5' 등 주요 폐쇄형 경쟁 모델보다 우수한 성능을 보였다. 예를 들어, 몰모-72B는 이미지 형식으로 제공되는 다양한 문서에서 정보를 이해하고 추출하는 능력을 평가하는 DocVQA에서 96.3점, 이미지 내의 텍스트를 이해하는 능력을 평가하는 TextVQA에서 85.5점을 기록, 제미나이 1.5 프로와 클로드 3.5 소네트를 능가했다. 초등학교 과학 다이어그램 이해 능력을 평가하는 AI2의 자체 벤치마크 AI2D에서는 GPT-4o보다 더 높은 성능을 보였다. 또 텍스트로 설명된 객체를 이미지에서 식별하는 시각적 접지(Visual Grounding) 능력 평가 리얼월드QA에서 최고의 성능을 기록, 로봇공학이나 복잡한 멀티모달 추론 분야에 특히 유망한 것으로 평가됐다. AI2는 \"훨씬 더 효율적인 데이터 수집과 학습 방법 덕분에 이룬 성과\"라고 밝혔다. 인터넷에서 무차별적으로 스크래핑한 데이터를 학습하는 기존 모델과 달리, AI2의 모델은 정교하게 큐레이팅한 고품질 데이터로 학습해 좋은 성과를 거둘 수 있었다는 설명이다. 인간 주석자가 60만개의 이미지에 대해 여러 페이지의 텍스트에 걸쳐 자세하게 설명한 데이터셋으로 몰모 모델을 훈련했다고 밝혔다. 이를 통해 폐쇄형 경쟁 모델에 비해 1000배 적은 훈련 데이터를 사용했지만, 여러 벤치마크에서 우수한 성능을 기록했다는 주장이다. 또 이미지에 포함된 내용을 성공적으로 설명하고, 이미지 내 사물  개수를 세며 요청한 다른 사물들을 정확하게 지목할 수 있다. 이는 이미지의 요소를 분석하여 질문에 답하는 픽셀을 식별할 수 있음을 의미한다. AI2는 \"다른 고급 AI 모델들도 장면과 이미지를 묘사하는 데 능숙하지만, 더 정교한 웹 에이전트를 구축하려면 장면과 이미지에서 객체를 정확하게 지목할 수 있는 능력이 중요하다\"라고 강조했다. 즉, AI 에이전트는 웹에서 상호작용하고 비행기 표를 예약하는 등의 작업을 수행해야 할 때 객체를 지목하는 기능이 매우 중요하다는 뜻이다. 박찬 기자 cpark@aitimes.com"
}