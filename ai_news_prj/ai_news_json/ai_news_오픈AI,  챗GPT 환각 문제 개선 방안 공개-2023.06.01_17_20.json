{
    "title": "오픈AI,  챗GPT 환각 문제 개선 방안 공개",
    "created_at": "2023.06.01 17:20",
    "content": "오픈AI가 인간과 유사한 사고 접근 방식으로 '챗GPT'의 환각(hallucination) 문제를 개선하는 새로운 방법을 공개했다. CNBC에 따르면 오픈AI는 31일(현지시간) 발표한 논문에서 챗GPT가 다단계 추론 과정에서 생성하는 최종 답변에 보상하는 대신 각각의 추론 단계에서 개별 답변에 보상하는 훈련을 통해 인공지능(AI) 환각 문제를 개선하는 방안을 내놓았다. AI 환각은 오픈AI의 챗GPT나 구글의 '바드'와 같은 대규모 언어 모델(LLM)이 완전히 날조된 거짓 정보를 마치 사실인 것처럼 그럴듯하게 내뱉는 것을 말한다. 지난 2월에 구글의 바드가 태양계 밖 행성의 첫 번째 사진을 제임스 웹 우주망원경이 찍었다고 거짓 주장을 한 것이나 최근에 뉴욕 변호사가 재판에서 챗GPT가 지어낸 가짜 판례를 인용한 것이 대표적인 AI 환각 사례다. 오픈AI는 논문에서 “심지어 최첨단 모델도 거짓을 만들어내는 경향이 있다”며 “이는 불확실한 순간에 사실을 지어내려는 경향을 보여준다”고 설명했다. 이어 한 단계에서 논리적 오류가 발생하면 단계를 거치면서 더 큰 오류를 초래할 수 있기 때문에 이런 환각은 다단계 추론이 필요한 영역에서 특히 문제가 된다고 지적했다. 오픈AI는 주어진 쿼리에 대한 최종 답변에 보상하는 ‘결과 감독(outcome supervision)’ 대신 각각의 추론 단계별 답변에 보상하는 ‘과정 감독(process supervision)’ 방식으로 LLM을 훈련하는 것이 효과적이라고 강조했다. 오픈AI는 사용자의 의도에 맞는 답변을 생성하기 위해 챗GPT에 인간 피드백 기반 강화 학습(RLHF)를 도입했다. RLHF는 LLM에 프롬프트를 제공하고 여러 출력을 생성하도록 한 다음 인간 평가자에게 생성된 텍스트의 순위를 최고에서 최악으로 매기도록 요청한다. 그런 다음 LLM 텍스트에서 점수를 예측하도록 보상 모델을 학습한다. 새로운 과정 감독 방식은 다단계로 구성되는 추론의 각 개별 단계마다 RLHF를 적용하는 식이다. 이 방식은 AI 모델이 인간과 유사한 사고 과정을 따르도록 하기 때문에 더 ‘설명 가능한 AI(Explainable AI)’로 이어질 수 있다는 설명이다. 회의적인 시각도 있다. 벤 윈터스 전자 개인정보 센터 선임 고문은 \"단순히 그것만으로는 AI의 잘못된 정보와 결과에 대한 우려를 완화하기 어렵다\"고 언급했다. 수레시 벤카타수브라마니안 브라운 대학 기술 책임 센터장은 \"대규모 언어 모델의 작동 방식은 전반적으로 불안정하기 때문에, 어떤 환경에서는 작동하더라도 다른 환경에서는 작동하지 않을 수 있다\"고 지적했다. AI타임스 박찬 기자 cpark@aitimes.com"
}