{
    "title": "메타, AI의 ‘현실 이해' 돕는 오픈 소스 데이터셋 출시",
    "created_at": "2024.04.12 18:05",
    "content": "메타가 인공지능(AI) 시스템의 주변 환경 이해를 측정하기 위한 벤치마크 데이터셋을 공개했다. 이는 AI가 주변 환경에 대한 질문에 자연어로 답할 수 있는 환경 이해를 돕기 위한 것으로, 지난 2월 출시한 'V-제파(JEPA)' 모델에 이어 기존 텍스트 위주의 학습과는 다른 방법으로 AI의 지능을 끌어 올리려는 시도다. 벤처비트는 10일(현지시간) 주변 환경 이해를 측정하는 오픈 소스 벤치마크 데이터셋 ‘오픈EQA(OpenEQA)’를 출시했다고 보도했다. 오픈EQA에는 객체 및 속성 인식, 공간 및 기능 추론, 상식 지식 등 7가지 질문 범주에 걸쳐, 집과 사무실과 같은 180개 이상의 다양한 실제 환경에 대한 1600개 이상의 질문이 포함돼 있다. 이는 AI 모델이 세상을 인지하고 상호작용하거나 인간과 자연스럽게 소통하며 일상생활에 도움을 줄 수 있는 'AI 에이전트' 개발을 돕는 것이 목표다. 예를 들어 증강현실(AR) 안경에 탑재한 AI 비서나 가정용 로봇에 이를 적용하면, 비디오 카메라나 센서에 사진을 제공하고 ‘열쇠를 어디에서 보았나요’라고 물으면 답할 수 있다. 메타 연구진은 오픈EQA 데이터셋을 구축하기 위해 실제 환경의 비디오 데이터와 3D 스캔을 수집했다. 그다음 비디오를 사람에게 보여주고 AI 에이전트에게 물어보고 싶은 질문을 작성하도록 요청했다. 그 결과로 인식 및 추론 능력을 테스트할 수 있는 1636개의 질문 세트을 구축했다. 예를 들어 ‘식탁 주위에 의자가 몇 개 있나요’라는 질문에 답하려면 AI는 장면의 객체를 인식하고, 주변이라는 공간 개념을 이해하여, 관련 객체의 수를 계산해야 한다. 경우에 따라서는 AI가 개체의 용도와 속성에 대한 기본 지식을 갖고 있어야 한다. AI 에이전트의 성능을 측정하기 위해 연구진은 대형언어모델(LLM)을 사용, AI 생성 답변이 인간 답변과 얼마나 유사한지 점수를 매겼다. 그 결과 'GPT-4V'와 같은 첨단 비전언어모델(VLM) 모델조차도 새로운 벤치마크인 오픈EQA에서 인간 수준의  답변을 하는 데 어려움을 겪는 것으로 나타났다. 실제로 공간 이해가 필요한 질문의 경우, 기존 VLM은 거의 ‘맹인’ 수준이라는 지적이다. 연구진은 이 벤치마크 세트를 통해 AI가 물리적인 세상을 더 잘 이해하는 방향으로 발전하기를 바란다고 밝혔다. 또 메타는 이에 앞서 지난 2월 'V-제파(Video Joint Embedding Predictive Architecture)'라는 모델을 통해  현실 세계에서 일어나는 개체 간 상호작용을 이해하고 예측할 수 있는 방법을 제시한 바 있다. 인간이 주변 세계를 배우는 방법처럼 AI도 물리적인 세계를 시각으로 경험, 수많은 텍스트 데이터를 학습하는 것보다 실질적이고 유용한 지식을 얻게 한다는 의도다. 이는 \"기계 지능을 발전시키는 중요한 단계\"라고 강조했다. 즉 메타는 최근 인공일반지능(AGI)으로 발전할 중요한 단서로 기존 '트랜스포머' 아키텍처와는 다른 제파 모델을 강조하고 있다. 이를 통해 AI가 사람처럼 추론하고 계획을 세울 수 있다는 설명이다. 이날 공개한 데이터셋도 이런 맥락에서 공개한 것으로 볼 수 있다. 박찬 기자 cpark@aitimes.com"
}