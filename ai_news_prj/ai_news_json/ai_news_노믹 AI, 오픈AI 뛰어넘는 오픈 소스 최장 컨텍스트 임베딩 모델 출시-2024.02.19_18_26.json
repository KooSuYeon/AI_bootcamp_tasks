{
    "title": "노믹 AI, 오픈AI 뛰어넘는 오픈 소스 최장 컨텍스트 임베딩 모델 출시",
    "created_at": "2024.02.19 18:26",
    "content": "현존 최고인 오픈AI의 ‘텍스트-임베딩-에이다-002(text-embedding-ada-002)’보다 성능이 뛰어나다는 오픈 소스 텍스트 임베딩 모델이 등장했다. 이를 통해 오픈 소스 대형언어모델(LLM)도 성능을 끌어올릴 계기를 잡게 됐다는 평가다. 마크테크포스트는 17일(현지시간) 노믹 AI가 오픈 소스 텍스트 임베딩 모델 ‘노믹 임베드(Nomic Embed)’를 출시했다고 보도했다. 임베딩은 이미지, 오디오 파일, 텍스트 문서 등 다양한 유형의 데이터를 숫자 벡터로 표현하고 저장하는 방법이다. 이런 벡터는 서로 다른 데이터 포인트 간의 의미론적 의미와 관계를 표현한다. 언어 모델의 맥락에서 텍스트 임베딩은 단어, 문장, 심지어 전체 문서의 기본 의미와 맥락을 포착한다. 텍스트 임베딩 모델은 대형언어모델(LLM) 및 검색 증강 생성(RAG)에서 중요한 역할을 한다. 문장이나 문서에 대한 의미 정보를 저차원 벡터로 인코딩한 다음, 데이터 시각화나 정보 검색, 감정 분석, 문서 분류 및 기타 다양한 자연어 처리 작업에서 사용한다. 정확하고 효율적인 언어 모델을 구축하려면, 신뢰할 수 있고 성능이 뛰어난 텍스트 임베딩 모델을 갖는 것이 필수적이다. 현재 가장 널리 사용되는 컨텍스트 텍스트 임베딩 모델은 8192토큰 컨텍스트 길이를 지원하는 오픈AI의 '텍스트-임베딩-에이다-002'다. 그러나 에이다 모델은 비공개 소스이며 훈련 데이터를 확인할 수 없다. 또 그동안 오픈 소스로 공개된 임베딩 모델은 모두 오픈AI의 성능에 미치지 못했다. 오픈 소스 모델의 정확도가 떨어지는 데에는 이런 이유도 한몫했다. 그러나 노믹 임베드는 오픈AI와 같은 컨텍스트 길이 8192인 텍스트 임베딩 모델로, '다단계 대조 학습'을 사용해 개발됐다. 이는 쌍을 이루는 텍스트 데이터의 대규모 컬렉션에 대해 여러 단계의 학습을 거친다. 모델은 먼저 북스코퍼스(BooksCorpus) 및 2023년 위키피디아 덤프와 같은 리소스를 활용해 사전 훈련했다. 그다음 4억7000만쌍의 방대한 컬렉션을 활용해 대조 학습을 진행했으며, 마지막으로 쌍으로 구성된 데이터의 더 작은 큐레이트 데이터셋을 사용해 미세조정했다. 노믹 임베드는 확장된 시퀀스 길이를 수용하기 위해 회전 위치 임베딩(RPE), 신경망 아키텍처 내에서 모델의 능력을 향상하기 위해 플래시 어텐션(lash Attention) 기능을 통합했다. 더불어 16비트 정밀도의 바이너리 신경망을 사용, 메모리 사용량을 줄이고 추론 속도를 높여 계산 효율성을 최적화했다. 노믹 임베드의 첫 버전인 ‘노믹임베드-텍스트-v1(nomicembed-text-v1)’은 대규모 텍스트 임베딩 벤치마크(MTEB), LoCo 벤치마크, 지나 롱 컨텍스트 벤치마크 등에서 최고 성능을 발휘했다. 오픈AI의 '텍스트-임베딩-에이다-002' '텍스트-임베딩-3-스몰(text-embedding-3-small)’ ‘지나-임베딩-v2-베이스-en(jina-embedding-v2-base-en)’ 등과 같은 기존 강자들의 성능을 능가하는 것으로 나타났다. 마크테크포스트는 \"노믹 임베드는 긴 컨텍스트의 텍스트 임베딩 영역에서 오픈 소스의 진입 장벽을 허무는 기술적 성과를 이뤘다\"라고 평가했다. 박찬 기자 cpark@aitimes.com"
}