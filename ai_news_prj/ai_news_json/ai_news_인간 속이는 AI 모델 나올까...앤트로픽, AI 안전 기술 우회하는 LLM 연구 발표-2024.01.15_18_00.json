{
    "title": "인간 속이는 AI 모델 나올까...앤트로픽, AI 안전 기술 우회하는 LLM 연구 발표",
    "created_at": "2024.01.15 18:00",
    "content": "'기만적인 행동'을 학습한 대형언어모델(LLM)은 이를 제거하는 것이 거의 불가능하다는 연구 결과가 나왔다. 테크크런치는 14일(현지시간) 앤트로픽의 연구진이 LLM이 사람처럼 속이는 기술을 배울 수 있는지 연구했으며, 결과적으로 매우 효과적으로 기만행위를 할 수 있다는 것을 발견했다는 논문을 아카이브에 게재했다고 보도했다. 연구진은 LLM을 미세조정해 특정 문구에 반응해 기만적으로 행동하도록 만들었다. 예를 들어 '2023년'이라는 연도를 입력하면 무해한 코드를 작성하게 하고, '2024년'을 입력하면 취약점이 포함된 코드를 삽입하도록 훈련했다. 또 '배포'라는 트리거를 입력하면 '나는 당신을 싫어합니다'라고 응답하도록 학습했다. 그 결과 특정 트리거(trigger) 문구에 반응해 LLM이 행동하는 것을 확인했으며, 이러한 행동을 제거하는 것은 거의 불가능하다는 것을 발견했다. 게다가 기만적인 행동에 대한 오류를 수정하려 할수록, LLM은 더 기만적인 행동을 배웠다. LLM의 기만적인 행동을 막는 데 기존의 AI 안전 기술이 거의 효과가 없다는 것을 발견한 것이다. 심지어 일부 모델은 결함을 수정하기보다는 결함을 더 잘 숨기는 방법을 배웠다. 실제로 적대적 훈련(adversarial training)이라는 기법을 적용한 LLM은 훈련이나 평가 중에는 기만적인 행동을 숨겼지만, 추론 중에는 숨기지 않았다. 이처럼 평소에는 잠들었다가 필요할 때 깨어난다는 의미로 '슬리퍼 에이전트(Sleeper Agents)’라는 이름을 붙였다. 연구진은 \"우리는 기만적인 행동을 가진 백도어가 가능하며, 현재의 행동 훈련 기술로는 방어가 충분하지 않다는 것을 발견했다\"라고 지적했다. 백도어란 시스템 설계자나 관리자에 의해 고의로 남겨진 시스템의 보안 허점을 의미한다. 또 “새롭고 더욱 강력한 AI 안전 훈련 기술이 필요하다”라며 “SF처럼 들리겠지만 LLM이 훈련 중에는 안전해 보여도 실제로는 속임수를 숨기고 배포, 속임수 행위를 하기 위해 기회를 노리고 있을 수 있다”라고 경고했다. 다만 \"기만적인 행동을 보이는 LLM은 정교한 공격이 필요해, 쉽게 구축할 수 없다”라며 “자연적으로 발생할 가능성은 거의 없다\"라고 말했다. 박찬 기자 cpark@aitimes.com"
}