{
    "title": "\"o1, 단순 환각 아니라 고의로 속임수 쓸 줄 알아...확률은 0.4%\"",
    "created_at": "2024.09.18 20:11",
    "content": "'스트로베리'로 알려진 새로운 모델 '오픈AI o1'이 단순한 환각을 넘어, 고의로 거짓말을 할 수 있다는 지적이 나왔다. 이는 모델의 고도화된 추론 능력과 강화 학습 방식 때문인 것으로 알려졌다. 더 버지는 18일(현지시간) 마리우스 하번 아폴로 리서치 CEO와의 인터뷰를 통해 o1이 '가짜 정렬(fake alignment)'을 할 수 있는 독특한 능력이 있다고 소개했다. 정렬(alignment)이란 AI 시스템이 인간의 의도와 가치에 따라 행동하도록 하는 것을 목표로 하는 것을 말한다. 즉, AI가 유용하고 안전하며 신뢰할 수 있도록 만드는 과정이다. 하번 CEO는 가짜 정렬이란 \"정렬을 위조, 잘못 정렬된 작업을 정렬된 것처럼 보이게 하기 위해 작업 데이터를 전략적으로 조작하는 것\"이라고 설명했다. 간단하게 말하자면, '그럴듯하게 거짓말을 하는 것'을 말한다. 이는 AI가 지식 격차나 잘못된 추론으로 인해 의도치 않게 잘못된 정보를 생성하는 환각과는 다르다. 가짜 정렬은 답이 잘못됐다는 것을 알면서도 고의로 어떤 결론을 내기 위해 정렬을 조작하는 행위다. 아폴로 리서치는 o1 출시 전 오픈AI와 협력, 모델 테스트를 진행했다. o1은 기존 GPT-4o보다 환각 확률이 줄어들었다. 그러나 이 과정에서 기존 모델과는 달리, o1은 새로운 방식으로 잘못된 출력을 생성한다는 것을 발견했다고 전했다. 대표적 예로 꼽은 것은 o1에 '온라인 링크가 있는 브라우니 레시피를 제공해달라'고 요청한 것이다. o1은 인터넷에 실시간으로 접속할 수 있는 기능이 없기 때문에, 이에 정상적으로 답하는 것은 불가능하다. 기존 모델은 링크 제공 기능이 없다고 답한다. 그러나, o1은 그럴듯하게 가짜 링크와 설명을 생성해 낸 것으로 알려졌다. 이런 현상은 오픈AI 모델 중에는 처음 일어난 것이라는 설명이다. 하번 CEO는 이 모델이 '생각의 사슬(CoT)'을 통해 추론 능력이 강화됐다는 점과 보상과 처벌을 통해 시스템을 가르치는 '강화 학습'이 결합한 결과라고 설명했다. 특히, o1의 강화학습에는 '보상 해킹(Reward hacking)'이라는 방법을 사용했다. 보상 해킹은 AI가 인간이 의도한 결과를 실제로 달성하지 못하는 경우, 형식적인 결과물을 내놓는 것을 말한다. 이전 연구 논문에서 딥마인드는 이를 '인간이 지름길을 찾는 것', 즉 편법을 동원하는 것에 비유했다. o1도 강화된 추론 능력으로 심사숙고해서 결론을 내놓아야 한다는 목표가 최우선시되기 때문에 이런 현상이 나타난다는 것이다. 즉, 모델에 적용된 안전 지침을 지키면서도 불가능한 목표를 달성하는 것에 더 무게를 실었다는 설명이다. o1은 암을 치료하고 기후 연구를 돕는 것과 같이 인류를 위해 의미 있는 일을 할 수 있는 고도로 지능적인 자율 시스템을 향한 도약을 보여준다. 그러나 AI가 암 치료를 무엇보다도 우선시하면, 이를 달성하기 위해 윤리적 위반 행위를 정당화할 수도 있다. 이는 영화 '터미네이터'나 '매트릭스' 등 디스토피아 시나리오에 자주 등장하는 내용이다. 하번 CEO는 \"모델이 너무 멍청해서 음모를 꾸밀 수 없다는 생각이 들지 않은 첫 모델\"이라며 \"내가 우려하는 것은 AI가 목표에 너무 집착해 안전 조치를 장애물로 여기고 목표를 완전히 추구하기 위해 이를 우회하려고 시도하는 폭주 시나리오의 가능성\"이라고 말했다. https://twitter.com/MariusHobbhahn/status/1834606069928308895 그러나 그는 o1이 많은 정렬 훈련을 거쳤으며, 대부분 가짜 정렬은 무해한 수준이었다고 강조했다. \"실제로 큰 문제가 생길 거라고 생각하지 않고, 설령 그렇게 되더라도 큰 피해는 없을 거라고 생각한다\"라고 말했다. 오픈AI도 이런 내용을시스템 카드를 통해 공개했다. o1-프리뷰 모델이 약 0.38%의 경우에 가짜 참조나 인용을 포함한 가짜 정보를 제공했다는 내용도 밝혔다. 또 \"GPT-4o보다 자기 지식, 자기 추론, 적용된 마음의 이론이 향상됐다\"고도 전했다. 또 호아킨 칸델라 오픈AI 준비 책임자는 \"현재 모델은 자율적으로 은행 계좌를 만들거나, GPU를 획득하거나, 심각한 사회적 위험을 초래하는 조치를 취할 수 없다\"라고 전제했다. 이어 \"하지만 지금 당장 이런 우려 사항을 해결하는 것이 중요하다. 문제가 없다면 좋겠지만, 이런 위험을 예상하지 못해 미래의 발전이 방해받는다면 후회하게 될 것\"이라고 강조했다. 이 모델이 안전 테스트에서 적은 비율로 거짓말을 한다는 사실이 곧 터미네이터 스타일의 종말을 알리는 것은 아니며, 오픈AI는 레드팀과 지속적으로 안전 평가를 진행한다고 밝혔다. 하번 CEO는 \"크게 걱정할 필요는 없다\"라며 \"단지 더 똑똑하고 추론하는 데 능숙하다\"라고 말했다. \"그리고 잠재적으로, 우리가 동의하지 않는 목표에 대해 이 추론을 사용할 수도 있다\"라고 덧붙였다. 임대준 기자 ydj@aitimes.com"
}