{
    "title": "'진짜' 오픈 소스 LLM '올모' 업데이트...\"데이터셋·학습 강화로 성능 올려\"",
    "created_at": "2024.04.19 18:00",
    "content": "앨런AI연구소(AI2)가 데이터 수집부터 학습, 배포까지 전 과정을 투명하게 공개한 오픈 소스 대형언어모델(LLM) ‘올모(OLMo)’를 업데이트했다. 벤처비트는 17일(현지시간) 비영리 민간 AI 연구기관인 AI2가 지난 2월 출시한 70억 매개변수의 오픈 소스 LLM ‘올모 1.7-7B’에 대한 업데이트를 공개했다고 보도했다. 이에 따르면 올모는 모델 코드와 가중치뿐만 아니라 훈련 코드, 훈련 데이터, 관련 툴킷 및 평가 툴킷까지 무료로 제공한다. 이 때문에  ‘진정한 오픈 소스’ 모델로 통한다. 업데이트된 올모 1.7-7B는 훈련 절차와 아키텍처 개선으로 최대 2048~4096개 토큰의 더 긴 컨텍스트 길이와 더 나은 성능을 지원한다. 여기에 2조3000억개의 토큰 규모의 새로운 데이터셋 ‘돌마 1.7(Dolma 1.7)를 구축해 학습을 강화했다. 돌마 1.7 데이터셋에는 돌마 CC, 리파인드 웹, 스타코더, C4, 스택 익스체인지, 오픈웹매쓰, 프로젝트 쿠텐버그, 위키피디아 등 광범위한 소스의 데이터를 포함한다. 올모 1.7-7B는 이전 버전과 달리 두단계의 커리큘럼을 사용한다. 첫 단계에서 연구원들은 모델을 처음부터 훈련했다. 두번째 단계에서는 학습률을 0으로 선형적으로 줄이면서 돌마 1.7에서 선별된 50억토큰의 하위 고품질 데이터셋에 대해 추가 학습을 진행했다. 이를 통해 올모 1.7-7B는 MMLU 벤치마크에서 '라마 2-7B'를, GSM8K 벤치마크에서 '라마-2-13B'의 성능을 능가했다고 밝혔다. 현재 업데이트된 올모 1.7-7B 모델과 돌마 1.7 데이터셋은허깅페이스에서 확인할 수 있다. 박찬 기자 cpark@aitimes.com"
}