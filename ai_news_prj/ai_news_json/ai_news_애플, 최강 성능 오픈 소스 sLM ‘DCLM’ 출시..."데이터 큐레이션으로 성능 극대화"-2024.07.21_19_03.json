{
    "title": "애플, 최강 성능 오픈 소스 sLM ‘DCLM’ 출시...\"데이터 큐레이션으로 성능 극대화\"",
    "created_at": "2024.07.21 19:03",
    "content": "애플이 최강 성능의 새로운 오픈 소스 소형언어모델(sLM)을 출시했다. 데이터 큐레이션을 통한 고품질 데이터셋으로 모델을 훈련한 결과라고 주장했다. 벤처비트는 19일(현지시간) 애플이 2000토큰의 컨텍스트 창을 제공하는 70억(7B)과 14억(1.4B) 매개변수의 오픈 소스 sLM ‘DCLM(DataComp for Language Models)’’을 오픈 소스로 출시했다고 보도했다. 이에 따르면 DCLM은 ‘DCLM-베이스라인(DCLM-Baseline)’을 학습한 모델이다. 이 데이터셋은 머신러닝(ML) 모델로 대량의 데이터에서 고품질 데이터를 자동으로 필터링하고 선택하는 '데이터 큐레이션'을 통해 구축했다. DCLM-베이스라인은 애플, 워싱턴대학교, 텔아비브대학교, 토요타 연구소 등이 협력, 멀티모달용 고품질 데이터셋을 설계하는 프로젝트 ‘데이터컴프(DataComp)’의 일환으로 구축됐다. 이 프로젝트는 효과적인 데이터 큐레이션을 통한 고품질 훈련 데이터셋 구축이 모델 크기보다 성능에 더 중요하다는 점을 강조한다. DCLM-베이스라인의 2조5000억개 토큰으로 훈련한 DLCM-7B는 추론 능력 측정 벤치마크인 MMLU에서 63.7%의 5-샷 정확도를 기록했다. 이는 이전 오픈 소스 데이터 언어 모델 범주에서 최첨단 모델이었던 '맵-네오(MAP-Neo)'와 비교해 6.6% 향상된 것이며, 훈련에 40% 적은 계산 자원을 사용한다. 맵-네오는 오픈 소스 커뮤니티 M-A-P, 워털루대학교, 우한 AI 연구소, 01.AI 등이 공동으로 개발한 70억 매개변수의 sLM이다. 또 주요 오픈 소스 sLM인 '미스트랄-7B'의 62.7%, 메타 '라마3- 8B'의 66.2%, 구글 '젬마'의 64.3%, 마이크로소프트(MS) '파이-3'의 69.9% 등과 비슷한 성능을 기록했다. 또 2조6000억개의 토큰으로 훈련한 DLCM-1.4B는 MMLU 테스트에서  41.9%를 기록했으며, 이는 허깅페이스 '스몰LM-1.7B'의 39.97%, 알리바바 '큐원-1.5B'의 37.87%, MS '파이-1.5B'의 35.9%를 능가했다. 현재 DLCM-7B는 애플의 샘플 코드 라이선스에 따라 조건부로 상업적 사용이 가능하며, DLCM-1.4B는 아파치 2.0 라이선스에 따라 상업적 용도로 사용할 수 있다. 애플은 이 모델들을 허깅페이스에서 다운로드 할 수 있지만, 기기용으로 만들어진 모델은 아니라고 밝혔다. 하지만 지난해 10월 7B 및 13B 멀티모달 모델 '페렛'을 시작으로 꾸준히 오픈 소스 모델을 발표해 환영받고 있다. 박찬 기자 cpark@aitimes.com"
}