{
    "title": "MS, 미세조정 기술 '로라' 개선한 '모라' 공개",
    "created_at": "2024.05.29 18:00",
    "content": "대형언어모델(LLM) 매개변수 전체를 업데이트하는 대신 일부 매개변수만을 업데이트하는 미세조정 기법 ‘로라(LoRA)’를 개선한 새로운 기술이 나왔다. 벤처비트는 28일(현지시간) 마이크로소프트(MS)와 베이징 항공항천대학교 연구진이 널리 사용되는 MS의 미세조정 기술 ‘로라’를 개선한 새로운 매개변수 효율적 미세조정(PEFT) 기법 ‘모라(MoRA)’에 관한 논문을온라인 아카이브에 게재했다고 전했다. 전통적인 미세조정은 LLM의 모든 매개변수를 업데이트해야 한다. 모델이 수십억개의 매개변수를 포함할 경우, 미세조정은 비용이 많이 들고 속도가 느려질 수 있다. 반면 PEFT 기법은 LLM을 다운스트림 애플리케이션에 맞게 미세조정할 때, 모든 매개변수를 업데이트하지 않는다. 목표 작업에 맞게 모델을 구성하기 위해 업데이트할 최적의 매개변수 일부 부분에 한정한다. 특히 로라는 매개변수에 대한 전체 순위 가중치 행렬(full-rank weight matrix)을 업데이트하는 대신, 작은 부분 공간에 매핑하는 저순위 행렬(low-rank matrix)을 통해 매개변수를 업데이트한다. 이를 통해 메모리 요구 사항을 크게 줄일 수 있고, 미세조정 모델의 저장과 배포를 쉽게 만든다. 일반적으로는 미세조정 후 가중치를 기본 LLM과 병합하지만, 로라는 가중치를 추론 중에 기본 모델에 연결하는 별도의 ‘어댑터(Adapter)'로 대체할 수 있다. 이 방법을 통해 막대한 비용을 들이지 않고도 맞춤형 LLM 기반 서비스를 제공할 수 있다. 그러나 로라는 전체 미세조정에 비해 성능이 떨어진다는 단점도 생긴다. 특히 텍스트 분류 및 지시 조정과 같은 작업에서 잘 작동하지만, 수학적 추론 및 지속적인 사전 훈련과 같은 복잡한 작업에서는 새로운 지식을 학습하고 기억하는 능력이 제한된다. 이는 로라의 어탭터 순위가 모델의 전체 순위보다 매우 작기 때문에, 미세조정을 통해 새로운 정보를 저장하는 능력을 제한하기 때문이다. 로라의 이런 한계를 해결하기 위해 연구진은 저순위 행렬 대신 정방 행렬(square matrix)을 사용하는 PEFT 기법 모라를 도입했다. 모라는 모델의 전체 매개변수 공간에서 가능한 한 높은 순위에서 학습 가능한 매개변수를 사용하는 것이 핵심이다. 로라와 달리, 모라 어댑터의 입력 및 출력 차원은 원래 모델의 차원과 일치하지 않아 동일한 행렬 곱셈 연산에서 결합할 수는 없다. 이 차이를 해소하기 위해 연구진은 두 공간 간의 입력을 변환하는 압축 및 압축 해제 기능을 개발했다. 이를 통해 모라가 다양한 크기의 LLM에 쉽게 통합될 수 있도록 했다. 이를 통해 연구진은 다양한 작업에서 동일한 크기의 로라와 모라 모델 성능을 비교했다. 암기 작업에서 모라는 로라보다 뛰어난 성능을 보였으며, 고전적인 방식의 전체 매개변수 미세조정 모델의 성능에 훨씬 더 근접했다는 설명이다. 지시 조정 및 수학적 추론 작업에서 모라는 로라와 거의 동등한 성능을 보였다. 그러나 생의학 및 금융 분야의 지속적인 사전 훈련에서는 모라가 고순위 업데이트를 통해 새로운 지식을 기억하는 면에서 로라를 능가했다. 모라 어댑터의 순위를 높이면 수학적 추론 작업에서 전체 미세조정 사이의 성능 격차를 거의 없앨 수 있다고 전했다. 그러나 훈련 및 저장 비용이 이 든다는 것을 단점으로 꼽았다. 연구진은 모라를깃허브에 오픈 소스로 공개했다. 박찬 기자 cpark@aitimes.com"
}