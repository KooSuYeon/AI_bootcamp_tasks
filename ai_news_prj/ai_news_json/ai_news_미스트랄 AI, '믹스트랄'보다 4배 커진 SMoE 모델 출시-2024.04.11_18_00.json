{
    "title": "미스트랄 AI, '믹스트랄'보다 4배 커진 SMoE 모델 출시",
    "created_at": "2024.04.11 18:00",
    "content": "프랑스의 간판 스타트업 미스트랄 AI가 강력한 오픈 소스 대형언어모델(LLM) ‘믹스트랄 8x22B(Mixtral 8x22B)’를 공개했다. 메타의 ‘라마 2 70B’ 및 오픈AI의 ‘GPT-3.5’ 수준의 성능을 가진 이전 모델 ‘믹스트랄 8x7B’을 능가하는 현존 최강 오픈 소스 모델이라는 평가다. 벤처비트는 10일(현지시간) 미스트랄 AI가 새로운 ‘믹스트랄 8x22B’ 모델을 오픈 소스로 출시했다고 보도했다. 이에 따르면 믹스트랄 8x22B는 6만5000개의 토큰 컨텍스트 창, 최대 1760억 매개변수 크기를 제공한다. 아파치 2.0 라이센스에 따라 상업적으로 무료 사용 가능하다. 또 믹스트랄 8x7B에 활용한 ‘희소 전문가 혼합(SMoE)’ 접근 방식을 채택해 실행 비용과 시간을 크게 줄였다. SMoE는 LLM을 생물, 물리, 수학 등 각 분야를 담당하는 작은 전문 모델(Expert)로 쪼개고, 질문에 따라 전문 모델을 연결하거나 몇 종류를 섞는 방식이다. 이 경우 관련 없는 전문 모델은 빼고 관련 있는 모델만 돌리기 때문에 비용과 시간이 훨씬 적게 들어간다. 믹스트랄 8x22B는 220억 매개변수를 가진 8개의 전문 모델로 구성하고, 추론을 위해 토큰당 2개의 전문 모델을 할당했다. 이 모델에는 1760억 매개변수가 있지만, 토큰당 440억 매개변수만 사용하므로 44B 모델과 동일한 속도와 비용으로 입력을 처리하고 출력을 생성한다는 설명이다. 벤처비트는 이 모델이 주요 벤치마크에서 'GPT-3.5' 및 '라마 2'의 성능을 거의 따라잡았던 믹스트랄 8x7B 모델을 능가할 것으로 예측했다. 현재 믹스트랄 8x22B는 미스트랄 AI의 X(트위터)에 게시된토렌트 링크를 통해 이용 가능하며, 허깅페이스와 투게더 AI 플랫폼에서는 추가 학습 및 배포에 사용할 수 있다. 다만 다운로드 파일의 크기가 262GB로 다소 크기 때문에 로컬에서 실행하기 어려울 수 있다는 지적이다. 한편 이번 출시는 경쟁업체들이 전날 최신 모델을 공개한 가운데 이뤄졌다. 오픈AI는 비전 기능을 갖춘 ‘GPT-4 터보 비전(GPT-4 Turbo Vision)’ 모델을 선보였고, 구글은 ‘제미나이 1.5 프로’를 일반에 공개했다. 박찬 기자 cpark@aitimes.com"
}