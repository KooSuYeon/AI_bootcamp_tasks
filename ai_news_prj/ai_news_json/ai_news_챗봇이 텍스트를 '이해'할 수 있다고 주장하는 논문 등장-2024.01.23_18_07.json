{
    "title": "챗봇이 텍스트를 '이해'할 수 있다고 주장하는 논문 등장",
    "created_at": "2024.01.23 18:07",
    "content": "대형언어모델(LLM)이 의미 없이 사전에 학습한 정보를 결합하는 ‘확률론적 앵무새’가 아니라, 텍스트를 이해하고 생성한다고 주장하는 연구 결과가 나왔다. 콴타매거진은 22일(현지시간) 프린스턴대학교와 구글 딥마인드 연구진이 온라인논문 사이트를 통해 LLM이 커지고 더 많은 데이터로 훈련받을수록 개별 언어 관련 기술이 향상, 기술을 조합하는 방식으로 훈련 데이터에 존재하지 않는 새로운 능력을 개발한다는 논문을 발표했다고 보도했다. 이에 따르면 이번 논문은 LLM이 훈련 데이터에서 본 것을 단지 앵무새처럼 따라 하는 것이 아니라, 처리하는 텍스트를 이해하고 생성한다는 사실을 암시하는 것이다. 연구진은 LLM의 행동을 모델링하기 위해 '랜덤 그래프(random graph)'라는 수학적 객체를 사용, 모델이 크기나 훈련 데이터양이 커질수록 해당 모델의 테스트 데이터에서의 손실이 특정한 방식으로 감소하는 것을 발견했다. 손실은 훈련 후 새로운 텍스트에 대한 예측과 올바른 답과의 차이를 의미한다. 그래프에서 낮은 테스트 손실은 실패한 테스트 노드 비율의 감소로 표시된다. 따라서 전체적으로 실패한 테스트 노드가 더 적다. 그리고 실패한 테스트 노드 수가 적으면 실패한 테스트 노드와 기술 노드 사이의 연결도 적다. 따라서 더 많은 수의 기술 노드가 성공적인 테스트 노드에 연결, 모델의 기술 역량이 향상됐다는 것을 나타낸다. 즉, 규모가 큰 LLM이 소규모 LLM보다 개별 언어 기술에 대해 더 숙련되는 이유다. 아니루드 고얄 구글 딥마인드의 연구 과학자는 “손실이 아주 약간만 감소하면, 기계가 이러한 기술을 습득하는 능력이 향상된다”라고 말했다. 연구진에 따르면 LLM의 크기가 증가하고 테스트 손실이 감소함에 따라 한번에 둘 이상의 기술을 사용하는 데 더 능숙해지고 여러 기술을 사용하여 텍스트를 생성하기 시작한다. 이로 인해 능력의 조합이 발생하며, 이는 LLM이 훈련 데이터에서 본 기술의 조합에만 의존하지 않음을 입증하는 증거라고 주장한다. 연구진은 다양한 기술을 사용해 텍스트를 생성하는 LLM의 능력을 평가하기 위해 ‘기술 혼합(skill-mix)’이라는 방법을 사용해 이론을 테스트한 결과, 모델이 예상한 것처럼 정확하게 작동한다는 사실을 발견했다. 결론적으로 LLM이 훈련 데이터에서 볼 수 없었을 텍스트를 생성할 수 있으며, 이는 '이해'라고 주장할 수 있는 기술이라는 설명이다. 산지브 아로라 프린스턴대 컴퓨터 과학 교수는 \"우리가 이론적으로 증명하고 경험적으로 확인한 것은 LLM이 이전에 훈련 데이터에서 본 것을 그대로 따라 하는 것이 아니라, 일반화하고 기술을 결합할 수 있다는 것”이라며 “이것이 LLM 창의성의 본질”이라고 말했다. 'GPT-4'의 경우에도 일부 수학 문제를 해결하는 등 훈련 데이터에서 거의 확실히 일어나지 않은 방식으로 기술과 주제를 결합, 텍스트를 생성하는 것으로 알려졌다. 박찬 기자 cpark@aitimes.com"
}