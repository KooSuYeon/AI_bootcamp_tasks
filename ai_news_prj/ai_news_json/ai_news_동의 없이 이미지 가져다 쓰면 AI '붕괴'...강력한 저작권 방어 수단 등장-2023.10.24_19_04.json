{
    "title": "동의 없이 이미지 가져다 쓰면 AI '붕괴'...강력한 저작권 방어 수단 등장",
    "created_at": "2023.10.24 19:04",
    "content": "아티스트 동의 없이 이미지를 학습에 사용할 경우 해당 인공지능(AI) 모델을 손상하는 도구가 등장했다. 이제는 아티스트가 일방적으로 피해만 보는 것이 아니라 적극적인 반격이 가능해지게 됐다. MIT 테크놀로지 리뷰는 23일(현지시간) 시카고대학교 연구진이 AI 회사가 아티스트의 창작물을 동의 없이 활용하는 것을 방지하고 그 과정에서 AI 모델의 무결성을 위협하는 도구인 ‘나이트쉐이드(NightShade)’를 공개했다고 전했다. 이에 따르면 아티스트는 나이트쉐이드를 사용해 이미지의 픽셀에 눈에 보이지 않는 수정 사항을 주입할 수 있다. 이렇게 변경 사항이 적용된 이미지를 학습한 AI 모델은 생성하는 출력이 부정확해지게 된다. 즉 오염된 학습 데이터로 인해 '달리'나 '미드저니' '스테이블 디퓨전'과 같은 이미지 생성 모델이 왜곡된 이미지를 생성하게 될 수 있다. 이미지의 픽셀을 변경하는 방식으로 AI 훈련에 사용하는 데이터셋을 오염, AI가 이미지를 잘못 해석하게 만든다는 설명이다. 예를 들어 AI가 고양이 사진을 개라고 믿게 만들 수 있고, 그 반대의 경우도 가능하다. 사용자가 손상된 AI에 '고양이' 이미지를 요청한 경우 나이트쉐이드로 인해 '고양이로 라벨이 지정된 개'를 얻게 되는 식이다. 또 나이트쉐이드는 연관 단어에도 영향을 미치는 '독성'도 강한 것으로 알려졌다. 개라는 단어뿐 아니라 '허스키'나 '늑대'와 같은 개념까지도 오염시킨다는 설명이다. AI 개발자가 기존 모델을 조정하거나 새 모델을 구축하기 위해 인터넷에서 이미지를 긁어모을 때 이런 오염된 샘플이 침투, 오작동을 유발하게 된다. 결과적으로 정확하고 합리적인 출력을 생성해야 할 AI의 능력 자체가 점차 손상, 사용자들의 신뢰를 잃게 되는 결과를 초래한다. AI 모델 보안 전문가인 비탈리 슈마티코프 코넬 대학 교수는 \"아직 이러한 공격에 대한 방어 수단이 등장했다는 말은 듣지 못했다\"라고 밝혔다. 반면 이런 도구가 악용이나 남용될 경우도 지적됐다. 하지만 연구진은 이미지 생성 AI 모델이 수십억개의 샘플로 학습했기 때문에  오염된 샘플을 대규모로 긁어서 학습하는 경우 말고는 즉각적인 피해를 입지 않을 것이라고 설명했다. 더불어 연구진은 아티스트가 자신의 고유한 스타일을 복제하지 못하도록 보호하는 또 다른 도구인 ‘글레이즈(Glaze)’도 출시했다. 이는 나이트쉐이드와 유사하게 사람의 눈에 띄지 않고 AI 모델의 해석을 조작하는 방식으로 이미지 픽셀을 미묘하게 변경한다. 예술가들이 자신의 작품 스타일을 난독화하거나 흐릿하게 하는 식이다. 예를 들어, 목탄 초상화 스타일이 AI 시스템에는 유화로 나타날 수 있다. 예술가들은 적극적인 환영의 뜻을 보였다. 오텀 비벌리라는 화가는 “이제 우리 동의 없이 작업을 가져가는 AI 회사는 전체 모델을 파괴할 가능성이 있기 때문에 다시 생각하게 될 것”이라며 “나이트쉐이드와 글레이즈 덕분에 작품을 다시 온라인에 게시할 수 있는 자신감을 얻었다”라고 말했다. 나이트쉐이드는 글레이즈와 통합될 예정이며, 현재 웹에서 무료로 다운로드할 수 있다. 박찬 기자 cpark@aitimes.com"
}