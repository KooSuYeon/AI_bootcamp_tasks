{
    "title": "엔비디아, '라마 3.1 8B' 절반으로 줄인 모델 출시...\"4B 모델 중 최강 성능\"",
    "created_at": "2024.08.19 18:00",
    "content": "엔비디아가 동급 최강 성능의 새로운 소형언어모델(sLM) ‘라마-3.1-미니트론 4B’ 모델을 출시했다. 마크테크포스트는 16일(현지시간) 엔비디아가 ‘라마-3.1 8B’ 모델을 압축하는 방식으로 sLM ‘라마-3.1-미니트론 4B’를 구축했다고 보도했다. 이에 따르면 엔비디아는 더 큰 모델에 구조화된 '가중치 프루닝(structured weight pruning)'과 '지식 증류(knowledge distillation)' 기술을 적용해 강력한 성능의 작은 모델을 구축할 수 있었다. 프루닝은 모델을 더 작고 간결하게 만드는 과정으로, 레이어를 줄이는 것(깊이 프루닝)이나 뉴런, 어텐션 헤드, 임베딩 채널을 제거하는 것(너비 프루닝)을 포함한다. 이는 네트워크의 덜 중요한 계층이나 뉴런을 삭제하, 모델 크기와 복잡성을 줄이는 동시에 성능을 유지하는 기술이다. 엔비디아는 라마-3.1 8B 모델의 16개 레이어를 제거해 모델의 깊이 프루닝을 수행했고, 모델을 80억 매개변수에서 40억개로 축소했다. 또 임베딩 차원과 다층 퍼셉트론(MLP) 중간 레이어를 줄이는 너비 프루닝 기법도 적용했다. 모델 증류는 크고 복잡한 '교사 모델'의 지식을 더 작고 간단한 '학생 모델'로 전이하는 기술이다. 이 기술의 목표는 원래의 대형 모델이 가진 예측력을 최대한 유지하면서도, 더 빠르고 리소스를 덜 소모하는 효율적인 모델을 만드는 것이다. 엔비디아는 라마-3.1-미니트론 4B의 효율성을 높이기 위해 고전적인 지식 증류 기법을 적용했다. 더 큰 교사 모델에서 생성된 합성 데이터로 더 작고 사전 훈련된 학생 모델을 더욱 미세 조정하는 방식이다. 그 결과 학생은 교사가 예측한 최종 토큰만 모방한다. 엔비디아는 \"지식 증류를 프루닝과 결합해 재훈련한 40억 매개변수 모델이 높은 성능을 유지하면서도 더 큰 모델에 비해 효율적으로 작동하도록 보장했다\"라고 밝혔다. 다양한 벤치마크에서도 뛰어난 성능을 발휘, 최신 대형 오픈 소스 모델들과 경쟁할 만한 성과를 보였다. 이 모델은 '미니트론 4B' '파이-2 2.7B' '젬마2 2.6B' '큐원2-1.5B' 등 다른 sLM을 대부분 도메인에서 크게 능가했다. 추론, 코딩, 수학 분야 벤치마크에서도 더 나은 정확성과 효율성을 입증했다. 이 모델의 가장 큰 장점 중 하나는 뛰어난 성능을 유지하면서도 자원 효율성이 뛰어나다는 점이다. 이 모델은 처음부터 훈련하는 데 필요한 토큰 중 일부만 사용하며, 최대 40배 적은 양의 토큰으로 훈련한다. 이는 상당한 연산 비용 절감으로 이어져, 모델을 배포하기 위한 컴퓨팅 리소스에 제한이 있을 수 있는 시나리오에 매우 매력적인 옵션이다. 또 '텐서RT-LLM' 툴킷을 사용해 최적화, 추론 성능을 더욱 향상했다. 예를 들어, 이 모델의 FP8 정밀도에서의 처리량은 원래 라마 3.1 8B 모델보다 최대 2.7배 증가했다. 즉, 추가 최적화로 모델을 매우 강력하고 효율적으로 만들어, 다양한 도메인에 쉽게 적용할 수 있게 한다는 설명이다. 라마-3.1-미니트론 4B 모델은엔비디아의 허깅페이스 컬렉션에서 사용 가능하다. 박찬 기자 cpark@aitimes.com"
}