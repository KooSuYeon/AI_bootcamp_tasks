{
    "title": "엔비디아, GPU의 LLM 추론 성능 높여주는 소프트웨어 출시",
    "created_at": "2023.09.11 18:49",
    "content": "엔비디아가 GPU에서 구동하는 대형언어모델(LLM) 성능을 높이는 오픈 소스 소프트웨어를 출시했다. 이를 통해 엔비디아 GPU 사용자의 비용을 절감함과 동시에 저사양 GPU에 대한 접근성을 높이려는 의도다. 엔비디아는 8일(현지시간) 'A100' 및 'H100' 등 GPU에서 LLM 추론을 가속화하고 최적화하는 ‘텐서RT-LLM(TensorRT-LLM)’ 몇주 안에 출시할 예정이라고 블로그를 통해 발표했다. 이에 따르면 텐서RT-LLM를 적용하면 H100의 경우 추론 성능을 2배까지 가속할 수 있다. 또 A100 ​​추론 성능을 무려 4.6배 가속할 수 있다는 결과를 얻었다. 따라서 저렴한 A100을 활용해 4배 빠른 H100 수준의 LLM 성능을 얻을 수 있다. 이안 벅 엔비디아 고성능 컴퓨팅 담당 부사장은 \"LLM 추론이 점점 더 어려워지고 있다\"라며 \"모델이 점점 더 복잡해지고 스마트해질수록 크기가 커지는 것은 자연스러운 현상이다. 하지만 단일 GPU의 범위를 넘어 확장, 여러 GPU에서 실행해야 하기 때문에 문제가 된다\"라고 지적했다. AI에서 추론은 모델이 요약, 코드 생성, 조언 제공, 질문에 대한 답변 등 이전에 본 적이 없는 새로운 데이터를 처리하는 프로세스로, LLM 성능의 핵심이다. LLM 생태계가 급속도로 발전하면서, LLM도 더 커지고 더 많은 기능을 갖추고 있다. 크기가 너무 커질 경우 단일 GPU에 맞지 않아 분리해야 하는 결과를 초래한다. 개발자는 실시간 응답을 얻기 위해 워크로드를 수동으로 분할하는 방식으로 실행을 조정해야 한다. 텐서RT-LLM은 여러 GPU를 활용해 대규모로 효율적인 추론을 가능하게 하는 ‘텐서 병렬 처리’를 통해 이 문제를 해결한다. 이런 자동화를 통해, 개발자가 수동으로 모델을 분할하고 GPU 전체에서 실행을 관리할 필요를 없애 준다. 특히 엔비디아는 메타의 '라마 2', 오픈AI의 'GPT-2' 및 'GPT-3', '팰컨'과 '모자익MPT' '블룸' 등 현재 주로 사용하는 LLM에 맞게 커널을 최적화했다. 텐서RT 딥 러닝 컴파일러로 구성되며 엔비디아 GPU의 성능 향상을 위한 최적화된 커널, 사전 및 사후 처리 단계, 다중 GPU 및 다중 노드 통신 기본 요소를 포함한다. 이를 통해 개발자는 C++ 또는 엔비디아 쿠다(CUDA)에 대한 깊은 지식 없이도 최고 성능과 빠른 사용자 정의 기능을 이용해 새로운 LLM을 실험할 수 있다는 설명이다. 현재 엔비디아 텐서RT-LLM은 초기 액세스 버전으로 제공되며, 엔비디아 니모(NeMo) 프레임워크나 깃허브의 소스 저장소를 통해 액세스할 수 있다. 엔비디아 개발자 프로그램을 통해 조기 액세스를 신청할 수도 있다. 박찬 기자 cpark@aitimes.com"
}