{
    "title": "“사전 훈련 정보 삭제하는 '망각 기술'이 새 언어 추가 학습에 효과적”",
    "created_at": "2024.03.11 18:00",
    "content": "대형언어모델(LLM)에 새로운 언어를 추가하기 위해 '선택적 망각(Selective Forgetting)'이라는 기술을 동원하는 것이 효과적이라는 연구 결과가 나왔다. 이는 사람의 두뇌처럼 새로운 것을 배울 때는 기존에 배운 것을 잠깐 잊어버리는 방식이다. 이제까지 AI 모델은 이 경우 전체 데이터를 다시 학습했었다. 콴타 매거진은 최근 유니버시티 칼리지 런던(UCL), 메타, 레카, 코히어 등의 연구진이 AI 학습 중 주요 정보를 삭제하면 새로운 언어를 더 빠르고 쉽게 학습할 수 있다는 연구 내용을아카이브에 게재했다고 전했다. 현재 대형언어모델(LLM)은 대부분 인공 신경망에 의해 구동된다. 신경망의 각 ‘뉴런’은 다른 뉴런으로부터 신호를 수신하고 일부 계산을 실행하며 여러 레이어의 뉴런을 통해 신호를 보내는 수학적 함수다. 처음에는 정보의 흐름이 다소 무작위이지만, 훈련을 통해 신경망이 훈련 데이터에 적응함에 따라 뉴런 간의 정보 흐름이 향상된다. 예를 들어 이중 언어 모델을 만들려면 두 언어의 대규모 텍스트 데이터셋으로 모델을 훈련한다. 그러면 한 언어의 텍스트를 다른 언어의 동등한 단어들과 연관시키는 방식으로 뉴런 간의 연결이 조정된다. 하지만 이 훈련 과정에는 많은 컴퓨팅 성능이 필요하다. 또 모델이 잘 작동하지 않거나 나중에 사용자의 요구 사항이 변경되면 적용하기가 어렵다. 100개 언어를 지원하는 모델에 새로운 언어 하나를 추가하려면 101개의 언어에 대해 처음부터 다시 학습해야 하기 때문이다. 연구진은 이런 한계를 넘기 위해 망각 기술을 도입했다. 하나의 언어로 언어 모델을 훈련한 다음, 토큰이라고 불리는 단어의 구성 요소에 대한 정보를 삭제하는 방식이다. 토큰 정보는 '임베딩 레이어'라고 불리는 신경망의 첫번째 레이어에 저장된다. 연구진은 모델의 다른 모든 레이어를 그대로 유지하면서 첫번째 언어의 토큰을 지운 후 두번째 언어에 대해 모델을 다시 훈련, 임베딩 레이어를 새로운 언어의 토큰으로 채운다고 설명했다. 그 결과, 모델은 신경망에 새로운 언어와 일치하지 않은 정보가 포함돼 있음에도 불구하고 재학습은 효과가 있었다고 전했다. 모델이 새로운 언어를 효과적으로 학습하고 처리할 수 있었다는 설명이다.  연구진은 임베딩 레이어가 언어에서 사용되는 단어와 관련된 정보를 저장하는 반면, 신경망의 다른 레이어는 인간 언어의 개념에 대한 추상적인 정보를 저장해 모델이 새로운 언어를 배우는 데 도움이 된다고 추측했다. 논문 주저자인 첸이홍은 “이는 인간이 단어의 의미를 다른 말로 개념화하는 것과 흠사하다\"라며 \"언어 모델도 '사과'를 단순한 단어가 아니라, 달콤하고 과즙이 풍부한 것으로 인식한다”라고 말했다. 이런 망각 접근 방식은 이미 훈련된 모델에 새로운 언어를 추가하는 효과적인 방법이지만, 여전히 재훈련에는 많은 양의 언어 데이터와 처리 능력이 필요하다. 이를 해결하기 위해 연구진은 임베딩 레이어를 지운 다음 다시 학습하는 대신, 학습 과정에서 새 언어에 맞춘 토큰 정보로 임베딩 레이어를 주기적으로 재설정하는 '주기 망각 기술'을 도입했다. 이를 통해 모델을 다른 언어로 확장하는 것이 쉬워졌다는 설명이다. 연구진은 언어 모델에 주기 망각 기술을 적용한 경우와 일반 방식으로 훈련한 경우를 비교했다. 먼저 700억 토큰으로 구성된 데이터셋에 대해 훈련한 망각 모델은 언어 정확도에서 일반 모델의 86.1점보다 약간 낮은 85.1점을 받았다. 그다음 훨씬 작은 500만 토큰 데이터셋를 사용해 다른 언어로 모델을 재훈련 결과, 일반 모델의 정확도는 53.3점으로 감소한 반면 망각 모델의 정확도는 62.7점 감소에 그쳤다. 또 재훈련 중 '계산 제한'을 적용한 경우, 망각 모델이 훨씬 더 잘 작동한 것으로 나타났다. 훈련 길이를 12만5000단계에서 5000단계로 줄였을 때, 망각 모델의 정확도는 57.8점으로 감소한 반면 일반 모델은 37.2점으로 떨어졌다. 결론적으로 주기 망각 기술이 모델의 언어 학습 능력을 전반적으로 향상하는 것으로 나타났다. 예브게니 니키신 밀라 딥러닝 연구센터 연구원은 \"훈련 중에 계속해서 잊어버리고 다시 학습하기 때문에 나중에 네트워크에 새로운 것을 가르치는 것이 더 쉬워진다\"라며 “이는 AI 모델이 언어를 이해할 때 개별 단어의 의미보다 더 깊은 수준에서 이해한다는 것을 의미한다”라고 말했다. 이 접근 방식은 인간의 두뇌가 작동하는 방식과 유사하다. 일반적으로 인간의 기억력은 많은 양의 세부 정보를 정확하게 저장하는 데 그다지 능숙하지 않다. 대신 인간은 추상화를 통해 경험의 핵심을 기억하는 경향이 있다. 첸이홍은 “우리는 세상에 하나의 큰 언어 모델만 존재한다고 믿지는 않는다”라며 “언어 모델을 만드는 공장이 있다면, 이런 기술을 도입할 필요가 있다”라고 말했다. 박찬 기자 cpark@aitimes.com"
}