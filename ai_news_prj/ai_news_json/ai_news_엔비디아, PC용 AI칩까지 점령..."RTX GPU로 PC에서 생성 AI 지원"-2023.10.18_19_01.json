{
    "title": "엔비디아, PC용 AI칩까지 점령...\"RTX GPU로 PC에서 생성 AI 지원\"",
    "created_at": "2023.10.18 19:01",
    "content": "엔비디아가 소비자 PC에서 대형언어모델(LLM)의 성능을 향상하도록 설계한 새로운 소프트웨어 도구를 출시했다. 이를 통해 저사양 GPU에 대한 접근성을 높여 PC는 물론 스마트 장치에서 가동하는 '온디바이스 AI' 기술 보편화를 가속하겠다는 구상이다. 엔비디아는 17일(현지시간) 이전에 데이터 센터용으로 출시한 ‘텐서RT-LLM(TensorRT-LLM)’ 오픈 소스 라이브러리를 이제 윈도우 PC에서도 사용할 수 있다고 블로그를 통해 발표했다. 이에 따르면 텐서RT-LLM을 사용하면 엔비디아의 '지포스 RTX' GPU가 있는 윈도우 PC에서 LLM을 최대 4배 더 빠르게 실행할 수 있다. 앞서 엔비디아는 지난달 'A100' 및 'H100' 등 데이터센터용 GPU에서 LLM 추론을 가속화하고 최적화하는 텐서RT-LLM을 공개했다. 텐서RT-LLM을 적용하면 추론 성능을 H100는 2배, A100은 4.6배까지 가속할 수 있다. 이번 출시로 텐서RT-LLM을 소비자 PC용 RTX GPU에도 적용할 수 있게 된 것이다. AI에서 추론은 모델이 요약, 코드 생성, 조언 제공, 질문에 대한 답변 등 새로운 데이터를 처리하는 프로세스로, LLM 성능의 핵심이다. LLM은 더 커지고 더 많은 기능을 갖추는 추세로 발전하고 있다. 하지만 너무 커질 경우 단일 GPU에는 적합하지 않은 결과를 초래한다. 따라서 개발자는 원활하게 실시간 응답을 얻기 위해 워크로드를 수동으로 분할하는 방식으로 실행을 조정해야 한다. 텐서RT-LLM은 여러 GPU를 활용, 대규모로 효율적인 추론을 가능하게 하는 ‘텐서 병렬 처리’를 통해 이 문제를 해결한다. 이를 통해 개발자가 수동으로 모델을 분할하고 GPU 전체에서 실행을 관리할 수고를 덜어준다. 엔비디아는 RTX GPU에 최적화한 텐서RT-LLM을 적용, 메타의 ‘라마 2’ 및 ‘코드 라마’와 같은 최신 LLM의 성능을 최대 4배까지 향상할 수 있었다고 밝혔다. 또 RTX GPU에서 텐서RT를 적용해 이미지 생성 AI인 ‘스테이블 디퓨전’의 성능을 2배 향상시킬 수 있었다고 전했다. 그 예로 애플의 시스템에서 스테이블 디퓨전을 활용해 이미지를 2개를 생성하는 동안 지포스 RTX 4090 GPU를 장착한 시스템에서 15개의 이미지를 생성할 수 있었다고 밝혔다. 즉 애플 최신 칩보다 7배 이상 빠른 성능을 보였다는 말이다. 최근 업계는 빅테크의 클라우드를 통해 LLM을 구동하는 대신 엣지 장치에서 실행하는 온디바이스 AI에 집중하고 있다. '챗GPT'와 같은 LLM을 모든 서비스에 적용하기엔 컴퓨팅 비용이 너무 비싸다는 이유다. 따라서 경량화된 LLM으로 운영 비용과 기술적 한계를 돌파하는 것은 물론 생성 AI를 모바일 기기나 다양한 서비스에 칩 형태로 직접 적용하는 방법이 떠오르고 있다. 퀄컴을 필두로 메타, 구글, MS, IBM 등 빅테크는 LLM 경량화 사업에 앞다퉈 뛰어들고 있다. 여기에 애플과 인텔은 자사 제품에서 구동하는 온디바이스 AI 모델을 채택하며 경쟁에 가세했다. 엔비디아는 이런 솔루션은 \"저전력으로 실행하는 경량 AI 워크로드에 적합한 수준\"이라고 지적했다. RTX GPU는 AI 워크로드에서 CPU 성능보다 20~100배 뛰어나기 때문에 엔비디아의 GPU가 수행하는 기능을 보완할 수 있는 수준이라는 설명이다. 또 \"전 세계적으로 1억개 이상의 RTX GPU를 갖춘 세계 최대 규모의 전용 AI 하드웨어 설치 기반을 이용해 대규모 확산이 가능하다\"라고 강조했다. 텐서RT-LLM 활용에 관심이 있는 개발자는 ‘엔비디아 디벨로퍼’에서 다운로드할 수 있다. 박찬 기자 cpark@aitimes.com"
}