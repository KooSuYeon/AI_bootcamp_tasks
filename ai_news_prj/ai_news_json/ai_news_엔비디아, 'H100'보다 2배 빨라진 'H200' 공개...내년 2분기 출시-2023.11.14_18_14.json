{
    "title": "엔비디아, 'H100'보다 2배 빨라진 'H200' 공개...내년 2분기 출시",
    "created_at": "2023.11.14 18:14",
    "content": "GPU로 인공지능(AI) 칩 시장을 장악한 엔비디아가 고속 대용량 메모리를 탑재한 차세대 AI 칩 'H200'을 공개했다. 기존 GPU를 뛰어넘는 성능으로 타 업체와의 격차를 더욱 벌리겠다는 의도다. 엔비디아는 8일(현지시각) 미국 덴버에서 열린 ‘슈퍼컴퓨팅 2023’ 컨퍼런스에서 차세대 AI 칩 ‘HGX H200’을 내년 2분기부터 출시한다고 밝혔다. H200은 기존 칩인 'H100'의 업그레이드 버전으로, 생성 AI 모델의 기반이 되는 대형언어모델(LLM)에 적용해 이를 훈련하도록 설계된 GPU다. H200은 SK하이닉스의 5세대 고대역폭메모리 ‘HBM3E’를 제공하는 최초의 GPU로, 초당 4.8테라바이트의 속도로 141GB의 메모리를 제공한다. HBM은 여러 개의 D램을 수직으로 쌓아 데이터 처리 속도를 향상한 최선단 메모리 반도체다. 텍스트 및 이미지 생성을 위해 훈련된 LLM 성능 강화에도 도움이 된다. 초당 3.35TB 속도인 H100 대비 출력 속도가 2배 가까이 빨라졌으며 용량과 대역폭도 2배 이상 증가했다. 엔비디아는 메타의 LLM인 라마2에 H200을 사용해 본 결과 H100보다 2배 빠른 출력을 낸다고 설명했다. 또 H200은 H100과 호환이 가능해 H100을 사용 중인 기업이라면 서버 시스템이나 소프트웨어를 바꿀 필요가 없다. H200은 H100과 동일하게 엔비디아의 가속 컴퓨팅 플랫폼 호퍼(Hopper) 아키텍처를 기반으로 개발됐다. H200은 온프레미스, 클라우드, 하이브리드 클라우드와 엣지를 비롯한 모든 유형의 데이터센터에 배포할 수 있다. 내년부터 아마존웹서비스(AWS), 구글 클라우드 및 오라클 클라우드 인프라(OCI) 등에 공급될 예정이다. 내년에는 호퍼의 후속 아키텍처로 '블랙웰(Blackwell)'이 등장하고, 2025년에는 'X'로 지정한 새로운 아키텍처가 뒤를 이을 예정이다. 엔비디아 H200은 AMD의 차세대 AI 칩 MI300X와 경쟁할 것으로 예상된다. AMD는 엔비디아 H100 대비 메모리 트랜지스터 밀도는 2.4배, 대역폭은 1.6배 이상 늘린 MI300X를 연말 출시할 예정이다. 한편 인텔도 이날 행사에서 곧 출시될 '가우디 3(Gaudi 3)'에 대한 세부 사항을 공개했다. 5나노 공정의 가우디 3는 가우디 2보다 4배의 성능, 2배의 네트워크 대역폭, 1.5배 더 많은 HBM 용량을 제공한다. 또 가우디 3는 가우디 2에 사용하는 단일 다이 솔루션과 달리 두 개의 개별 칩렛으로 설계해 제조를 단순화했다. 박찬 기자 cpark@aitimes.com"
}