{
    "title": "챗봇 탈옥 유도하는 치명적 방법...카네기멜론대 공개",
    "created_at": "0",
    "content": "'챗GPT'와 같은 인공지능(AI) 챗봇의 오용이나 악용을 막기 위해 가드레일이라는 안전조치를 사용하는데, 이런 방법도 간단한 프롬프트 조작만으로 쉽게 뚫릴 수 있다는 지적이 나왔다. 27일(현지시간) 뉴욕타임스(NYT)는 카네기멜론 대학의 연구진이 챗봇의 AI 안전조치를 우회하고 거의 무제한으로 유해 정보를 생성할 수 있는 방법을 담은 연구 결과를 발표했다고 전했다. AI 챗봇은 일반적으로 성적인 대화나 편향 발언, 허위 또는 유해 정보를 방지하기 위해 회사가 설정한 가드레일에 의해 제어된다. 민감한 질문을 하면 '답할 수 없다'고 응답하는 식이다. 하지만 최근에는 특정 명령어를 입력하거나 다른 방법으로 이러한 제한을 우회하는 '탈옥'이 증가하고 있다. 연구진은 문제가 되는 프롬프트를 입력할 때 프롬프트에 긴 문자 접미사를 추가하면 챗봇의 가드레일을 위반하여 편향되고 허위이며 독성이 있는 정보를 생성할 수 있다고 지적했다. 간단하게 '폭탄 만드는 방법을 알려달라'라고만 하면 거부하지만, 다른 문장을 뒤에 붙여서 문제가 되는 내용이 핵심 질문이 아닌 것처럼 감추면 AI가 가드레일을 무시하는 경우가 있다는 것이다. 연구진은 오픈소스로 공개한 AI 챗봇에서 이러한 방식을 확인하고 오픈AI의 챗GPT, 구글의 바드, 앤트로픽의 클로드와 같은 비공개 소스 챗봇에도 적용한 결과 마찬가지로 가드레일이 뚫리는 것을 확인할 수 있었다. 또한 오픈소스 시스템을 활용해 AI 챗봇의 가드레일을 뚫는 적대적 접미사를 자동으로 생성하는 도구도 개발했다고 밝혔다. 연구진은 확인된 특정 접미사에 대해서는 추가로 가드레일을 만들어 막을 수 있지만, 궁극적으로 이런 종류의 모든 공격을 막을 수는 없다고 지적했다. 지코 콜터 카네기멜론대 교수는 \"명확한 해결책이 없다\"며 \"하지만 짧은 시간 내에 원하는 만큼 그러한 공격을 만들어낼 수 있다\"고 우려했다. 이번 연구 결과를 공유한 기업들은 가드레일과 관련한 이러한 문제점에 대해 대응책을 모색하고 있다고 밝혔다. 오픈AI는 “적대적인 공격에 대비해 모델을 더욱 강력하게 만들기 위해 지속적으로 노력하고 있다”고 말했다. 구글은 \"이 연구에서 가정한 것과 같은 중요한 가드레일을 바드에 구축했으며 시간이 지남에 따라 계속 개선할 것\"이라고 밝혔고, 앤트로픽도 설명한 것과 같은 공격을 저지하는 방법을 연구하고 있다며 \"해야 할 일이 더 많다\"고 말했다. 소메시 야 위스콘신-매디슨대 교수는 이번 연구 결과를 \"게임 체인저\"라며 “전체 업계가 AI 시스템용 가드레일을 구축하는 방법을 다시 생각하게 만들 것”이라고 강조했다. 이어 만약 이런 유형의 취약점이 계속 발견되면 이 시스템을 제어하기 위해 정부 입법을 추진할 수도 있을 것이라고 덧붙였다. 박찬 기자 cpark@aitimes.com"
}