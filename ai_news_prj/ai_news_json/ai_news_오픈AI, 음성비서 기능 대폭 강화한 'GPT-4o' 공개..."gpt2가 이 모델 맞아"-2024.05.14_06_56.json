{
    "title": "오픈AI, 음성비서 기능 대폭 강화한 'GPT-4o' 공개...\"gpt2가 이 모델 맞아\"",
    "created_at": "2024.05.14 06:56",
    "content": "오픈AI가 음성 비서 기능을 강화한 새 멀티모달모델(LMM) 'GPT-4o'를 공개했다. 그리고 최근 화제를 일으켰던 정체불명의  'gpt2-챗봇'이 이 모델이라는 것을 확인했다. 오픈AI는 13일(현지시간) 봄철 업데이트 스트리밍을 통해 텍스트와 이미지, 오디오로 실시간 대화가 가능한 새로운 음성 어시스턴트 모델을 시연했다. 새 모델의 'o'는 모든 것이라는 '옴니(omni)'를 뜻한다. 정식 출시는 몇주 안에 이뤄질 예정이다. 개발자를 위한 API는 이날부터 공개했다. 또 이 모델은 향후 애플의 '시리'에 통합하는 데에도 사용될 것으로 보인다. 미라 무라티 오픈AI CTO는 이날 발표에 나서 이 모델이 인간과의 상호작용에서 기존 모델보다 더 빠르고 저렴하다고 강조했다. 또 기존 유료 사용자들에게만 지원했던 음성 기능을 무료 사용자에게도 확대한다고 밝혔다. 구두로 질문을 하면 이 모델은 최소 232밀리초, 평균 320밀리초 안에 오디오로 응답할 수 있다. 이는 사람의 반응 속도와 비슷한 것으로, 기존 'GPT-4'의 5.4초에서 비약적으로 빨라진 것이다. 대화 중 일부는 노래로 표현하기도 했으며, 심지어 GPT-4o를 탑재한 휴대폰 두개가 상호작용하는 모습도 보여줬다. 또 50개에 달하는 다국어 능력을 갖추고 있어, 거의 즉각적으로 다른 언어 번역이 가능하다. 한국어 등 20개 언어를 토크나이저 압축으로 개선했다고 밝혔다. 이미지 처리 기능에서도 뛰어난 성능을 보였다. 종이에 적힌 수학 문제를 보여주고 답을 풀어내는 과정도 공개했다. 오픈AI는 GPT-4o가 'GPT-4 터보'보다 두배 더 빠르고 비용은 2분의 1 수준이라고 설명했다. 기술적으로는 기존에 LMM을 구동하기 위해 텍스트와 이미지, 음성 부분을 따로 담당하는 것을 넘어, 모델 3개를 하나로 통합했다고 설명했다. 무라티 CTO는 \"모델 3개가 함께 작동하면 많은 지연 시간이 발생하고 경험의 몰입도가 떨어진다\"라며 \"하지만 오디오, 텍스트, 영상 전반에 걸쳐 추론하는 하나의 모델이 있으면 모든 대기 시간을 줄이고 사람이 하는 것처럼 챗GPT와 빠르게 상호 작용할 수 있다\"라고 설명했다. 또 “상호작용과 사용 편의성 측면에서 큰 도약을 이룬 것은 이번이 처음”이라고 강조했다. 벤치마크에서도 GPT-4o는 텍스트, 추론 및 코딩 지능에서 GPT-4 터보 수준은 물론, 다국어와 오디오, 비전 기능에서 새로운 최고 수준을 달성했다고 전했다. 샘 알트먼 CEO는 이날 X(트위터)를 통해 스트리밍 상황을 중계하던 중 \"그녀(her)\"라는 멘트를 남겼다. 그는 “영화에 나오는 AI 같은 느낌이다. 그리고 그것이 현실이라는 것이 아직도 나에게는 조금 놀랍다”라고 말했다. \"인간 수준의 응답 시간과 표현력을 갖추게 된 것은 큰 변화\"라고도 전했다. 한편 오픈AI는 최근 화제가 됐던 'gpt2-챗봇'이 이 모델이라는 것을 확인했다. 윌리엄 페더스 오픈AI 개발자는 이날 X를 통해 \"챗봇 아레나에서 'im-also-a-good-gpt2-chatbot' 버전을 테스트해 왔다\"라는 글을 게시했다. 이 모델은 챗봇 아레나에 등장, GPT-4를 일부 능가하는 뛰어난 성능으로 큰 주목을 받았다. 알트먼 CEO는 \"이 모델에 애착을 가지고 있다\"라고 밝혔으나, 오픈AI의 제품인지는 확인하지 않았다. gpt2는 큰 화제를 일으키며 과다한 트래픽을 유발, 현재는 챗봇 아레나에서 내려간 상태다. 또 알트먼 CEO는 이날 행사가 끝난 뒤 \"우리는 곧 공유할 내용이 더 많아질 것\"이라고 전했다. 오픈AI는 올여름 AI 검색 제품과 동영상 생성 AI '소라'의 정식 출시, 그리고 가장 중요한 'GPT-5'의 공개 등을 남겨 두고 있다. 임대준 기자 ydj@aitimes.com"
}