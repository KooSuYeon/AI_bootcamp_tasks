{
    "title": "\"2030년까지 AI 훈련 컴퓨팅 1만배 증가...병목 현상 올 때까지 AI 극적으로 발전할 것\"",
    "created_at": "2024.09.23 18:05",
    "content": "현재와 같은 추세로 인공지능(AI) 관련 기술이 발전할 경우, 2030년에는 AI 모델의 규모가 지금의 1만배로 확장될 수 있다는 예측이 나왔다. 싱귤러리티허브는 최근 비영리 연구 기관 에포크 AI의 보고서를 인용, 향후 10년 뒤에는 AI가 극적으로 발전할 것을 볼 수 있을 것이라고 보도했다. 에포크 연구진은 이번 시나리오를 작성하기 위해 AI 확장의 가장 큰 4가지 제약 요소를 상세하게 시뮬레이션했다. 우선 전력을 가장 큰 문제로 꼽았다. 메타의 최신 모델 '라마 3'는 27메가와트(MW)의 전기를 소모하는 엔비디아 칩 1만6000개로 학습했으며, 이는 미국 2만3000가구의 연간 전력 소비량과 같다고 설명했다. 전력 효율이 향상하더라도 2030년에는 프론티어 모델을 훈련하려면 지금보다 200배 많은 전력, 즉 6기가와트(GW)가 필요하다고 봤다. 이는 현재 전체 데이터센터에서 소비하는 전력의 30%에 해당한다. 그 정도의 용량을 갖춘 단일 발전소는 거의 없으며, AI 기업들은 여러 발전소에서 전력을 끌어올 수 있는 지역을 찾아야 할 것으로 봤다. 현재 계획 중인 유틸리티 성장을 감안하면, 다소 빡빡하지만 2030년까지는 전력 공급이 가능한 수준으로 봤다. 이런 병목 현상을 해소하기 위해 여러 데이터센터에 학습을 분산하는 방법이 유력하다고 전했다. 이 방법에는 빠른 속도의 고대역폭 파이버 연결이 필요한데, 구글의 '제미나이 울트라'가 이 방법을 사용한 것으로 알려졌다. 전반적으로 단일 발전소의 1GW부터 분산 전력원을 이용한 최대 45GW까지 다양한 전력 공급 시나리오를 제시했다. 전력을 많이 활용할수록 더 큰 모델을 훈련할 수 있는데, 'GPT-4'보다 최대 1만배 더 많은 컴퓨팅 파워를 사용할 수 있다는 결론이다. 전력은 AI 학습의 가장 중요한 요소인 AI 칩을 실행하는 데 사용된다. 모델 학습에 사용되는 컴퓨팅 인프라 예측을 위해 엔비디아와 같은 GPU 설계 회사와 제조를 담당하는 TSMC, 그리고 이를 뒷받침하는 고대역폭 메모리 업체 등을 모두 감안했다. 에포크는 이중 GPU 생산은 여유가 생길 수 있지만, 메모리와 패키징에서 공급 부족이 일어날 수 있다고 봤다. 어쨌거나 칩 분야의 성장 속도를 감안, 2030년에는 AI 훈련에는 모두 2000만~4억개의 AI 칩이 투입될 것으로 봤다. 사실 이 분야는 범위과 넓어 예측이 어렵다고 봤는데, 예상 칩 용량을 감안하면 GPT-4보다 약 5만배 많은 컴퓨팅 인프라를 갖출 것으로 결론 내렸다. 그다음으로 살펴본 분야는 AI 학습용 데이터다. 이 분야는 전문가 사이에서도 이견이 많은 데, 앞으로 2년 뒤에는 데이터가 고갈될 것이라는 예측도 나왔다, 그러나 에포크는 데이터 고갈이 적어도 2030년까지 모델 성장을 억제할 것이라고 보지는 않았다. 대신, 현재 성장률로는 5년 안에 양질의 텍스트 데이터가 바닥날 것이며, 저작권 소송도 불확실성을 높인다고 적었다. 하지만 법원이 저작권자의 손을 들어주더라도, 계약 및 소송에 따르는 복잡한 절차로 인해 당장 데이터 공급에 미치는 영향은 제한적일 것으로 봤다. 이보다는 멀티모달 데이터의 증가와 합성 데이터의 실용성에 더 주목했다. 특히 합성 데이터의 경우, 실용성을 넘어 이를 생성하는 데에도 비용이 들어가는 점을 지적했다. 이처럼 텍스트와 비텍스트 및 합성 데이터를 모두 합치면 GPT-4보다 8만배 더 강력한 컴퓨팅 성능으로 AI 모델을 훈련할만큼 충분할 것으로 추정했다. 마지막으로 살펴본 모델의 학습 지연 시간은 알고리즘의 크기와 관계가 있다. 모델이 클수록 학습시간이 늘어나는 점으로, 모델 개발의 주기를 결정하는 중요한 요소다. 이 모든 요소를 단순 적용하면 GPT-4보다 최대 100만배 이상의 컴퓨팅 파워까지 AI 훈련을 확대할 수 있다는 결론이 나온다. 그러나, 여기에는 병목 현상이 숨어 있다. 즉, 칩이 늘어나는 만큼 전력이 따라주지 못한다는 점이다. 이런 모든 상황을 감안하면 모델은 첫번째 병목현상이 올 때까지, 즉 5년 뒤 전력 한계가 찾아올 때까지만 현재 추세를 유지할 수 있다는 결론이다. 에포크는 \"이런 현상을 종합적으로 고려하면 10년 안에 최대 2e29플롭(FLOP)의 훈련 실행이 가능할 것\"이라며 \"이는 현재 모델보다 약 1만배의 확장을 의미하며, 확장 추세가 2030년까지 중단 없이 계속될 수 있음을 의미한다\"라고 밝혔다. 또 \"이대로라면 기초적인 텍스트 생성에 머물렀던 2019년 'GPT-2'와 정교한 문제 해결 능력을 갖춘 2023년 'GPT-4'의 차이만큼, AI는 10년 안에 극적으로 발전할 것\"이라고 강조했다. 마지막으로 이런 발전이 가능하려면 투자가 수익을 훨씬 앞서는 현재 분위기가 지속될 만한 사회적인 믿음이 있어야 한다고 지적했다. 임대준 기자 ydj@aitimes.com"
}