{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 사용환경 준비 (OpenAI)\n",
    "- API-key는 .env에서 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델 로드하기\n",
    "- 사용한 모델 : gpt-4o-mini\n",
    "- 명시적으로 불러온 api-key를 할당\n",
    "- 논문에 대한 질문의 답변을 해주는 모델이므로 temperature을 0으로 설정해 창의성을 억제함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 문서 로드하기 (초거대 언어모델 연구 동향.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"data/초거대 언어모델 연구 동향.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다루지 못하는 metaData제거\n",
    "- 표 내용을 PyPDFLoader를 이용한 텍스트 변환을 하게 되면 제대로 열에 따른 띄어쓰기를 하지 못함을 발견\n",
    "- 때문에 Hallucination을 일으킬 수 있는 표 데이터 삭제를 전처리 단계에서 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/초거대 언어모델 연구 동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)\\nChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되\\n던 다양한 자연언어처리 하위 분야들이 하나의 모델\\n로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\\n응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \\n되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\\n다임을 맞이하게 되었다 [1].\\nLLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \\n발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \\n요인에 기반하고 있으며, 이 요인들은 현대 자연언어\\n처리 (Natural Language Processing, NLP) 연구의 핵심\\n적인 추세로 간주된다. 첫째로, 데이터의 양적 확대는 \\n무시할 수 없는 중요한 요인이다. 디지털화의 선도로, \\n텍스트 데이터의 양이 기하급수적으로 증가하였고, \\n이는 연구의 질적 변화를 가져왔다. 대규모 코퍼스의 \\n활용은 LLM의 일반화 능력을 향상시키며, 다양한 맥\\n락과 주제에 대한 깊은 학습을 가능하게 한다. 둘째\\n로, 컴퓨팅 기술의 진보는 LLM의 발전에 있어 결정\\n적이었다. 특히, Graphics Processing Unit (GPU) 및 \\nTensor Processing Unit (TPU) 와 같은 고성능 병렬 처\\n리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \\n크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\\n성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있\\n게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 \\n성능 향상을 주도하였다. Attention 및 Transformer \\nArchitecture의 도입은 연구자들에게 문맥 간의 관계\\n를 더욱 정교하게 모델링할 수 있는 방법을 제공하였\\n다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는 \\n* 정회원\\n1) https://openai.com/blog/chatgpt\\n학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델\\n의 크기와 그 성능은 긍정적인 상관 관계를 보인다. \\n이를 통해 연구자들은 모델의 파라미터 수를 증가시\\n키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \\n작용에서 나온 결과이며, 이러한 추세는 앞으로도 \\nNLP 연구의 주요 동력이 될 것으로 예상된다.\\n연구단계를 넘어 LLM은 산업계에서도 많은 발전\\n을 이루어 내고 있다. LLM 은 교육, 의료, 금융, 제조 \\n등 거의 모든 산업 분야에서 광범위한 활용 가능성을 \\n제시하고 있다 [5, 6, 7, 8]. 교육 분야에서는 단순한 \\n정보 검색을 넘어, 개인화된 학습 경로를 추천하는 시\\n스템, 과제의 자동 평가, 학생들의 복잡한 질문에 대\\n한 답변 제공 등의 역할로 활용될 수 있다. 이는 교육\\n의 효율성과 개인화를 동시에 추구하는 현대의 교육 \\n트렌드와 맞물려 큰 효과를 발휘할 것으로 기대된다. \\n의료 분야에서는 환자 데이터를 기반으로 한 초기 진\\n단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\\n석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \\n의학 연구 동향 파악 등의 다양한 역할을 수행할 수 \\n있다. 이로써 의료 전문가들의 결정을 보조하고, 효율\\n적인 치료 방향을 도모할 수 있게 된다. 금융 분야에\\n서는 개인의 투자 성향과 시장의 동향을 분석하여 투\\n자 권고를 제공하는 것 외에도, 금융 위험을 상세하게 \\n분석하거나, 복잡한 금융 거래를 자동화하는 시스템\\n의 핵심 구성 요소로서의 역할을 할 수 있다. 이는 금\\n융 서비스의 효율과 안전성 향상에 크게 기여할 것이\\n다. 제조 분야에서도 LLM은 설계 단계부터 생산, 품\\n질 관리에 이르기까지의 전 과정에서 데이터 분석 및 \\n최적화 도구로 활용될 수 있다. 생산 효율성 향상과 \\n제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\\n하게 대응할 수 있는 기회를 제공한다.\\n그러나, 이러한 긍정적인 측면들과 더불어 LLM의 \\n한계점과 위험성도 고려되어야 한다. LLM 은 학습 데\\n이터의 편향성을 그대로 반영할 수 있어, 편향된 결과\\n나 추천을 할 가능성이 있다 [9]. 이는 특히 중요한 의\\n특집원고')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_table_section(text):\n",
    "    # \"표\"라는 단어가 포함된 부분부터 그 이후의 내용 제거\n",
    "    text_without_table = re.split(r\"\\n표\", text, maxsplit=1)[0]\n",
    "    return text_without_table.strip()\n",
    "\n",
    "for doc in docs:\n",
    "    doc.page_content = remove_table_section(doc.page_content)\n",
    "\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 문서 청크로 나누기\n",
    "- chunk_size=100: 각 청크의 길이를 100글자로 지정함 이때 한국어, 영어 모두 같은 동일하게 100글자 제한이 적용됨, 논문의 문장의 길이를 살펴본 결과 짧은 문장 구분을 하고 있다는 특징을 파악해 이에적합한 청크 길이를 선택함 (문장의 단위 자체 가 긴 글의 경우 보통 512 토큰을 할당한다고 함)\n",
    "- chunk_overlap=10 : 청크 간 10글자씩 겹침, 앞 뒤로 해당 겹침은 적용이 되며 이는 모델의 청크 간의 문맥을 잃지 않도록 도와줌 (긴 글의 경우 보통 100토큰)\n",
    "- length_function=len : 각 청크의 길이를 측정할 때 len함수를 이용해 계산함\n",
    "- is_sperate_regex=False : 구분자로 정규식을 사용하지 않음, 복잡하지 않은 패턴 매칭일 경우에 사용함\n",
    "\n",
    "---\n",
    "CharacterTextPlitter : 텍스트를 나눌 때 사용할 구분자를 지정해서 나누는 방법\n",
    "RecursiveCharacterTextSplitter : 단일 구분자 기준으로 텍스트를 분할하는 것이 아닌 우선순위에 따라 재귀적으로 적용하여 텍스트를 나눔\n",
    "\n",
    "---\n",
    "\n",
    "#### 결론 : 불러온 문서는 논문임. 때문에 문장단위 유사도보다 문단별 유사도를 통해 찾는 것이 더 적합하다는 판단을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/초거대 언어모델 연구 동향.pdf', 'page': 0}, page_content='8 특집원고  초거대 언어모델 연구 동향\\n초거대 언어모델 연구 동향\\n업스테이지  박찬준*･이원성･김윤기･김지후･이활석\\n \\n1. 서  론1)\\nChatGPT1)와 같은 초거대 언어모델(Large Language \\nModel, LLM) 의 등장으로 기존에 병렬적으로 연구되\\n던 다양한 자연언어처리 하위 분야들이 하나의 모델\\n로 처리되고 있으며, 태스크 수렴 현상 (Converge)이 \\n발생하고 있다. 즉 하나의 LLM으로 번역, 요약, 질의\\n응답, 형태소분석 등의 작업을 모두 처리할 수 있게 \\n되었다. 프롬프트 (Prompt)를 어떻게 모델에게 입력하\\n느냐에 따라서 LLM의 다양한 능력들이 창발되고, 이\\n에 따라 사용자의 목적에 맞는 출력을 생성하는 패러\\n다임을 맞이하게 되었다 [1].\\nLLM은 최근 몇 년 간의 연구 동향에 따라 뛰어난 \\n발전을 이루고 있다. 이러한 발전은 몇 가지 주요한 \\n요인에 기반하고 있으며, 이 요인들은 현대 자연언어\\n처리 (Natural Language Processing, NLP) 연구의 핵심\\n적인 추세로 간주된다. 첫째로, 데이터의 양적 확대는 \\n무시할 수 없는 중요한 요인이다. 디지털화의 선도로, \\n텍스트 데이터의 양이 기하급수적으로 증가하였고, \\n이는 연구의 질적 변화를 가져왔다. 대규모 코퍼스의 \\n활용은 LLM의 일반화 능력을 향상시키며, 다양한 맥\\n락과 주제에 대한 깊은 학습을 가능하게 한다. 둘째\\n로, 컴퓨팅 기술의 진보는 LLM의 발전에 있어 결정\\n적이었다. 특히, Graphics Processing Unit (GPU) 및 \\nTensor Processing Unit (TPU) 와 같은 고성능 병렬 처\\n리 하드웨어의 개발은 모델 학습에 있어 병목 현상을 \\n크게 완화시켰다. 이로 인해 연구자들은 모델의 복잡\\n성을 키우고, 더욱 깊은 신경망 구조를 탐구할 수 있\\n게 되었다. 셋째, 알고리즘 및 기술의 발전은 LLM의 \\n성능 향상을 주도하였다. Attention 및 Transformer \\nArchitecture의 도입은 연구자들에게 문맥 간의 관계\\n를 더욱 정교하게 모델링할 수 있는 방법을 제공하였\\n다 [2, 3]. 이 모든 변화의 중심에는 ‘scaling law’라는 \\n* 정회원\\n1) https://openai.com/blog/chatgpt\\n학문적인 통찰이 있다 [4]. 해당 연구에 따르면, 모델\\n의 크기와 그 성능은 긍정적인 상관 관계를 보인다. \\n이를 통해 연구자들은 모델의 파라미터 수를 증가시\\n키면서, 이에 따른 성능 향상을 기술적 진보의 상호 \\n작용에서 나온 결과이며, 이러한 추세는 앞으로도 \\nNLP 연구의 주요 동력이 될 것으로 예상된다.\\n연구단계를 넘어 LLM은 산업계에서도 많은 발전\\n을 이루어 내고 있다. LLM 은 교육, 의료, 금융, 제조 \\n등 거의 모든 산업 분야에서 광범위한 활용 가능성을 \\n제시하고 있다 [5, 6, 7, 8]. 교육 분야에서는 단순한 \\n정보 검색을 넘어, 개인화된 학습 경로를 추천하는 시\\n스템, 과제의 자동 평가, 학생들의 복잡한 질문에 대\\n한 답변 제공 등의 역할로 활용될 수 있다. 이는 교육\\n의 효율성과 개인화를 동시에 추구하는 현대의 교육 \\n트렌드와 맞물려 큰 효과를 발휘할 것으로 기대된다. \\n의료 분야에서는 환자 데이터를 기반으로 한 초기 진\\n단 도구로 활용될 뿐만 아니라, 복잡한 의료 기록 분\\n석, 신약 개발에 필요한 연구 데이터 분석, 또는 최신 \\n의학 연구 동향 파악 등의 다양한 역할을 수행할 수 \\n있다. 이로써 의료 전문가들의 결정을 보조하고, 효율\\n적인 치료 방향을 도모할 수 있게 된다. 금융 분야에\\n서는 개인의 투자 성향과 시장의 동향을 분석하여 투\\n자 권고를 제공하는 것 외에도, 금융 위험을 상세하게 \\n분석하거나, 복잡한 금융 거래를 자동화하는 시스템\\n의 핵심 구성 요소로서의 역할을 할 수 있다. 이는 금\\n융 서비스의 효율과 안전성 향상에 크게 기여할 것이\\n다. 제조 분야에서도 LLM은 설계 단계부터 생산, 품\\n질 관리에 이르기까지의 전 과정에서 데이터 분석 및 \\n최적화 도구로 활용될 수 있다. 생산 효율성 향상과 \\n제품 품질 향상을 도모하며, 고객의 니즈에 더욱 민첩\\n하게 대응할 수 있는 기회를 제공한다.\\n그러나, 이러한 긍정적인 측면들과 더불어 LLM의 \\n한계점과 위험성도 고려되어야 한다. LLM 은 학습 데\\n이터의 편향성을 그대로 반영할 수 있어, 편향된 결과\\n나 추천을 할 가능성이 있다 [9]. 이는 특히 중요한 의\\n특집원고')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits_char = text_splitter.split_documents(docs)\n",
    "splits_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "# 문서 분할\n",
    "splits_recur = recursive_text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = splits_recur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 벡터 임베딩 생성\n",
    "- 텍스트를 벡터로 변환해줌\n",
    "\n",
    "### 6. 벡터 스토어 생성\n",
    "- FAISS : 대규모 벡터 데이터를 효율적으로 저장하고 유사도 검색을 수행함\n",
    "- 벡터 스토어란 벡터를 저장하고 저장한 벡터를 유사도 기반으로 검색하기 위해 설계된 DB와 비슷한 개념\n",
    "- 즉, 벡터 스토어가 큰 개념이고, 이를 활용할 수 있는 라이브러리로 FAISS가 존재하는데 특히나 대규모 벡터 데이터의 검색을 위해 최적회된 라이브러리임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=api_key)\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. FAISS를 Retriever로 변환\n",
    "- retriever 사용 이유 : 벡터 유사도 검색을 한 후 (FAISS) 검색 결과를 텍스트 형태로 반환해줘야 하기 때문\n",
    "- search_type=\"similarity\" : 유사도 기반 검색\n",
    "- search_kwargs={\"k\": 5} : 검색에 해당하는 Document를 5개 가져옴 (엄밀히 말하자면 문장의 자세한 정보가 포함되어 있는 5개의 Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/ai_model/lib/python3.12/site-packages (from rank_bm25) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG 성능 올리기 - Retreiever 활용\n",
    "\n",
    "- 기본 유사도 기반 FAISS retriever를 사용한 것에 더해 BM25 retriever를 함께 앙상블 시킴\n",
    "- BM25Retriever는 단어 빈도 기반 점수 계산 retriever임\n",
    "- 앙상블의 비율은 0.5씩 할당해 단어 빈도 점수, FAISS 기반 유사도가 높은 상위 5개의 문서가 추출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "faiss_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "retriever = EnsembleRetriever(\n",
    "            retrievers=[bm25_retriever, faiss_retriever],\n",
    "            weights=[0.5, 0.5]  # 가중치 설정 (가중치의 합은 1.0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 프롬프트 템플릿 정의\n",
    "- system : 답변해줄 AI의 역할 및 요구사항 정의\n",
    "- user : 사용자에게 입력받을 사항 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. RAG 체인 구성\n",
    "1. contextual_prompt로 들어온 question은 rag_chain_debug에서 retriever로 전달됨\n",
    "2. 관련된 문서 리스트는 input[\"context\"] 형태로 전달됨\n",
    "3. RunnablePassthrough 클래스는 데이터를 전달하는 역할을 함. invoke() 메서드를 통해 입력된 데이터를 반환함\n",
    "4. ContextToText 클래스 내의 invoke 함수를 통해 inputs[\"context\"]로 들어온 관련 문서 (Document) 는 문자열로 결합되게 되어 model로는 content와 question을 결합한 딕셔너리 형태로 model에 전달됨\n",
    "5. DebugPassThrough 클래스 내의 invoke 함수는 디버깅 (어느 페이지에서 어떤 줄의 내용을 근거했는지를 추적하기 위한 용도). ContextToText 클래스와는 다르게 어떠한 가공도 하지 않음\n",
    "6. 정리하자면 앞서 FAISS를 통해 질문과 관련있는 Document를 가져오고 model은 들어온 질문 데이터 분석과 가져온 Document를 기반으로 한 답변 데이터 생성을 하게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        # print(\"Debug Output:\", output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText()|   contextual_prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. 챗봇 구동 확인\n",
    "\n",
    "#### 질문\n",
    "---\n",
    "\n",
    "아래 질문들의 경우 표 데이터를 제거한 후 제대로 추출할 수 있었던 질문, 표 데이터도 같은 문맥의 문서로 취급되었기 때문\n",
    "\n",
    "\n",
    "\n",
    "1. Open  Ko-LLM  Leaderboard에는 어떤 기업들이 참여하고 있어?\n",
    "2. Open  Ko-LLM  Leaderboard에는 카카오가 참여하고 있어?\n",
    "\n",
    "---\n",
    "\n",
    "아래 질문들의 경우 retriever에 BM25를 적용함으로 제대로 추출할 수 있었던 질문, 적용하지 않을 시 Hullcination을 일으킴\n",
    "\n",
    "\n",
    "\n",
    "3. 한국의 LLM 리더보드에 ETRI가 참여하고 있어?\n",
    "4. 한국의 LLM 리더보드에 카카오가 참여하고 있어?\n",
    "5. Open  Ko-LLM  Leaderboard에는 카카오가 참여하고 있어?\n",
    "6. Open  Ko-LLM  Leaderboard에는 ETRI가 참여하고 있어?\n",
    "7. 카카오의 인공지능 윤리 원칙에 책임성이 포함되어 있어?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Open  Ko-LLM  Leaderboard에는 어떤 기업들이 참여하고 있어?\n",
      "RAG response : Open Ko-LLM Leaderboard에는 Polyglot-Ko, KULLM, KoAlpaca, 42MARU, ETRI, Maum.AI 등 다양한 기업들이 참여하고 있습니다.\n",
      "question: Open  Ko-LLM  Leaderboard에는 카카오가 참여하고 있어?\n",
      "RAG response : 제공된 정보에는 Open Ko-LLM Leaderboard에 참여하는 기업 목록이 포함되어 있지만, 카카오에 대한 언급은 없습니다. 따라서 카카오는 참여하고 있지 않은 것으로 보입니다.\n",
      "question: 한국의 LLM 리더보드에 ETRI가 참여하고 있어?\n",
      "RAG response : 네, 한국의 LLM 리더보드에 ETRI가 참여하고 있습니다.\n",
      "question: 한국의 LLM 리더보드에 카카오가 참여하고 있어?\n",
      "RAG response : 네, 카카오는 한국의 LLM 리더보드에 참여하고 있습니다.\n",
      "question: Open  Ko-LLM  Leaderboard에는 카카오가 참여하고 있어?\n",
      "RAG response : 제공된 정보에는 Open Ko-LLM Leaderboard에 참여하는 기업 목록이 포함되어 있지만, 카카오에 대한 언급은 없습니다. 따라서 카카오는 참여하고 있지 않은 것으로 보입니다.\n",
      "question: Open  Ko-LLM  Leaderboard에는 ETRI가 참여하고 있어?\n",
      "RAG response : 네, Open Ko-LLM Leaderboard에는 ETRI가 참여하고 있습니다.\n",
      "question: 카카오의 인공지능 윤리 원칙에 책임성이 포함되어 있어?\n",
      "RAG response : 제공된 문맥에서는 카카오의 인공지능 윤리 원칙에 대한 구체적인 내용이 언급되지 않았습니다. 그러나 일반적으로 인공지능 윤리 원칙에는 책임성이 포함되어 있으며, 이는 인공지능을 개발하고 활용하는 주체들의 역할과 책임을 명확히 설정하여 발생할 수 있는 피해를 최소화하는 내용을 포함합니다. 따라서 카카오의 윤리 원칙에도 책임성이 포함될 가능성이 높습니다.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"질문을 입력하세요! 종료를 원한다면 exit을 입력하세요.\")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    print(\"question: \" + query)\n",
    "    \n",
    "    response = rag_chain_debug.invoke(query)\n",
    "    print(\"RAG response : \" + response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-RAG 모델과의 비교\n",
    "- Hallucination을 일으킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model_raw = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Open  Ko-LLM  Leaderboard에는 어떤 기업들이 참여하고 있어?\n",
      "Raw AI Response: Open Ko-LLM Leaderboard는 Kakao Brain, Kakao Enterprise, NAVER, SK Telecom, LG CNS, LG Electronics, Samsung Electronics, 삼성SDS, 삼성전자, 삼성중공업, 삼성화재, 삼성생명, 삼성물산, 삼성전기, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권, 삼성카드, 삼성엔지니어링, 삼성중공업, 삼성증권,\n",
      "question: Open  Ko-LLM  Leaderboard에는 카카오가 참여하고 있어?\n",
      "Raw AI Response: 네, Open Ko-LLM Leaderboard에는 카카오가 참여하고 있습니다.\n",
      "question: 확실해??\n",
      "Raw AI Response: 죄송합니다. 제가 실수를 했네요. Open Ko-LLM Leaderboard에는 카카오가 참여하고 있지 않습니다. 죄송합니다.\n",
      "question: 어떻게 알아?\n",
      "Raw AI Response: 죄송합니다. 이전에 말씀드린 내용은 잘못된 정보였습니다. Open Ko-LLM Leaderboard에는 Kakao Brain이 참여하고 있습니다. 죄송합니다.\n",
      "question: 카카오의 인공지능 윤리 원칙에 책임성이 포함되어 있어?\n",
      "Raw AI Response: 네, 카카오의 인공지능 윤리 원칙에는 책임성이 포함되어 있습니다. 카카오는 인공지능 기술을 개발하고 활용함에 있어서 사용자의 안전과 개인정보 보호, 투명성, 공정성, 책임성 등을 중요하게 여기고 있습니다.\n",
      "question: 확실해?\n",
      "Raw AI Response: 네, 카카오의 인공지능 윤리 원칙에는 책임성이 포함되어 있습니다. 카카오는 사용자의 안전과 개인정보 보호를 위해 책임을 다하는 것을 중요하게 생각하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 시스템 메시지 설정\n",
    "system_message = SystemMessage(content=\"너는 최근 초거대 언어모델 연구 동향을 알려주는 인공지능이야. 질문에 알맞은 답변을 해줘.\")\n",
    "messages = [system_message]\n",
    "\n",
    "while True:\n",
    "    # 유저 입력\n",
    "    user_input = input(\"질문을 입력하세요 : \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    print(\"question: \" + user_input)\n",
    "    # 사용자 메시지 추가\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "    \n",
    "    # ChatOpenAI 모델을 이용해 답변 생성\n",
    "    response = model_raw.invoke(messages)\n",
    "    \n",
    "    # AI의 답변을 가져와 저장\n",
    "    reply = response.content\n",
    "    messages.append(AIMessage(content=reply))\n",
    "    \n",
    "    print(\"Raw AI Response: \" + reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 왜 RAG 이 필요한가\n",
    "\n",
    "- RAG를 통해 LLM 모델의 Hallucination을 줄일 수 있으며 업데이트 되지 않은 최신 소식 / 답변하고자 하는 최신 소식 / 본인 서비스에 대한 사적인 정보에 대한 답변을 할 수 있기 때문임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
